شجرة القرار

**شجرة القرار (DTs)** هي طريقة تعلم إشرافية غير معلمية تستخدم للتصنيف والتنبؤ. الهدف هو إنشاء نموذج يتنبأ بقيمة متغير الهدف عن طريق تعلم قواعد اتخاذ القرار البسيطة المستنبطة من ميزات البيانات. يمكن النظر إلى الشجرة على أنها تقريب ثابت قطاعي.

على سبيل المثال، في المثال أدناه، تتعلم أشجار القرار من البيانات لتقريب منحنى الجيب بمجموعة من قواعد اتخاذ القرار if-then-else. كلما زاد عمق الشجرة، زادت تعقيد قواعد القرار، وزاد ملاءمة النموذج.

بعض مزايا أشجار القرار هي:

- بسيطة الفهم والتفسير. يمكن تصور الأشجار.

- تتطلب القليل من الإعداد للبيانات. غالبًا ما تتطلب التقنيات الأخرى تطبيع البيانات، ويجب إنشاء متغيرات وهمية، ويجب إزالة القيم الفارغة. تدعم بعض مجموعات الأشجار والخوارزميات القيم المفقودة.

- تكلفة استخدام الشجرة (أي التنبؤ بالبيانات) لوغاريتمية في عدد نقاط البيانات المستخدمة لتدريب الشجرة.

- القدرة على التعامل مع البيانات العددية والتصنيفية. ومع ذلك، لا يدعم تنفيذ sklearn-learn المتغيرات التصنيفية حاليًا. عادة ما تتخصص التقنيات الأخرى في تحليل مجموعات البيانات التي تحتوي على نوع واحد فقط من المتغيرات. راجع الخوارزميات لمزيد من المعلومات.

- القدرة على التعامل مع المشكلات متعددة الإخراج.

- يستخدم نموذج الصندوق الأبيض. إذا كان من الممكن ملاحظة حالة معينة في نموذج، فمن السهل تفسير الشرط باستخدام المنطق البولياني. على النقيض من ذلك، في نموذج الصندوق الأسود (على سبيل المثال، في شبكة عصبية اصطناعية)، قد يكون من الصعب تفسير النتائج.

- من الممكن التحقق من صحة نموذج باستخدام الاختبارات الإحصائية. وهذا يجعل من الممكن مراعاة موثوقية النموذج.

- الأداء جيد حتى إذا كانت افتراضاته منتهكة إلى حد ما بواسطة النموذج الحقيقي الذي تم من خلاله إنشاء البيانات.

تشمل عيوب أشجار القرار ما يلي:

- يمكن أن يؤدي متعلمو شجرة القرار إلى إنشاء أشجار معقدة للغاية لا تعمم البيانات جيدًا. يُعرف هذا الإفراط في الملاءمة. تعد الآليات، مثل التشذيب، وتحديد الحد الأدنى لعدد العينات المطلوبة في عقدة ورقة أو تعيين العمق الأقصى للشجرة، ضرورية لتجنب هذه المشكلة.

- يمكن أن تكون أشجار القرار غير مستقرة لأن التغيرات الطفيفة في البيانات قد تؤدي إلى ظهور شجرة مختلفة تمامًا. يتم تخفيف هذه المشكلة عن طريق استخدام أشجار القرار ضمن مجموعة.

- تنبؤات أشجار القرار ليست سلسة ولا مستمرة، ولكنها تقريبات ثابتة قطاعيًا كما هو موضح في الشكل أعلاه. لذلك، فإنهم لا يجيدون الاستقراء.

- تعتبر مشكلة تعلم شجرة قرار مثالية معروفة NP-كاملة من عدة جوانب للمثالية وحتى للمفاهيم البسيطة. وبالتالي، تستند خوارزميات تعلم شجرة القرار العملية إلى خوارزميات启发式 مثل الخوارزمية الجشعة حيث يتم اتخاذ قرارات محلية مثالية في كل عقدة. لا يمكن لهذه الخوارزميات أن تضمن العودة إلى شجرة القرار المثالية عالميًا. يمكن التخفيف من حدة ذلك عن طريق تدريب أشجار متعددة في متعلم مجموعة، حيث يتم أخذ عينات عشوائية من الميزات والعينات بالاستبدال.

- هناك مفاهيم يصعب تعلمها لأن أشجار القرار لا تعبر عنها بسهولة، مثل مشكلات XOR أو التكافؤ أو المضاعف.

- ينشئ متعلمو شجرة القرار أشجار متحيزة إذا كانت بعض الفئات تهيمن عليها. لذلك، يوصى بتوازن مجموعة البيانات قبل الملاءمة باستخدام شجرة القرار.

التصنيف

قادر على إجراء التصنيف متعدد الفئات على مجموعة من البيانات.

كما هو الحال مع المصنفات الأخرى، يأخذ كإدخال صفيفين: صفيف X، ناقص أو كثيف، بشكل (n_samples، n_features) الذي يحمل عينات التدريب، ومصفوفة Y من القيم الصحيحة، الشكل (n_samples)، الذي يحمل تسميات الفصل لمجموعات التدريب::

    >>> from sklearn import tree
    >>> X = [[0, 0], [1, 1]]
    >>> Y = [0, 1]
    >>> clf = tree.DecisionTreeClassifier()
    >>> clf = clf.fit(X, Y)

بعد الملاءمة، يمكن بعد ذلك استخدام النموذج للتنبؤ بفئة العينات::

    >>> clf.predict([[2., 2.]])
    array([1])

في حالة وجود فئات متعددة بنفس الاحتمالية الأعلى، فإن المصنف سيتنبأ بالفئة ذات المؤشر الأدنى من بين تلك الفئات.

كبديل لإخراج فئة محددة، يمكن التنبؤ باحتمالية كل فئة، والتي هي كسر عينات التدريب للفئة في ورقة::

    >>> clf.predict_proba([[2., 2.]])
    array([[0., 1.]])

قادر على كل من التصنيف الثنائي (حيث التصنيفات هي [-1، 1]) والتصنيف متعدد الفئات (حيث التصنيفات هي [0، ...، K-1]).

باستخدام مجموعة بيانات Iris، يمكننا إنشاء شجرة كما يلي::

    >>> from sklearn.datasets import load_iris
    >>> from sklearn import tree
    >>> iris = load_iris()
    >>> X, y = iris.data, iris.target
    >>> clf = tree.DecisionTreeClassifier()
    >>> clf = clf.fit(X, y)

بمجرد التدريب، يمكنك رسم الشجرة باستخدام وظيفة plot_tree::


    >>> tree.plot_tree(clf)
    [...]

يمكن أيضًا تصدير الشجرة بتنسيق Graphviz باستخدام مصدر التصدير. إذا كنت تستخدم مدير الحزم conda، فيمكن تثبيت برامج Graphviz الثنائية وحزمة Python باستخدام conda install python-graphviz.

بدلاً من ذلك، يمكن تنزيل البرامج الثنائية لـ Graphviz من صفحة مشروع Graphviz الرئيسية، ويمكن تثبيت الغلاف Python من pypi باستخدام pip install graphviz.

فيما يلي مثال على تصدير Graphviz للشجرة الموضحة أعلاه والتي تم تدريبها على مجموعة بيانات Iris بالكامل؛ يتم حفظ النتائج في ملف إخراج يسمى iris.pdf::


      >>> import graphviz # doctest: +SKIP
      >>> dot_data = tree.export_graphviz(clf, out_file=None) # doctest: +SKIP
      >>> graph = graphviz.Source(dot_data) # doctest: +SKIP
      >>> graph.render("iris") # doctest: +SKIP

يدعم مصدر التصدير أيضًا مجموعة متنوعة من الخيارات الجمالية، بما في ذلك تلوين العقد حسب فئاتها (أو قيمتها للتنبؤ) واستخدام أسماء المتغيرات والفئات الصريحة إذا لزم الأمر. تعرض دفاتر Jupyter أيضًا هذه الرسوم البيانية تلقائيًا داخلها::

      >>> dot_data = tree.export_graphviz(clf, out_file=None, # doctest: +SKIP
      ...                      feature_names=iris.feature_names,  # doctest: +SKIP
      ...                      class_names=iris.target_names,  # doctest: +SKIP
      ...                      filled=True, rounded=True,  # doctest: +SKIP
      ...                      special_characters=True)  # doctest: +SKIP
      >>> graph = graphviz.Source(dot_data)  # doctest: +SKIP
      >>> graph # doctest: +SKIP

أمثلة

* sphx_glr_auto_examples_tree_plot_iris_dtc.py
* sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py

التنبؤ

يمكن أيضًا تطبيق أشجار القرار على مشكلات الانحدار، باستخدام فئة DecisionTreeRegressor.

كما هو الحال في إعداد التصنيف، ستأخذ طريقة التجهيز كحجج صفيفين X و y، باستثناء أنه في هذه الحالة، من المتوقع أن تكون y ذات قيم ذات نقطة عائمة بدلاً من قيم صحيحة::

    >>> from sklearn import tree
    >>> X = [[0, 0], [2, 2]]
    >>> y = [0.5, 2.5]
    >>> clf = tree.DecisionTreeRegressor()
    >>> clf = clf.fit(X, y)
    >>> clf.predict([[1, 1]])
    array([0.5])

أمثلة

* sphx_glr_auto_examples_tree_plot_tree_regression.py

مشكلات متعددة الإخراج
مشكلة المخرجات المتعددة هي مشكلة تعلم إشرافي مع عدة مخرجات يتعين التنبؤ بها، أي عندما يكون Y مصفوفة ثنائية الأبعاد على الشكل (n_samples، n_outputs).

عندما لا توجد علاقة بين المخرجات، هناك طريقة بسيطة جدًا لحل هذا النوع من المشكلات تتمثل في بناء n من النماذج المستقلة، أي نموذج واحد لكل مخرج، ثم استخدام تلك النماذج للتنبؤ بشكل مستقل بكل مخرج من المخرجات n. ومع ذلك، نظرًا لأنه من المحتمل أن تكون قيم المخرجات المتعلقة بنفس المدخلات مرتبطة ببعضها البعض، فغالبًا ما تكون هناك طريقة أفضل تتمثل في بناء نموذج واحد قادر على التنبؤ في وقت واحد بجميع المخرجات n. أولاً، يتطلب وقت تدريب أقل نظرًا لأنه يتم بناء مُقدِّر واحد فقط. ثانيًا، غالبًا ما يمكن زيادة دقة تعميم المُقدِّر الناتج.

فيما يتعلق بشجرة القرارات، يمكن استخدام هذه الاستراتيجية بسهولة لدعم مشكلات المخرجات المتعددة. يتطلب ذلك التغييرات التالية:

- تخزين n من قيم المخرجات في الأوراق بدلاً من 1؛
- استخدام معايير التقسيم التي تحسب متوسط الانخفاض عبر جميع المخرجات n.

تقدم هذه الوحدة دعمًا لمشكلات المخرجات المتعددة من خلال تنفيذ هذه الاستراتيجية في كل من class:DecisionTreeClassifier و class:DecisionTreeRegressor. إذا تم ضبط شجرة قرار على مصفوفة مخرجات Y ذات الشكل (n_samples، n_outputs)، فسيقوم المُقدِّر الناتج بما يلي:

* إخراج n_output من القيم عند التنبؤ؛

* إخراج قائمة بمصفوفات n_output من احتمالات الفئات عند التنبؤ بالاحتمالات.

يتم توضيح استخدام أشجار المخرجات المتعددة للانحدار في مثال: ref:sphx_glr_auto_examples_tree_plot_tree_regression_multioutput.py. في هذا المثال، يكون المدخل X قيمة حقيقية واحدة والمخرجات Y هي جيب وجيب تمام X.

.. figure:: ../auto_examples/tree/images/sphx_glr_plot_tree_regression_multioutput_001.png
   :target: ../auto_examples/tree/plot_tree_regression_multioutput.html
   :scale: 75
   :align: center

يتم توضيح استخدام أشجار المخرجات المتعددة للتصنيف في مثال: ref:sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py. في هذا المثال، تكون المدخلات X هي بكسلات النصف العلوي من الوجوه والمخرجات Y هي بكسلات النصف السفلي من تلك الوجوه.

.. figure:: ../auto_examples/miscellaneous/images/sphx_glr_plot_multioutput_face_completion_001.png
   :target: ../auto_examples/miscellaneous/plot_multioutput_face_completion.html
   :scale: 75
   :align : center

.. rubric:: الأمثلة

* :ref:sphx_glr_auto_examples_tree_plot_tree_regression_multioutput.py
* :ref:sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py

.. rubric:: المراجع

* M. Dumont et al، Fast multi-class image annotation with random subwindows and multiple output randomized trees، International Conference on Computer Vision Theory and Applications 2009

.. _tree_complexity:

التعقيد
==========

بشكل عام، تبلغ تكلفة وقت التشغيل لبناء شجرة ثنائية متوازنة: math: `O (n_ {samples} n_ {features} log (n_ {samples}))` ووقت الاستعلام: math: `O (log (n_ {samples}))`. على الرغم من أن خوارزمية بناء الشجرة تحاول إنشاء أشجار متوازنة، إلا أنها لن تكون متوازنة دائمًا. بافتراض أن الأشجار الفرعية تظل متوازنة تقريبًا، فإن التكلفة في كل عقدة تتكون من البحث عبر: math: `O (n_ {features})` للعثور على الميزة التي توفر أكبر انخفاض في معيار عدم النقاء، على سبيل المثال. الخسارة اللوجستية (التي تعادل مكسب المعلومات). تبلغ تكلفتها: math: `O (n_ {features} n_ {samples} log (n_ {samples}))` في كل عقدة، مما يؤدي إلى تكلفة إجمالية عبر الأشجار بالكامل (عن طريق جمع التكلفة في كل عقدة) من: math: `O (n_ {features} n_ {samples} ^ 2 log (n_ {samples}))`.


نصائح حول الاستخدام العملي
=====================

* تميل أشجار القرارات إلى الإفراط في تناسب البيانات التي تحتوي على عدد كبير من الميزات. من المهم الحصول على النسبة الصحيحة من العينات إلى عدد الميزات، لأن الشجرة ذات العينات القليلة في الفضاء عالي الأبعاد من المحتمل أن تفرط في التناسب.

* ضع في اعتبارك إجراء تقليل الأبعاد (PCA، ICA، أو feature_selection) مسبقًا لمنح شجرتك فرصة أفضل للعثور على ميزات مميزة.

* سوف يساعد المثال: ref:sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py في اكتساب المزيد من الأفكار حول كيفية قيام شجرة القرارات بالتنبؤات، وهو أمر مهم لفهم الميزات المهمة في البيانات.

* قم بتصور شجرتك أثناء التدريب باستخدام وظيفة "التصدير". استخدم "max_depth=3" كعمق شجرة أولي للحصول على شعور بكيفية ملاءمة الشجرة لبياناتك، ثم قم بزيادة العمق.

* تذكر أن عدد العينات المطلوبة لملء الشجرة يتضاعف لكل مستوى إضافي تنمو فيه الشجرة. استخدم "max_depth" للتحكم في حجم الشجرة لمنع الإفراط في التناسب.

* استخدم "min_samples_split" أو "min_samples_leaf" لضمان أن العديد من العينات تعلم كل قرار في الشجرة، عن طريق التحكم في الانقسامات التي سيتم أخذها في الاعتبار. عادة ما يعني العدد الصغير جدًا أن الشجرة ستفرط في التناسب، في حين أن العدد الكبير سيمنع الشجرة من تعلم البيانات. جرب "min_samples_leaf=5" كقيمة أولية. إذا اختلف حجم العينة بشكل كبير، فيمكن استخدام رقم عائم كنسبة مئوية في هذين المعلمين. في حين أن "min_samples_split" يمكن أن يخلق أوراقًا صغيرة بشكل تعسفي، فإن "min_samples_leaf" يضمن أن يكون لكل ورقة حد أدنى من الحجم، مما يتجنب عقد أوراق ذات انحدار منخفض، وعقد إفراط في تناسب في مشكلات الانحدار. بالنسبة للتصنيف باستخدام عدد قليل من الفئات، غالبًا ما يكون "min_samples_leaf=1" هو الخيار الأفضل.

  لاحظ أن "min_samples_split" يأخذ العينات في الاعتبار مباشرة وبشكل مستقل عن "sample_weight"، إذا تم توفيره (على سبيل المثال، تتم معاملة العقدة التي تحتوي على m من العينات المرجحة على أنها تحتوي بالضبط على m من العينات). ضع في اعتبارك "min_weight_fraction_leaf" أو "min_impurity_decrease" إذا كان المحاسبة للاوزان العينات مطلوبة في الانقسامات.

* قم بموازنة مجموعة بياناتك قبل التدريب لمنع الشجرة من التحيز نحو الفئات السائدة. يمكن إجراء موازنة الفئات عن طريق أخذ عدد متساوٍ من العينات من كل فئة، أو يفضل عن طريق تطبيع مجموع أوزان العينات (sample_weight) لكل فئة إلى نفس القيمة. لاحظ أيضًا أن معايير التشذيب المسبق المستندة إلى الوزن، مثل "min_weight_fraction_leaf"، ستكون أقل تحيزًا نحو الفئات السائدة من المعايير التي لا تدرك أوزان العينات، مثل "min_samples_leaf".

* إذا كانت العينات مرجحة، فسيصبح من الأسهل تحسين بنية الشجرة باستخدام معيار تشذيب مسبق يعتمد على الوزن مثل "min_weight_fraction_leaf"، والذي يضمن أن تحتوي عقد الأوراق على الأقل على جزء من إجمالي مجموع أوزان العينات.

* تستخدم جميع أشجار القرارات صفائف "np.float32" داخليًا. إذا لم تكن بيانات التدريب بهذا التنسيق، فسيتم إجراء نسخة من مجموعة البيانات.

* إذا كانت مصفوفة الإدخال X متفرقة جدًا، فيوصى بالتحويل إلى "csc_matrix" متفرقة قبل استدعاء fit و "csr_matrix" متفرقة قبل استدعاء التنبؤ. يمكن أن يكون وقت التدريب أسرع بعدة مرات لمصفوفة متفرقة مقارنة بالمصفوفة الكثيفة عندما تكون للميزات قيم صفرية في معظم العينات.


.. _tree_algorithms:

خوارزميات الشجرة: ID3 و C4.5 و C5.0 و CART
==========================================

ما هي جميع خوارزميات شجرة القرار المختلفة وكيف تختلف عن بعضها البعض؟ أي منها يتم تنفيذه في scikit-learn؟

.. dropdown:: خوارزميات شجرة القرار المختلفة

  تم تطوير ID3_ (Iterative Dichotomiser 3) في عام 1986 بواسطة Ross Quinlan. تقوم الخوارزمية بإنشاء شجرة متعددة الاتجاهات، حيث تجد لكل عقدة (أي بطريقة جشعة) الميزة الفئوية التي ستعطي أكبر مكسب للمعلومات للمستهدفات الفئوية. يتم تنمية الأشجار إلى حجمها الأقصى ثم يتم عادة تطبيق خطوة التشذيب لتحسين قدرة الشجرة على التعميم على البيانات غير المرئية.

  C4.5 هو الخلف لـ ID3 ويزيل القيد الذي يجب أن تكون الميزات فئوية من خلال تعريف سمة منفصلة (بناءً على المتغيرات العددية) تقوم بتقسيم قيمة السمة المستمرة إلى مجموعة من الفواصل الزمنية المحددة بشكل منفصل. يحول C4.5 الأشجار المدربة (أي إخراج خوارزمية ID3) إلى مجموعات من القواعد الشرطية. يتم بعد ذلك تقييم دقة كل قاعدة لتحديد الترتيب الذي يجب تطبيقها به. يتم التشذيب عن طريق إزالة الشرط المسبق للقاعدة إذا تحسنت دقة القاعدة بدونها.

  C5.0 هو أحدث إصدار لـ Quinlan تم إصداره بموجب ترخيص مملوك. يستخدم ذاكرة أقل ويبني مجموعات قواعد أصغر من C4.5 مع كونها أكثر دقة.

  CART (Classification and Regression Trees) مشابه جدًا لـ C4.5، ولكنه يختلف في أنه يدعم المتغيرات المستهدفة العددية (الانحدار) ولا يحسب مجموعات القواعد. يقوم CART ببناء أشجار ثنائية باستخدام الميزة والعتبة التي تعطي أكبر مكسب للمعلومات في كل عقدة.

يستخدم scikit-learn إصدارًا محسنًا من خوارزمية CART؛ ومع ذلك، لا يدعم التنفيذ في scikit-learn المتغيرات الفئوية الآن.

.. _ID3: https://en.wikipedia.org/wiki/ID3_algorithm


.. _tree_mathematical_formulation:

الصيغة الرياضية
فيما يلي الترجمة العربية للنص المُقدم، مع الالتزام بالتعليمات المُرفقة:

========================

بالنسبة لمتجهات التدريب :math:`x_i \in R^n`، حيث i=1,..., l ومتجه التصنيف :math:`y \in R^l`، تقوم شجرة القرار بتقسيم مساحة الميزة بشكل متكرر بحيث يتم تجميع العينات التي لها نفس التصنيفات أو قيم الهدف المتشابهة معًا.

دعنا نمثل البيانات في العقدة :math:`m` بـ :math:`Q_m` مع :math:`n_m`
عينات. بالنسبة لكل مرشح تقسيم :math:`\theta = (j, t_m)` يتكون من ميزة :math:`j` وعتبة :math:`t_m`، قم بتقسيم البيانات إلى المجموعتين الفرعيتين :math:`Q_m^{left}(\theta)` و :math:`Q_m^{right}(\theta)`

.. math::

    Q_m^{left}(\theta) = \{(x, y) | x_j \leq t_m\}

    Q_m^{right}(\theta) = Q_m \setminus Q_m^{left}(\theta)

تُحسب جودة مرشح التقسيم للعقدة :math:`m` باستخدام دالة عدم النقاء أو دالة الخسارة :math:`H()`، ويعتمد اختيار الدالة على المهمة التي يتم حلها (تصنيف أو رجوع)

.. math::

   G(Q_m, \theta) = \frac{n_m^{left}}{n_m} H(Q_m^{left}(\theta))
   + \frac{n_m^{right}}{n_m} H(Q_m^{right}(\theta))

قم بتحديد المعلمات التي تقلل من عدم النقاء

.. math::

    \theta^* = \operatorname{argmin}_\theta G(Q_m, \theta)

كرر العملية للمجموعتين الفرعيتين :math:`Q_m^{left}(\theta^*)` و
:math:`Q_m^{right}(\theta^*)` حتى يتم الوصول إلى العمق الأقصى المسموح به،
:math:`n_m < \min_{samples}` أو :math:`n_m = 1`.

معايير التصنيف
-----------------------

إذا كان الهدف ناتج تصنيف يأخذ القيم 0،1,...,K-1،
بالنسبة للعقدة :math:`m`، دعنا

.. math::

    p_{mk} = \frac{1}{n_m} \sum_{y \in Q_m} I(y = k)

نسبة ملاحظات الفئة k في العقدة :math:`m`. إذا كانت :math:`m` عقدة
نهائية، يتم تعيين `predict_proba` لهذه المنطقة إلى :math:`p_{mk}`.
تتمثل مقاييس عدم النقاء الشائعة فيما يلي.

جيني:

.. math::

    H(Q_m) = \sum_k p_{mk} (1 - p_{mk})

خسارة اللوغاريتم أو الإنتروبيا:

.. math::

    H(Q_m) = - \sum_k p_{mk} \log(p_{mk})

.. dropdown:: إنتروبيا شانون

  يحسب معيار الإنتروبيا إنتروبيا شانون للفئات الممكنة. إنه
  يأخذ تكرارات الفئات لنقاط البيانات التدريبية التي وصلت إلى ورقة معينة
  :math:`m` كاحتمالية لها. إن استخدام **إنتروبيا شانون كمعيار لتقسيم عقدة الشجرة يعادل تقليل خسارة اللوغاريتم** (المعروف أيضًا باسم الإنتروبيا المتقاطعة وانحراف متعدد الحدود) بين التصنيفات الحقيقية :math:`y_i`
والتنبؤات الاحتمالية :math:`T_k(x_i)` لنموذج الشجرة :math:`T` للفئة :math:`k`.

ولرؤية ذلك، تذكر أولاً أن خسارة اللوغاريتم لنموذج الشجرة
:math:`T` المحسوب لمجموعة بيانات :math:`D` يتم تعريفه على النحو التالي:

.. math::

      \mathrm{LL}(D, T) = -\frac{1}{n} \sum_{(x_i, y_i) \in D} \sum_k I(y_i = k) \log(T_k(x_i))

حيث :math:`D` هي مجموعة بيانات تدريبية مكونة من :math:`n` أزواج :math:`(x_i, y_i)`.

في شجرة التصنيف، تكون احتمالية الفئة المتوقعة داخل العقد الورقية
ثابتة، أي: لكل :math:`(x_i, y_i) \in Q_m`، لدينا:
:math:`T_k(x_i) = p_{mk)` لكل فئة :math:`k`.

تسمح هذه الخاصية بإعادة كتابة :math:`\mathrm{LL}(D, T)` كمجموع
إنتروبيا شانون المحسوبة لكل ورقة من :math:`T` مرجحة بعدد نقاط بيانات التدريب
التي وصلت إلى كل ورقة:

.. math::

      \mathrm{LL}(D, T) = \sum_{m \in T} \frac{n_m}{n} H(Q_m)

معايير الانحدار
-------------------

إذا كانت القيمة المستهدفة قيمة مستمرة، فإن المعايير الشائعة لتقليلها لتحديد مواقع الانقسامات المستقبلية هي متوسط مربع الخطأ (MSE أو خطأ L2)، وانحراف بواسون، بالإضافة إلى متوسط الخطأ المطلق (MAE أو خطأ L1). يحدد كل من MSE وانحراف بواسون القيمة المتوقعة للعقد النهائية إلى القيمة المتوسطة المُتعلمة :math:`\bar{y}_m` للعقدة
بينما يحدد MAE القيمة المتوقعة للعقد النهائية إلى الوسيط
:math:`median(y)_m`.

متوسط مربع الخطأ:

.. math::

    \bar{y}_m = \frac{1}{n_m} \sum_{y \in Q_m} y

    H(Q_m) = \frac{1}{n_m} \sum_{y \in Q_m} (y - \bar{y}_m)^2

متوسط انحراف بواسون:

.. math::

    H(Q_m) = \frac{2}{n_m} \sum_{y \in Q_m} (y \log\frac{y}{\bar{y}_m}
    - y + \bar{y}_m)

قد يكون تحديد `criterion="poisson"` خيارًا جيدًا إذا كان هدفك عبارة عن عدد
أو تكرار (عدد لكل وحدة). وفي جميع الأحوال، :math:`y >= 0` هو
شرط ضروري لاستخدام هذا المعيار. لاحظ أنه يناسب بشكل أبطأ بكثير من
معيار MSE. لأسباب تتعلق بالأداء، فإن التنفيذ الفعلي يقلل من نصف انحراف بواسون المتوسط، أي انحراف بواسون المتوسط مقسومًا على 2.

متوسط الخطأ المطلق:

.. math::

    median(y)_m = \underset{y \in Q_m}{\mathrm{median}}(y)

    H(Q_m) = \frac{1}{n_m} \sum_{y \in Q_m} |y - median(y)_m|

لاحظ أنه يناسب بشكل أبطأ بكثير من معيار MSE.

.. _tree_missing_value_support:

دعم القيم المفقودة
======================

:class:`DecisionTreeClassifier`، :class:`DecisionTreeRegressor`
لديهما دعم مدمج للقيم المفقودة باستخدام `splitter='best'`، حيث
يتم تحديد الانقسامات بطريقة جشعة.
:class:`ExtraTreeClassifier`، و :class:`ExtraTreeRegressor` لديهما دعم مدمج
للقيم المفقودة لـ `splitter='random'`، حيث يتم تحديد الانقسامات بشكل عشوائي. لمزيد من التفاصيل حول كيفية اختلاف القاطع على
القيم غير المفقودة، راجع قسم <الغابة المرجعية`.

المعيار المدعوم عند وجود قيم مفقودة هو
`'gini'`، `'entropy'`، أو `'log_loss'`، للتصنيف أو
`'squared_error'`، `'friedman_mse'`، أو `'poisson'` للانحدار.

سنقوم أولاً بوصف كيفية تعامل :class:`DecisionTreeClassifier`، :class:`DecisionTreeRegressor`
مع القيم المفقودة في البيانات.

بالنسبة لكل عتبة محتملة على البيانات غير المفقودة، سيقيم القاطع الانقسام
مع انتقال جميع القيم المفقودة إلى العقدة اليسرى أو اليمنى.

يتم اتخاذ القرارات على النحو التالي:

- بشكل افتراضي عند التنبؤ، يتم تصنيف العينات ذات القيم المفقودة
  باستخدام الفئة المستخدمة في الانقسام الذي تم العثور عليه أثناء التدريب::

    >>> from sklearn.tree import DecisionTreeClassifier
    >>> import numpy as np

    >>> X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)
    >>> y = [0, 0, 1, 1]

    >>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)
    >>> tree.predict(X)
    array([0, 0, 1, 1])

- إذا كان تقييم المعيار هو نفسه لكلتا العقدتين،
  يتم كسر التعادل للقيمة المفقودة عند التنبؤ بالانتقال إلى
  العقدة اليمنى. كما يتحقق القاطع من الانقسام الذي تنتقل فيه جميع القيم المفقودة
  إلى أحد الأبناء والقيم غير المفقودة إلى الآخر::

    >>> from sklearn.tree import DecisionTreeClassifier
    >>> import numpy as np

    >>> X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)
    >>> y = [0, 0, 1, 1]

    >>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)

    >>> X_test = np.array([np.nan]).reshape(-1, 1)
    >>> tree.predict(X_test)
    array([1])

- إذا لم يتم رؤية أي قيم مفقودة أثناء التدريب لميزة معينة، فسيتم أثناء
  التنبؤ تعيين القيم المفقودة إلى الطفل الذي يحتوي على معظم العينات::

    >>> from sklearn.tree import DecisionTreeClassifier
    >>> import numpy as np

    >>> X = np.array([0, 1, 2, 3]).reshape(-1, 1)
    >>> y = [0, 1, 1, 1]

    >>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)

    >>> X_test = np.array([np.nan]).reshape(-1, 1)
    >>> tree.predict(X_test)
    array([1])

:class:`ExtraTreeClassifier`، و :class:`ExtraTreeRegressor` يتعاملان مع القيم المفقودة
بطريقة مختلفة قليلاً. عند تقسيم عقدة، سيتم اختيار عتبة عشوائية
للانقسام على القيم غير المفقودة. بعد ذلك، يتم إرسال القيم غير المفقودة إلى الطفل الأيسر والأيمن بناءً على العتبة العشوائية المحددة، بينما يتم أيضًا إرسال القيم المفقودة بشكل عشوائي إلى الطفل الأيسر أو الأيمن. يتم تكرار هذه العملية لكل ميزة يتم أخذها في الاعتبار في كل انقسام. ويتم اختيار أفضل انقسام من بين هذه الانقسامات.

أثناء التنبؤ، يكون التعامل مع القيم المفقودة مماثلاً لطريقة شجرة القرار:

- بشكل افتراضي عند التنبؤ، يتم تصنيف العينات ذات القيم المفقودة
  باستخدام الفئة المستخدمة في الانقسام الذي تم العثور عليه أثناء التدريب.

- إذا لم يتم رؤية أي قيم مفقودة أثناء التدريب لميزة معينة، فسيتم أثناء
  التنبؤ تعيين القيم المفقودة إلى الطفل الذي يحتوي على معظم العينات.

.. _minimal_cost_complexity_pruning:

التقليم الأمثل لتكلفة التعقيد
===============================

التقليم الأمثل لتكلفة التعقيد هو خوارزمية تستخدم لتقليم شجرة لتجنب
المبالغة في التعلم، موصوفة في الفصل 3 من [BRE]_. يتم معلمجة هذه الخوارزمية بواسطة :math:`\alpha\ge0` المعروف باسم معامل التعقيد. يتم استخدام معامل التعقيد لتعريف تدبير تكلفة التعقيد، :math:`R_\alpha(T)` لشجرة معينة :math:`T`:

.. math::

  R_\alpha(T) = R(T) + \alpha|\widetilde{T}|

حيث :math:`|\widetilde{T}|` هو عدد العقد النهائية في :math:`T` و :math:`R(T)`
يتم تعريفه تقليديًا على أنه معدل الخطأ الإجمالي للتصنيف للعقد
النهائية. من ناحية أخرى، يستخدم sklearn مجموع عدم النقاء المرجح للعينة للعقد النهائية لـ :math:`R(T)`. كما هو موضح أعلاه، يعتمد عدم نقاء العقدة على المعيار. يجد التقليم الأمثل لتكلفة التعقيد الجزء الفرعي من
:math:`T` الذي يقلل من :math:`R_\alpha(T)`.

إن تدبير تكلفة التعقيد لعقدة واحدة هو
:math:`R_\alpha(t)=R(t)+\alpha`. الفرع، :math:`T_t`، هو شجرة حيث
:math:`t` هي العقدة الجذرية. بشكل عام، يكون عدم نقاء العقدة أكبر من مجموع عدم النقاء للعقد
النهائية، :math:`R(T_t)<R(t)`. ومع ذلك، يمكن أن يكون تدبير تكلفة التعقيد لعقدة،
:math:`t`، وفرعها، :math:`T_t`، متساويين اعتمادًا على
:math:`\alpha`. نحن نُعرف :math:`\alpha` الفعال لعقدة على أنه
القيمة التي يتساويان عندها، :math:`R_\alpha(T_t)=R_\alpha(t)` أو
:math:`\alpha_{eff}(t)=\frac{R(t)-R(T_t)}{|T|-1}`. العقدة غير النهائية
مع أقل قيمة لـ :math:`\alpha_{eff}` هي الحلقة الأضعف وسيتم تقليمها. تتوقف هذه العملية عندما يكون :math:`\alpha_{eff}` الأدنى للشجرة المقلمة أكبر من معلمة ``ccp_alpha``.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`

.. rubric:: مراجع

.. [BRE] L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification
  and Regression Trees. Wadsworth, Belmont, CA, 1984.

* https://en.wikipedia.org/wiki/Decision_tree_learning

* https://en.wikipedia.org/wiki/Predictive_analytics

* J.R. Quinlan. C4. 5: programs for machine learning. Morgan
  Kaufmann, 1993.

* T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical
  Learning, Springer, 2009.