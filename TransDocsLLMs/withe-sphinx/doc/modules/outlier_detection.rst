كشف النُقاط الشاذة والبيانات الجديدة

تتطلب العديد من التطبيقات القدرة على تحديد ما إذا كان المُلاحظة الجديدة تنتمي إلى نفس التوزيع مثل الملاحظات الموجودة (فهي نُقطة داخلية)، أو يجب اعتبارها مختلفة (فهي نُقطة شاذة). غالبًا ما تُستخدم هذه القدرة لتنظيف مجموعات البيانات الحقيقية. يجب إجراء تمييزين مهمين:

- كشف النُقاط الشاذة: تحتوي بيانات التدريب على نُقاط شاذة يتم تعريفها على أنها مُلاحظات بعيدة عن الآخرين. لذلك، تحاول خوارزميات تقدير كشف النُقاط الشاذة أن تتناسب مع المناطق التي تكون فيها بيانات التدريب مُركَّزة بشكل أكبر، مع تجاهل المُلاحظات المنحرفة.

- كشف البيانات الجديدة: بيانات التدريب غير مُلوثة بالنُقاط الشاذة، ونحن مهتمون بالكشف عما إذا كانت المُلاحظة الجديدة نُقطة شاذة. في هذا السياق، يُطلق على النُقطة الشاذة أيضًا اسم البيانات الجديدة.

يُستخدم كل من كشف النُقاط الشاذة وكشف البيانات الجديدة في كشف الانحرافات، حيث يكون الهدف هو اكتشاف الملاحظات غير الطبيعية أو غير المعتادة. بعد ذلك، يُعرف كشف النُقاط الشاذة أيضًا باسم كشف الانحرافات غير المُشرف، وكشف البيانات الجديدة باسم كشف الانحرافات شبه المُشرف. في سياق كشف النُقاط الشاذة، لا يُمكن أن تشكل النُقاط الشاذة/الانحرافات مجموعة مُتراصة، حيث تفترض الخوارزميات المُتاحة أن النُقاط الشاذة/الانحرافات تقع في مناطق ذات كثافة منخفضة. على العكس من ذلك، في سياق كشف البيانات الجديدة، يُمكن أن تشكل البيانات الجديدة/الانحرافات مجموعة مُتراصة طالما أنها تقع في منطقة ذات كثافة منخفضة من بيانات التدريب، والتي تُعتبر طبيعية في هذا السياق.

يوفر مشروع scikit-learn مجموعة من أدوات التعلم الآلي التي يُمكن استخدامها لاكتشاف النُقاط الشاذة أو البيانات الجديدة. يتم تنفيذ هذه الاستراتيجية باستخدام كائنات تتعلم بطريقة غير مُشرفة من البيانات:

estimator.fit(X_train)

بعد ذلك، يُمكن تصنيف المُلاحظات الجديدة على أنها نُقاط داخلية أو نُقاط شاذة باستخدام طريقة "التوقع":

estimator.predict(X_test)

يتم وضع علامة على النُقاط الداخلية بالرقم 1، في حين يتم وضع علامة على النُقاط الشاذة بالرقم -1. تستخدم طريقة "التوقع" عتبة لدالة التسجيل الخام التي يحسبها المُقدر. يُمكن الوصول إلى دالة التسجيل هذه من خلال طريقة "score_samples"، في حين يُمكن التحكم في العتبة بواسطة مُعامل "التلوث".

يتم أيضًا تحديد طريقة "decision_function" من دالة التسجيل، بحيث تكون القيم السالبة نُقاطًا شاذة والقيم غير السالبة نُقاطًا داخلية:

estimator.decision_function(X_test)

ملاحظة: لا يدعم neighbors.LocalOutlierFactor بشكل افتراضي طرق "التوقع" و"decision_function" و"score_samples"، ولكنه يدعم فقط طريقة "fit_predict"، حيث كان المقصود في الأصل أن يتم تطبيق هذا المُقدر لاكتشاف النُقاط الشاذة. يُمكن الوصول إلى درجات الشذوذ لعينات التدريب من خلال الخاصية "negative_outlier_factor".

إذا كنت ترغب حقًا في استخدام neighbors.LocalOutlierFactor لاكتشاف البيانات الجديدة، أي التنبؤ بالتصنيفات أو حساب درجة الشذوذ لبيانات جديدة غير مرئية، فيُمكنك إنشاء مثيل للمُقدر مع تعيين مُعامل "البيانات الجديدة" إلى "True" قبل ضبط المُقدر. في هذه الحالة، لا تتوفر طريقة "fit_predict".

تحذير: كشف البيانات الجديدة باستخدام Local Outlier Factor

عند تعيين مُعامل "البيانات الجديدة" إلى "True"، يجب الانتباه إلى أنه يجب استخدام طرق "التوقع" و"decision_function" و"score_samples" فقط على بيانات جديدة غير مرئية، وليس على عينات التدريب، حيث قد يؤدي ذلك إلى نتائج خاطئة. أي أن نتيجة طريقة "التوقع" لن تكون مُطابقة لنتيجة طريقة "fit_predict". يُمكن دائمًا الوصول إلى درجات الشذوذ لعينات التدريب من خلال الخاصية "negative_outlier_factor".

يتم تلخيص سلوك neighbors.LocalOutlierFactor في الجدول التالي:

| الأسلوب | كشف النُقاط الشاذة | كشف البيانات الجديدة |
| -------- | ------------------- | -------------------- |
| fit_predict | متاح | غير متاح |
| predict | غير متاح | استخدم فقط على بيانات جديدة |
| decision_function | غير متاح | استخدم فقط على بيانات جديدة |
| score_samples | استخدم negative_outlier_factor | استخدم فقط على بيانات جديدة |
| negative_outlier_factor | متاح | متاح |

نظرة عامة على طرق كشف النُقاط الشاذة

مقارنة بين خوارزميات كشف النُقاط الشاذة في scikit-learn. لا يُظهر Local Outlier Factor (LOF) حدًا فاصلاً باللون الأسود حيث لا توجد لديه طريقة "توقع" لتطبيقها على بيانات جديدة عند استخدامها لاكتشاف النُقاط الشاذة.

يؤدي كل من ensemble.IsolationForest وneighbors.LocalOutlierFactor أداءً جيدًا بشكل معقول على مجموعات البيانات المُستخدمة هنا. من المعروف أن svm.OneClassSVM حساس للنُقاط الشاذة، وبالتالي لا يؤدي أداءً جيدًا جدًا لاكتشاف النُقاط الشاذة. ومع ذلك، فإن كشف النُقاط الشاذة في الأبعاد العالية، أو بدون أي افتراضات حول توزيع بيانات النُقاط الداخلية، يُمثل تحديًا صعبًا. لا يزال يُمكن استخدام svm.OneClassSVM مع كشف النُقاط الشاذة، ولكنه يتطلب ضبط دقيق لمعامله "nu" للتعامل مع النُقاط الشاذة ومنع الإفراط في التلائم. يوفر linear_model.SGDOneClassSVM تنفيذًا لخوارزمية One-Class SVM الخطية ذات التعقيد الخطي في عدد العينات. يتم استخدام هذا التنفيذ هنا مع تقنية تقريب النواة للحصول على نتائج مُشابهة لنتائج svm.OneClassSVM الذي يستخدم نواة غاوسية بشكل افتراضي. وأخيرًا، يفترض covariance.EllipticEnvelope أن البيانات تتبع توزيع غاوسي ويتعلم شكل إهليلجي. لمزيد من التفاصيل حول المُقدرات المُختلفة، يُرجى الرجوع إلى المثال sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py والأقسام أدناه.

أمثلة

* يُرجى الرجوع إلى sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py لمُقارنة بين svm.OneClassSVM وensemble.IsolationForest وneighbors.LocalOutlierFactor وcovariance.EllipticEnvelope.

* يُرجى الرجوع إلى sphx_glr_auto_examples_miscellaneous_plot_outlier_detection_bench.py لمثال يوضح كيفية تقييم خوارزميات كشف النُقاط الشاذة، باستخدام منحنيات ROC من metrics.RocCurveDisplay.

كشف البيانات الجديدة

لنأخذ مجموعة بيانات تتكون من n مُلاحظة من نفس التوزيع الذي يصفه p من الخصائص. دعونا الآن نضيف مُلاحظة واحدة أخرى إلى مجموعة البيانات هذه. هل المُلاحظة الجديدة مُختلفة جدًا عن الآخرين بحيث يُمكننا الشك في كونها مُنحرفة؟ (أي هل تأتي من نفس التوزيع؟) أو على العكس من ذلك، هل هي مُشابهة جدًا للآخرين بحيث لا يُمكن تمييزها عن المُلاحظات الأصلية؟ هذا هو السؤال الذي تتناوله أدوات وطرق كشف البيانات الجديدة.

بشكل عام، يتعلق الأمر بتعلم حدود قريبة تقريبية تُحدد محيط توزيع المُلاحظات الأولية، المُخططة في مساحة تضم p بعدًا. بعد ذلك، إذا كانت المُلاحظات الإضافية تقع ضمن المنطقة الفرعية المُحددة بالحدود، فيُفترض أنها تأتي من نفس التوزيع مثل المُلاحظات الأولية. من ناحية أخرى، إذا كانت تقع خارج الحدود، فيُمكننا القول إنها غير طبيعية بمستوى ثقة مُعين.

تم تقديم One-Class SVM من قبل شولكوف وآخرون. لهذا الغرض وتم تنفيذه في وحدة نمطية svm في كائن svm.OneClassSVM. يتطلب اختيار نواة ومُعامل قياسي لتحديد حد. عادة ما يتم اختيار نواة RBF على الرغم من عدم وجود صيغة أو خوارزمية دقيقة لتحديد مُعامل نطاقها. هذا هو الافتراضي في تنفيذ scikit-learn. يتوافق مُعامل "nu"، المعروف أيضًا باسم هامش One-Class SVM، مع احتمال العثور على مُلاحظة جديدة ولكنها طبيعية، خارج الحدود.

مراجع

* تقدير دعم توزيع الأبعاد العالية، شولكوف، برنارد، وآخرون. الحساب العصبي 13.7 (2001): 1443-1471.

أمثلة

* يُرجى الرجوع إلى sphx_glr_auto_examples_svm_plot_oneclass.py لتصور الحدود المُتعلمة حول بعض البيانات بواسطة كائن svm.OneClassSVM.

* sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py

تحجيم One-Class SVM

تم تنفيذ إصدار One-Class SVM الخطي عبر الإنترنت في linear_model.SGDOneClassSVM. ينمو هذا التنفيذ بشكل خطي مع عدد العينات ويُمكن استخدامه مع تقريب النواة لتقريب حل One-Class SVM المُستخدم مع نواة، والذي يكون تعقيده في أفضل الأحوال تربيعيًا في عدد العينات. يُرجى الرجوع إلى القسم sgd_online_one_class_svm لمزيد من التفاصيل.

أمثلة

* يُرجى الرجوع إلى sphx_glr_auto_examples_linear_model_plot_sgdocsvm_vs_ocsvm.py لتوضيح تقريب One-Class SVM المُستخدم مع النواة باستخدام linear_model.SGDOneClassSVM المُدمج مع تقريب النواة.

كشف النُقاط الشاذة
يكشف الشواذ  مشابه لكشف  في كون الهدف هو فصل نواة الملاحظات العادية عن بعض الملوثات، والتي تسمى *الشاذة*. ومع ذلك، في حالة كشف الشواذ، لا نملك مجموعة بيانات نظيفة تمثل مجموعة السكان من الملاحظات العادية التي يمكن استخدامها لتدريب أي أداة.

تناسب غلاف إهليلجي
---------------------

هناك طريقة شائعة لأداء كشف الشواذ وهي افتراض أن البيانات العادية تأتي من توزيع معروف (على سبيل المثال، البيانات موزعة بشكل غاوسي). من هذا الافتراض، نحاول عمومًا تحديد "شكل" البيانات، ويمكننا تحديد الملاحظات الشاذة على أنها ملاحظات تقف بعيدًا بما فيه الكفاية عن الشكل المناسب.

يوفر scikit-learn كائنًا covariance.EllipticEnvelope الذي يناسب تقديرًا قويًا للتباين المشترك للبيانات، وبالتالي يناسب إهليلجًا لنقاط البيانات المركزية، متجاهلاً النقاط خارج الوضع المركزي.

على سبيل المثال، بافتراض أن بيانات النقاط الداخلية موزعة بشكل غاوسي، فإنه سيقدر موقع النقاط الداخلية والتباين المشترك بطريقة قوية (أي بدون التأثر بالنقاط الشاذة). يتم استخدام مسافات ماهالانوبيس المستمدة من هذا التقدير لاستنباط مقياس للشذوذ. توضح هذه الاستراتيجية أدناه.

مثال
-----

* راجعsphx_glr_auto_examples_covariance_plot_mahalanobis_distances.py للحصول على توضيح للفرق بين استخدام تقدير قياسي (covariance.EmpiricalCovariance) أو تقدير قوي (covariance.MinCovDet) للموقع والتباين المشترك لتقييم درجة شذوذ الملاحظة.

* راجعsphx_glr_auto_examples_applications_plot_outlier_detection_wine.py للحصول على مثال على تقدير التباين المشترك القوي على مجموعة بيانات حقيقية.

المراجع
---------

* روسيو، ب. جي، فان دريسين، ك. "خوارزمية سريعة لمقدّر الحد الأدنى للتحديد المشترك" Technometrics 41(3)، 212 (1999)

غابة العزل
---------------------

هناك طريقة فعالة لأداء كشف الشواذ في مجموعات البيانات عالية الأبعاد هي استخدام الغابات العشوائية.
يعزل class='ensemble.IsolationForest' الملاحظات عن طريق اختيار ميزة بشكل عشوائي ثم اختيار قيمة تقسيم بشكل عشوائي بين الحد الأقصى والحد الأدنى لقيم الميزة المحددة.

بما أن التقسيم المتكرر يمكن تمثيله بهيكل شجري، فإن عدد الانقسامات المطلوبة لعزل عينة يساوي طول المسار من العقدة الجذرية إلى العقدة النهائية.

يمثل طول هذا المسار، بمتوسطه عبر غابة من الأشجار العشوائية مثل هذه، مقياسًا للطبيعية ودالة القرار الخاصة بنا.

تنتج التقسيمات العشوائية مسارات أقصر بشكل ملحوظ للشذوذ.
لذلك، عندما تنتج غابة من الأشجار العشوائية بشكل جماعي مسارات أقصر لمجموعات معينة من العينات، فمن المحتمل جدًا أن تكون هذه العينات شاذة.

يستند تنفيذ class='ensemble.IsolationForest' إلى مجموعة من class='tree.ExtraTreeRegressor'. وفقًا لورقة غابة العزل الأصلية، يتم تعيين العمق الأقصى لكل شجرة إلى math: \lceil \log_2(n) \rceil حيث math: n هو عدد العينات المستخدمة لبناء الشجرة (راجع (ليو وآخرون، 2008) لمزيد من التفاصيل).

توضح هذه الخوارزمية أدناه.

مثال
-----

* راجعsphx_glr_auto_examples_ensemble_plot_isolation_forest.py للحصول على توضيح لاستخدام IsolationForest.

* راجعsphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py لمقارنة class='ensemble.IsolationForest' مع class='neighbors.LocalOutlierFactor' و class='svm.OneClassSVM' (ضبطت لأداء مثل طريقة كشف الشواذ)، class='linear_model.SGDOneClassSVM'، وكشف الشواذ القائم على التباين المشترك باستخدام class='covariance.EllipticEnvelope'.

المراجع
---------

* ليو، في توني، تينغ، كاي مينغ، تشو، تشي هوا. "غابة العزل."

تعدين البيانات، 2008. ICDM'08. المؤتمر الدولي الثامن IEEE حول.

عامل الشذوذ المحلي
---------------------
هناك طريقة فعالة أخرى لأداء كشف الشواذ في مجموعات البيانات عالية الأبعاد بشكل معتدل هي استخدام خوارزمية عامل الشذوذ المحلي (LOF).

يحسب class='neighbors.LocalOutlierFactor' (LOF) خوارزمية درجة (تسمى عامل الشذوذ المحلي) تعكس درجة شذوذ الملاحظات.
إنه يقيس انحراف الكثافة المحلية لنقطة بيانات معينة فيما يتعلق بجيرانها. الفكرة هي اكتشاف العينات التي لها كثافة أقل بكثير من جيرانها.

في الممارسة العملية، يتم الحصول على الكثافة المحلية من أقرب k جار.
تساوي درجة LOF لملاحظة نسبة متوسط الكثافة المحلية لجيرانها k الأقرب وكثافتها المحلية: من المتوقع أن يكون للملاحظة العادية كثافة محلية مشابهة لتلك الخاصة بجيرانها، في حين أن البيانات الشاذة من المتوقع أن يكون لها كثافة محلية أقل بكثير.

يتم اختيار عدد الجيران k المعتبرين، (الاسم المستعار لمعلمة n_neighbors) عادةً 1) أكبر من الحد الأدنى لعدد الكائنات التي يجب أن تحتويها المجموعة، بحيث يمكن أن تكون الكائنات الأخرى شاذة محليًا بالنسبة لهذه المجموعة، و2) أصغر من الحد الأقصى لعدد الكائنات القريبة التي يمكن أن تكون شاذة محليًا.
في الممارسة العملية، لا تتوفر مثل هذه المعلومات بشكل عام، ويبدو أن اختيار n_neighbors=20 يعمل بشكل جيد بشكل عام.
عندما تكون نسبة الشواذ مرتفعة (أي أكثر من 10٪، كما في المثال أدناه)، يجب أن يكون n_neighbors أكبر (n_neighbors=35 في المثال أدناه).

تكمن قوة خوارزمية LOF في أنها تأخذ في الاعتبار الخصائص المحلية والعالمية لمجموعات البيانات: فيمكنها الأداء بشكل جيد حتى في مجموعات البيانات التي يكون فيها للعينات الشاذة كثافات أساسية مختلفة.
السؤال ليس هو مدى عزلة العينة، ولكن مدى عزلتها فيما يتعلق بالحي المحيط.

عند تطبيق LOF للكشف عن الشواذ، لا توجد طرق "التنبؤ" و"دالة القرار" و"score_samples"، ولكن فقط طريقة "fit_predict". يمكن الوصول إلى درجات الشذوذ لعينات التدريب من خلال سمة "negative_outlier_factor_".
لاحظ أنه يمكن استخدام "التنبؤ" و"دالة القرار" و"score_samples" على بيانات جديدة غير مرئية عند تطبيق LOF للكشف عن الأنماط، أي عندما يتم تعيين معلمة "novelty" إلى "True"، ولكن قد تختلف نتيجة "التنبؤ" عن نتيجة "fit_predict". راجع :ref: novelty_with_lof.


توضح هذه الاستراتيجية أدناه.

مثال
-----

* راجعsphx_glr_auto_examples_neighbors_plot_lof_outlier_detection.py للحصول على توضيح لاستخدام class='neighbors.LocalOutlierFactor'.

* راجعsphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py للحصول على مقارنة مع طرق اكتشاف الشذوذ الأخرى.

المراجع
---------

* بريونيج، كريجيل، نج، وساندر (2000)

كشف الأنماط باستخدام عامل الشذوذ المحلي
----------------------------------------

لاستخدام class='neighbors.LocalOutlierFactor' للكشف عن الأنماط، أي التنبؤ بالعلامات أو حساب درجة شذوذ البيانات الجديدة غير المرئية، يجب عليك إنشاء مثيل للمقدّر بمعلمة "novelty"
تعيين إلى "صحيح" قبل تناسب المقدّر::

تنبيه
------

عند تعيين "novelty" إلى "صحيح"، كن على دراية بأنه يجب عليك فقط استخدام "التنبؤ" و"دالة القرار" و"score_samples" على بيانات جديدة غير مرئية وليس على عينات التدريب، حيث قد يؤدي ذلك إلى نتائج خاطئة.
أي أن نتيجة "التنبؤ" لن تكون هي نفسها "fit_predict". يمكن دائمًا الوصول إلى درجات الشذوذ لعينات التدريب من خلال سمة "negative_outlier_factor_".

يوضح الشكل أدناه كشف الأنماط باستخدام عامل الشذوذ المحلي.

.. figure:: ../auto_examples/neighbors/images/sphx_glr_plot_lof_novelty_detection_001.png
    :target: ../auto_examples/neighbors/plot_lof_novelty_detection.html
    :align: center
    :scale: 75%