أهمية ميزة التبديل
==============================

تقنية تفتيش النموذج التي تقيس مساهمة كل ميزة في الأداء الإحصائي لنموذج مُدرب على مجموعة بيانات جدولة معينة. هذه التقنية مفيدة بشكل خاص للمُقدرات غير الخطية أو المعتمة، وتنطوي على خلط قيم ميزة واحدة بشكل عشوائي ومراقبة التدهور الناتج في نتيجة النموذج [1] _ . من خلال كسر العلاقة بين الميزة والهدف، نحدد مدى اعتماد النموذج على هذه الميزة المحددة.

في الأشكال التالية، نلاحظ تأثير تبديل الميزات على الارتباط بين الميزة والهدف وبالتالي على الأداء الإحصائي للنموذج.

في الشكل العلوي، نلاحظ أن تبديل ميزة تنبؤية يكسر الارتباط بين الميزة والهدف، وبالتالي ينخفض الأداء الإحصائي للنموذج. في الشكل السفلي، نلاحظ أن تبديل ميزة غير تنبؤية لا يقلل بشكل كبير من الأداء الإحصائي للنموذج.

تتمثل إحدى المزايا الرئيسية لأهمية ميزة التبديل في أنها لا تعتمد على النموذج، أي يمكن تطبيقها على أي مُقدر مُدرب. علاوة على ذلك، يمكن حسابه عدة مرات بتكرارات مختلفة للميزة، مما يوفر مقياسًا لتغاير التقديرات لأهمية الميزات بالنسبة للنموذج المدرب المحدد.

يوضح الشكل أدناه أهمية ميزة التبديل لمصنف غابة عشوائية مدرب على نسخة محسنة من مجموعة بيانات التيتانيك تحتوي على ميزتي "random_cat" و "random_num"، أي ميزة تصنيف وميزة رقمية غير مرتبطتين بأي طريقة مع متغير الهدف:

تحذير:

قد تكون الميزات التي تعتبر ذات **أهمية منخفضة بالنسبة لنموذج سيئ** (درجة تحقق منخفضة) **مهمة للغاية بالنسبة لنموذج جيد**. لذلك من المهم دائمًا تقييم القوة التنبؤية للنموذج باستخدام مجموعة بيانات محجوزة (أو الأفضل باستخدام التحقق من التداخل) قبل حساب الأهميات. لا تعكس أهمية التبديل القيمة التنبؤية الجوهرية لميزة بنفسها ولكن **مدى أهمية هذه الميزة لنموذج معين**.

تحسب دالة "permutation_importance" أهمية الميزة للمُقدرات لمجموعة بيانات معينة. يحدد معلمة "n_repeats" عدد المرات التي يتم فيها خلط ميزة بشكل عشوائي وإرجاع عينة من أهمية الميزة.

لنأخذ في الاعتبار نموذج الانحدار المدرب التالي::

  >>> from sklearn.datasets import load_diabetes
  >>> from sklearn.model_selection import train_test_split
  >>> from sklearn.linear_model import Ridge
  >>> diabetes = load_diabetes()
  >>> X_train, X_val, y_train, y_val = train_test_split(
  ...     diabetes.data, diabetes.target, random_state=0)
  ...
  >>> model = Ridge(alpha=1e-2).fit(X_train, y_train)
  >>> model.score(X_val, y_val)
  0.356...

أداؤه التحققي، الذي يتم قياسه عبر درجة R^2، أكبر بشكل ملحوظ من مستوى الفرصة. وهذا يجعل من الممكن استخدام دالة "permutation_importance" للتحقق من الميزات الأكثر تنبؤية::

  >>> from sklearn.inspection import permutation_importance
  >>> r = permutation_importance(model, X_val, y_val,
  ...                            n_repeats=30,
  ...                            random_state=0)
  ...
  >>> for i in r.importances_mean.argsort()[::-1]:
  ...     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:
  ...         print(f"{diabetes.feature_names[i]:<8}"
  ...               f"{r.importances_mean[i]:.3f}"
  ...               f" +/- {r.importances_std[i]:.3f}")
  ...
  s5      0.204 +/- 0.050
  bmi     0.176 +/- 0.048
  bp      0.088 +/- 0.033
  sex     0.056 +/- 0.023

لاحظ أن قيم الأهمية للميزات الأعلى تمثل جزءًا كبيرًا من النتيجة المرجعية البالغة 0.356.

يمكن حساب أهمية التبديل إما على مجموعة التدريب أو على مجموعة اختبار أو تحقق محجوزة. باستخدام مجموعة محجوزة، يمكن تسليط الضوء على الميزات التي تساهم بشكل أكبر في قوة تعميم النموذج قيد الفحص. قد تتسبب الميزات المهمة في مجموعة التدريب ولكن ليس في المجموعة المحجوزة في زيادة ملاءمة النموذج.

تعتمد أهمية ميزة التبديل على دالة النتيجة المحددة باستخدام حجة "التسجيل". يقبل هذا الحجة العديد من المُقيمين، وهو أكثر كفاءة من الناحية الحسابية من استدعاء دالة "permutation_importance" بشكل متسلسل بعدة مرات بمُقيم مختلف، حيث يعيد استخدام تنبؤات النموذج.

مثال على أهمية ميزة التبديل باستخدام مُقيمين متعددين:

في المثال أدناه، نستخدم قائمة من المقاييس، ولكن هناك تنسيقات إدخال أخرى ممكنة، كما هو موثق في: ref: `multimetric_scoring` .

    >>> scoring = ['r2', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error']
    >>> r_multi = permutation_importance(
    ...     model, X_val, y_val, n_repeats=30, random_state=0, scoring=scoring)
    ...
    >>> for metric in r_multi:
    ...     print(f"{metric}")
    ...     r = r_multi[metric]
    ...     for i in r.importances_mean.argsort()[::-1]:
    ...         if r.importances_mean[i] - 2 * r.importances_std[i] > 0:
    ...             print(f"    {diabetes.feature_names[i]:<8}"
    ...                   f"{r.importances_mean[i]:.3f}"
    ...                   f" +/- {r.importances_std[i]:.3f}")
    ...
    r2
        s5      0.204 +/- 0.050
        bmi     0.176 +/- 0.048
        bp      0.088 +/- 0.033
        sex     0.056 +/- 0.023
    neg_mean_absolute_percentage_error
        s5      0.081 +/- 0.020
        bmi     0.064 +/- 0.015
        bp      0.029 +/- 0.010
    neg_mean_squared_error
        s5      1013.866 +/- 246.445
        bmi     872.726 +/- 240.298
        bp      438.663 +/- 163.022
        sex     277.376 +/- 115.123

ترتيب الميزات هو نفسه تقريبًا لمقاييس مختلفة حتى إذا كانت نطاقات قيم الأهمية مختلفة جدًا. ومع ذلك، لا يتم ضمان ذلك وقد تؤدي المقاييس المختلفة إلى أهمية ميزة مختلفة بشكل كبير، خاصة بالنسبة للنماذج المدربة لمشكلات التصنيف غير المتوازنة، والتي قد يكون **اختيار المقياس التصنيفي فيها حاسمًا**.

مخطط خوارزمية أهمية التبديل
-----------------------------------------------

- المدخلات: نموذج تنبئي مدرب m، مجموعة بيانات جدولة (تدريب أو تحقق) D.
- احسب النتيجة المرجعية s للنموذج m على البيانات D (على سبيل المثال الدقة لمصنف أو R^2 لمُرجح).
- لكل ميزة j (عمود من D):

  - لكل تكرار k في {1، ...، K}:

    - قم بخلط العمود j من مجموعة البيانات D بشكل عشوائي لإنشاء إصدار تالف من البيانات يسمى ~D_ {k، j}.
    - احسب النتيجة s_ {k، j} للنموذج m على البيانات التالفة ~D_ {k، j}.

  - احسب الأهمية i_j للميزة j_f محددة على النحو التالي:

    .. math:: i_j = s - \frac{1}{K} \ sum_ {k = 1} ^ {K} s_ {k، j}

العلاقة بأهمية النقاء في الأشجار
----------------------------------------------

تقدم النماذج المستندة إلى الشجرة مقياسًا بديلاً لأهمية الميزة بناءً على الانخفاض المتوسط في النقاء (MDI). يتم تحديد النقاء بواسطة معيار التقسيم لأشجار القرار (Gini، أو سجل الخسارة، أو متوسط ​​خطأ المربعات). ومع ذلك، يمكن أن تعطي هذه الطريقة أهمية عالية للميزات التي قد لا تكون تنبؤية على بيانات غير مرئية عندما يكون النموذج مفرطًا في الملاءمة. من ناحية أخرى، تتجنب أهمية الميزة المستندة إلى التبديل هذه المشكلة، حيث يمكن حسابها على بيانات غير مرئية.

علاوة على ذلك، فإن أهمية الميزة المستندة إلى النقاء للأشجار **متحيزة بشدة** و **تفضل ميزات التعداد المرتفع** (عادة الميزات الرقمية) على ميزات التعداد المنخفض مثل الميزات الثنائية أو المتغيرات الفئوية بعدد صغير من الفئات الممكنة.

لا تظهر أهمية ميزة التبديل مثل هذا التحيز. بالإضافة إلى ذلك، يمكن حساب أهمية الميزة بالتبديل باستخدام أي مقياس للأداء على تنبؤات النموذج ويمكن استخدامه لتحليل أي فئة من فئات النماذج (وليس فقط النماذج المستندة إلى الشجرة).

يسلط المثال التالي الضوء على قيود أهمية الميزة المستندة إلى النقاء على النقيض من أهمية الميزة المستندة إلى التبديل: ref: `sphx_glr_auto_examples_inspection_plot_permutation_importance.py` .

القيم المضللة للميزات ذات الارتباط القوي
-------------------------------------------------

عندما تكون هناك ميزتان مرتبطتان، وإذا تم تبديل إحدى الميزتين، فسيظل بإمكان النموذج الوصول إلى الأخيرة من خلال ميزتها المرتبطة. يؤدي هذا إلى انخفاض قيمة الأهمية المبلغ عنها لكلتا الميزتين، على الرغم من أنهما قد تكونان *في الواقع* مهمتين.

يوضح الشكل أدناه أهمية ميزة التبديل لمصنف غابة عشوائية مدرب باستخدام مجموعة بيانات سرطان الثدي، والتي تحتوي على ميزات مرتبطة ارتباطًا وثيقًا. قد يوحي التفسير البسيط بأن جميع الميزات غير مهمة:

يمكن التعامل مع هذه المشكلة عن طريق تجميع الميزات المرتبطة واختيار ميزة واحدة فقط من كل مجموعة.

لمزيد من التفاصيل حول هذه الاستراتيجية، راجع المثال: ref: `sphx_glr_auto_examples_inspection_plot_permutation_importance_multicollinear.py` .

.. rubric:: أمثلة

* : ref: `sphx_glr_auto_examples_inspection_plot_permutation_importance.py`
* : ref: `sphx_glr_auto_examples_inspection_plot_permutation_importance_multicollinear.py`

.. rubric:: المراجع

.. [1] L. Breiman،: doi: `"Random Forests" <10.1023 / A: 1010933404324>`،
   التعلم الآلي، 45 (1)، 5-32، 2001.