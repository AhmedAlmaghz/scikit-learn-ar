

.. rubric:: Related examples

.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example demonstrates how to approximate a function with polynomials up to degree degree by using ridge regression. We show two different ways given n_samples of 1d points x_i:">

.. only:: html

  .. image:: /auto_examples/linear_model/images/thumb/sphx_glr_plot_polynomial_interpolation_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Polynomial and Spline interpolation</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_linear_model_plot_polynomial_interpolation.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example compares two different bayesian regressors:">

.. only:: html

  .. image:: /auto_examples/linear_model/images/thumb/sphx_glr_plot_ard_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Comparing Linear Bayesian Regressors</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_linear_model_plot_ard.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Here a sine function is fit with a polynomial of order 3, for values close to zero.">

.. only:: html

  .. image:: /auto_examples/linear_model/images/thumb/sphx_glr_plot_robust_fit_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_linear_model_plot_robust_fit.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Robust linear estimator fitting</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_linear_model_plot_robust_fit.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="SVCs aim to find a hyperplane that effectively separates the classes in their training data by maximizing the margin between the outermost data points of each class. This is achieved by finding the best weight vector w that defines the decision boundary hyperplane and minimizes the sum of hinge losses for misclassified samples, as measured by the hinge_loss function. By default, regularization is applied with the parameter C=1, which allows for a certain degree of misclassification tolerance.">

.. only:: html

  .. image:: /auto_examples/svm/images/thumb/sphx_glr_plot_svm_kernels_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Plot classification boundaries with different SVM Kernels</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_svm_plot_svm_kernels.py`

.. thumbnail-parent-div-close

.. raw:: html

    </div>

