

.. rubric:: Related examples

.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Investigating the Iris dataset, we see that sepal length, petal length and petal width are highly correlated. Sepal width is less redundant. Matrix decomposition techniques can uncover these latent patterns. Applying rotations to the resulting components does not inherently improve the predictive value of the derived latent space, but can help visualise their structure; here, for example, the varimax rotation, which is found by maximizing the squared variances of the weights, finds a structure where the second component only loads positively on sepal width.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_varimax_fa_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_varimax_fa.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Factor Analysis (with rotation) to visualize patterns</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_decomposition_plot_varimax_fa.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as the EmpiricalCovariance. It is unbiased, i.e. it converges to the true (population) covariance when given many observations. However, it can also be beneficial to regularize it, in order to reduce its variance; this, in turn, introduces some bias. This example illustrates the simple regularization used in shrunk_covariance estimators. In particular, it focuses on how to set the amount of regularization, i.e. how to choose the bias-variance trade-off.">

.. only:: html

  .. image:: /auto_examples/covariance/images/thumb/sphx_glr_plot_covariance_estimation_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example applies to olivetti_faces_dataset different unsupervised matrix decomposition (dimension reduction) methods from the module sklearn.decomposition (see the documentation chapter decompositions).">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_faces_decomposition_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Faces dataset decompositions</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Incremental principal component analysis (IPCA) is typically used as a replacement for principal component analysis (PCA) when the dataset to be decomposed is too large to fit in memory. IPCA builds a low-rank approximation for the input data using an amount of memory which is independent of the number of input data samples. It is still dependent on the input data features, but changing the batch size allows for control of memory usage.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_incremental_pca_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_incremental_pca.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Incremental PCA</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_decomposition_plot_incremental_pca.py`

.. thumbnail-parent-div-close

.. raw:: html

    </div>

