

.. rubric:: Related examples

.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="An example of estimating sources from noisy data.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_ica_blind_source_separation_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_ica_blind_source_separation.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Blind source separation using FastICA</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_decomposition_plot_ica_blind_source_separation.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble.">

.. only:: html

  .. image:: /auto_examples/ensemble/images/thumb/sphx_glr_plot_bias_variance_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_ensemble_plot_bias_variance.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Single estimator versus bagging: bias-variance decomposition</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_ensemble_plot_bias_variance.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="The Iris dataset represents 3 kind of Iris flowers (Setosa, Versicolour and Virginica) with 4 attributes: sepal length, sepal width, petal length and petal width.">

.. only:: html

  .. image:: /auto_examples/decomposition/images/thumb/sphx_glr_plot_pca_vs_lda_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Comparison of LDA and PCA 2D projection of Iris dataset</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Ridge regression is basically minimizing a penalised version of the least-squared function. The penalising shrinks the value of the regression coefficients. Despite the few data points in each dimension, the slope of the prediction is much more stable and the variance in the line itself is greatly reduced, in comparison to that of the standard linear regression">

.. only:: html

  .. image:: /auto_examples/linear_model/images/thumb/sphx_glr_plot_ols_ridge_variance_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_linear_model_plot_ols_ridge_variance.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Ordinary Least Squares and Ridge Regression Variance</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_linear_model_plot_ols_ridge_variance.py`

.. thumbnail-parent-div-close

.. raw:: html

    </div>

