

.. rubric:: Related examples

.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example compares non-nested and nested cross-validation strategies on a classifier of the iris data set. Nested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. Choosing the parameters that maximize non-nested CV biases the model to the dataset, yielding an overly-optimistic score.">

.. only:: html

  .. image:: /auto_examples/model_selection/images/thumb/sphx_glr_plot_nested_cross_validation_iris_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Nested versus non-nested cross-validation</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Plot the classification probability for different classifiers. We use a 3 class dataset, and we classify it with a Support Vector classifier, L1 and L2 penalized logistic regression (multinomial multiclass), a One-Vs-Rest version with logistic regression, and Gaussian process classification.">

.. only:: html

  .. image:: /auto_examples/classification/images/thumb/sphx_glr_plot_classification_probability_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Plot classification probability</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Once a binary classifier is trained, the predict method outputs class label predictions corresponding to a thresholding of either the decision_function or the predict_proba output. The default threshold is defined as a posterior probability estimate of 0.5 or a decision score of 0.0. However, this default strategy may not be optimal for the task at hand.">

.. only:: html

  .. image:: /auto_examples/model_selection/images/thumb/sphx_glr_plot_tuned_decision_threshold_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_model_selection_plot_tuned_decision_threshold.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Post-hoc tuning the cut-off point of decision function</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_model_selection_plot_tuned_decision_threshold.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="The most naive strategy to solve such a task is to independently train a binary classifier on each label (i.e. each column of the target variable). At prediction time, the ensemble of binary classifiers is used to assemble multitask prediction.">

.. only:: html

  .. image:: /auto_examples/multioutput/images/thumb/sphx_glr_plot_classifier_chain_yeast_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_multioutput_plot_classifier_chain_yeast.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Multilabel classification using a classifier chain</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_multioutput_plot_classifier_chain_yeast.py`

.. thumbnail-parent-div-close

.. raw:: html

    </div>

