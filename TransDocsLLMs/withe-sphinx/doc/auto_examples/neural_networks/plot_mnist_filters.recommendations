

.. rubric:: Related examples

.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example visualizes some training loss curves for different stochastic learning strategies, including SGD and Adam. Because of time-constraints, we use several small datasets, for which L-BFGS might be more suitable. The general trend shown in these examples seems to carry over to larger datasets, however.">

.. only:: html

  .. image:: /auto_examples/neural_networks/images/thumb/sphx_glr_plot_mlp_training_curves_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Compare Stochastic learning strategies for MLPClassifier</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Here we fit a multinomial logistic regression with L1 penalty on a subset of the MNIST digits classification task. We use the SAGA algorithm for this purpose: this a solver that is fast when the number of samples is significantly larger than the number of features and is able to finely optimize non-smooth objective functions which is the case with the l1-penalty. Test accuracy reaches &gt; 0.8, while weight vectors remains sparse and therefore more easily interpretable.">

.. only:: html

  .. image:: /auto_examples/linear_model/images/thumb/sphx_glr_plot_sparse_logistic_regression_mnist_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_linear_model_plot_sparse_logistic_regression_mnist.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">MNIST classification using multinomial logistic + L1</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_linear_model_plot_sparse_logistic_regression_mnist.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Demonstrate the resolution of a regression problem using a k-Nearest Neighbor and the interpolation of the target using both barycenter and constant weights.">

.. only:: html

  .. image:: /auto_examples/neighbors/images/thumb/sphx_glr_plot_regression_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_neighbors_plot_regression.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Nearest Neighbors regression</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_neighbors_plot_regression.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Plot decision function of a weighted dataset, where the size of points is proportional to its weight.">

.. only:: html

  .. image:: /auto_examples/svm/images/thumb/sphx_glr_plot_weighted_samples_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_svm_plot_weighted_samples.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">SVM: Weighted samples</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_svm_plot_weighted_samples.py`

.. thumbnail-parent-div-close

.. raw:: html

    </div>

