

.. rubric:: Related examples

.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="An example comparing nearest neighbors classification with and without Neighborhood Components Analysis.">

.. only:: html

  .. image:: /auto_examples/neighbors/images/thumb/sphx_glr_plot_nca_classification_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_neighbors_plot_nca_classification.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Comparing Nearest Neighbors with and without Neighborhood Components Analysis</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_neighbors_plot_nca_classification.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Simple usage of Support Vector Machines to classify a sample. It will plot the decision surface and the support vectors.">

.. only:: html

  .. image:: /auto_examples/svm/images/thumb/sphx_glr_plot_custom_kernel_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_svm_plot_custom_kernel.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">SVM with custom kernel</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_svm_plot_custom_kernel.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Show below is a logistic-regression classifiers decision boundaries on the first two dimensions (sepal length and width) of the iris dataset. The datapoints are colored according to their labels.">

.. only:: html

  .. image:: /auto_examples/linear_model/images/thumb/sphx_glr_plot_iris_logistic_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_linear_model_plot_iris_logistic.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Logistic Regression 3-class Classifier</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_linear_model_plot_iris_logistic.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as the EmpiricalCovariance. It is unbiased, i.e. it converges to the true (population) covariance when given many observations. However, it can also be beneficial to regularize it, in order to reduce its variance; this, in turn, introduces some bias. This example illustrates the simple regularization used in shrunk_covariance estimators. In particular, it focuses on how to set the amount of regularization, i.e. how to choose the bias-variance trade-off.">

.. only:: html

  .. image:: /auto_examples/covariance/images/thumb/sphx_glr_plot_covariance_estimation_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`

.. thumbnail-parent-div-close

.. raw:: html

    </div>

