

.. rubric:: Related examples

.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Example of Precision-Recall metric to evaluate classifier output quality.">

.. only:: html

  .. image:: /auto_examples/model_selection/images/thumb/sphx_glr_plot_precision_recall_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_model_selection_plot_precision_recall.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Precision-Recall</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_model_selection_plot_precision_recall.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example plots the covariance ellipsoids of each class and the decision boundary learned by LinearDiscriminantAnalysis (LDA) and QuadraticDiscriminantAnalysis (QDA). The ellipsoids display the double standard deviation for each class. With LDA, the standard deviation is the same for all the classes, while each class has its own standard deviation with QDA.">

.. only:: html

  .. image:: /auto_examples/classification/images/thumb/sphx_glr_plot_lda_qda_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_classification_plot_lda_qda.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Linear and Quadratic Discriminant Analysis with covariance ellipsoid</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_classification_plot_lda_qda.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as the EmpiricalCovariance. It is unbiased, i.e. it converges to the true (population) covariance when given many observations. However, it can also be beneficial to regularize it, in order to reduce its variance; this, in turn, introduces some bias. This example illustrates the simple regularization used in shrunk_covariance estimators. In particular, it focuses on how to set the amount of regularization, i.e. how to choose the bias-variance trade-off.">

.. only:: html

  .. image:: /auto_examples/covariance/images/thumb/sphx_glr_plot_covariance_estimation_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate.">

.. only:: html

  .. image:: /auto_examples/covariance/images/thumb/sphx_glr_plot_lw_vs_oas_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_covariance_plot_lw_vs_oas.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Ledoit-Wolf vs OAS estimation</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_covariance_plot_lw_vs_oas.py`

.. thumbnail-parent-div-close

.. raw:: html

    </div>

