

.. rubric:: Related examples

.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Shows the effect of collinearity in the coefficients of an estimator.">

.. only:: html

  .. image:: /auto_examples/linear_model/images/thumb/sphx_glr_plot_ridge_path_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_linear_model_plot_ridge_path.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Plot Ridge coefficients as a function of the regularization</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_linear_model_plot_ridge_path.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example illustrates differences between a kernel ridge regression and a Gaussian process regression.">

.. only:: html

  .. image:: /auto_examples/gaussian_process/images/thumb/sphx_glr_plot_compare_gpr_krr_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_gaussian_process_plot_compare_gpr_krr.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Comparison of kernel ridge and Gaussian process regression</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_gaussian_process_plot_compare_gpr_krr.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example compares 2 dimensionality reduction strategies:">

.. only:: html

  .. image:: /auto_examples/cluster/images/thumb/sphx_glr_plot_feature_agglomeration_vs_univariate_selection_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Feature agglomeration vs. univariate selection</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Ridge regression is basically minimizing a penalised version of the least-squared function. The penalising shrinks the value of the regression coefficients. Despite the few data points in each dimension, the slope of the prediction is much more stable and the variance in the line itself is greatly reduced, in comparison to that of the standard linear regression">

.. only:: html

  .. image:: /auto_examples/linear_model/images/thumb/sphx_glr_plot_ols_ridge_variance_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_linear_model_plot_ols_ridge_variance.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Ordinary Least Squares and Ridge Regression Variance</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_linear_model_plot_ols_ridge_variance.py`

.. thumbnail-parent-div-close

.. raw:: html

    </div>

