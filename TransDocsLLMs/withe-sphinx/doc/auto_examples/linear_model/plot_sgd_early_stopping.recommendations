

.. rubric:: Related examples

.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Gradient Boosting is an ensemble technique that combines multiple weak learners, typically decision trees, to create a robust and powerful predictive model. It does so in an iterative fashion, where each new stage (tree) corrects the errors of the previous ones.">

.. only:: html

  .. image:: /auto_examples/ensemble/images/thumb/sphx_glr_plot_gradient_boosting_early_stopping_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Early stopping in Gradient Boosting</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="This example focuses on model selection for Lasso models that are linear models with an L1 penalty for regression problems.">

.. only:: html

  .. image:: /auto_examples/linear_model/images/thumb/sphx_glr_plot_lasso_model_selection_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Lasso model selection: AIC-BIC / cross-validation</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_model_selection.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Comparing various online solvers">

.. only:: html

  .. image:: /auto_examples/linear_model/images/thumb/sphx_glr_plot_sgd_comparison_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_comparison.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Comparing various online solvers</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_comparison.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="histogram_based_gradient_boosting (HGBT) models may be one of the most useful supervised learning models in scikit-learn. They are based on a modern gradient boosting implementation comparable to LightGBM and XGBoost. As such, HGBT models are more feature rich than and often outperform alternative models like random forests, especially when the number of samples is larger than some ten thousands (see sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py).">

.. only:: html

  .. image:: /auto_examples/ensemble/images/thumb/sphx_glr_plot_hgbt_regression_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Features in Histogram Gradient Boosting Trees</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`

.. thumbnail-parent-div-close

.. raw:: html

    </div>

