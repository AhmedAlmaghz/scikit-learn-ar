

.. rubric:: Related examples

.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="When performing classification you often want to predict not only the class label, but also the associated probability. This probability gives you some kind of confidence on the prediction. However, not all classifiers provide well-calibrated probabilities, some being over-confident while others being under-confident. Thus, a separate calibration of predicted probabilities is often desirable as a postprocessing. This example illustrates two different methods for this calibration and evaluates the quality of the returned probabilities using Brier&#x27;s score (see https://en.wikipedia.org/wiki/Brier_score).">

.. only:: html

  .. image:: /auto_examples/calibration/images/thumb/sphx_glr_plot_calibration_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_calibration_plot_calibration.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Probability calibration of classifiers</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_calibration_plot_calibration.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Well calibrated classifiers are probabilistic classifiers for which the output of predict_proba can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the samples such that for the samples to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the positive class.">

.. only:: html

  .. image:: /auto_examples/calibration/images/thumb/sphx_glr_plot_compare_calibration_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_calibration_plot_compare_calibration.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Comparison of Calibration of Classifiers</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_calibration_plot_compare_calibration.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="When performing classification one often wants to predict not only the class label, but also the associated probability. This probability gives some kind of confidence on the prediction. This example demonstrates how to visualize how well calibrated the predicted probabilities are using calibration curves, also known as reliability diagrams. Calibration of an uncalibrated classifier will also be demonstrated.">

.. only:: html

  .. image:: /auto_examples/calibration/images/thumb/sphx_glr_plot_calibration_curve_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_calibration_plot_calibration_curve.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Probability Calibration curves</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_calibration_plot_calibration_curve.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="Plot the class probabilities of the first sample in a toy dataset predicted by three different classifiers and averaged by the VotingClassifier.">

.. only:: html

  .. image:: /auto_examples/ensemble/images/thumb/sphx_glr_plot_voting_probas_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_ensemble_plot_voting_probas.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Plot class probabilities calculated by the VotingClassifier</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_ensemble_plot_voting_probas.py`

.. thumbnail-parent-div-close

.. raw:: html

    </div>

