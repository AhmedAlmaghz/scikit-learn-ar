هذا نص بتنسيق RST أريد ترجمته إلى اللغة العربية، مع الحفاظ على الرموز الخاصة والرموز والمعادلات الرياضية والروابط والتاجات والشفرة البرمجية:

.. _computational_performance:

.. currentmodule:: sklearn

الأداء الحاسوبي

(ملاحظة: لم يتم ترجمة النص البرمجي والروابط والتاجات والرموز الخاصة والرموز والمعادلات الرياضية، بناءً على طلبك.)

للبعض من التطبيقات، يكون أداء (بشكل أساسي زمن الاستجابة والإنتاجية في وقت التوقع) المقدرين أمرًا بالغ الأهمية. قد يكون من المهم أيضًا النظر في إنتاجية التدريب ولكن هذا غالبًا ما يكون أقل أهمية في إعداد الإنتاج (حيث غالبًا ما يحدث في وضع عدم الاتصال).

سنراجع هنا أوامر الحجم التي يمكنك توقعها من عدد من مقدري scikit-learn في سياقات مختلفة ونقدم بعض النصائح والحيل للتغلب على اختناقات الأداء.

يتم قياس زمن استجابة التوقع بالوقت المنقضي الضروري لإجراء توقع (على سبيل المثال بالميكروثانية). غالبًا ما يُنظر إلى زمن الاستجابة على أنه توزيع، وغالبًا ما يركز مهندسو العمليات على زمن الاستجابة عند نسبة مئوية معينة من هذا التوزيع (على سبيل المثال، النسبة المئوية 90).

يتم تعريف إنتاجية التوقع على أنها عدد التوقعات التي يمكن للبرنامج تقديمها في فترة زمنية معينة (على سبيل المثال بالتوقعات في الثانية).

جانب مهم آخر من تحسين الأداء هو أنه يمكن أن يضر بدقة التوقع. في الواقع، غالبًا ما تعمل النماذج البسيطة (مثل الخطية بدلاً من غير الخطية، أو ذات عدد أقل من المعلمات) بشكل أسرع ولكنها ليست دائمًا قادرة على أخذ نفس الخصائص الدقيقة للبيانات في الاعتبار مثل النماذج الأكثر تعقيدًا.

زمن استجابة التوقع

    
(ملاحظة: لم يتم ترجمة الرموز الخاصة والرموز والمعادلات الرياضية والروابط والتاجات والشفرة البرمجية حسب طلبك)

إحدى الاهتمامات المباشرة التي قد تكون لدى المرء عند استخدام/اختيار مجموعة أدوات تعلم الآلة هي فترة الكمون التي يمكن فيها إجراء التنبؤات في بيئة الإنتاج.

العوامل الرئيسية التي تؤثر على فترة الكمون للتنبؤ هي:

1. عدد الميزات
2. تمثيل البيانات المدخلة وتفرقها
3. تعقيد النموذج
4. استخراج الميزات

المعلمة الرئيسية الأخيرة هي أيضًا إمكانية إجراء تنبؤات بشكل مجمّع أو بشكل فردي.

الوضع المجمّع مقابل الوضع الذري
........................

بشكل عام، إجراء التنبؤات بشكل مجمّع (عديد من الحالات في نفس الوقت) يكون أكثر كفاءة لعدد من الأسباب (قابلية التنبؤ بالتفرع، ذاكرة التخزين المؤقت لوحدة المعالجة المركزية، تحسينات مكتبات الجبر الخطي، إلخ). هنا نرى في إعداد مع ميزات قليلة أنه بغض النظر عن اختيار المقدّر، فإن الوضع المجمّع يكون دائمًا أسرع، وبالنسبة لبعضها بأمر إلى أمرين من الحجم:

.. |atomic_prediction_latency| image::  ../auto_examples/applications/images/sphx_glr_plot_prediction_latency_001.png
    :target: ../auto_examples/applications/plot_prediction_latency.html
    :scale: 80

.. centered:: |atomic_prediction_latency|

.. |bulk_prediction_latency| image::  ../auto_examples/applications/images/sphx_glr_plot_prediction_latency_002.png
    :target: ../auto_examples/applications/plot_prediction_latency.html
    :scale: 

.. centered:: |bulk_prediction_latency|

لاعتماد مقاييس مختلفة لمقدّرك لحالتك، يمكنك ببساطة تغيير معامل ``n_features`` في هذا المثال: :ref:`sphx_glr_auto_examples_applications_plot_prediction_latency.py`. هذا يجب أن يعطيك تقديرًا لترتيب حجم فترة كمون التنبؤ.

تكوين Scikit-learn لتقليل الحمل الزائد للتحقق
.........................................................

يتم إجراء بعض التحقق على البيانات في Scikit-learn مما يزيد من الحمل الزائد لكل استدعاء لوظيفة ``predict`` والوظائف المماثلة. على وجه الخصوص، يتضمن التحقق من أن الميزات هي قيمة محدودة (وليس NaN أو غير محدود) تمريرًا كاملاً على البيانات. إذا كنت تضمن أن بياناتك مقبولة، يمكنك قمع التحقق من القيمة المحدودة عن طريق تعيين متغير البيئة ``SKLEARN_ASSUME_FINITE`` إلى سلسلة غير فارغة قبل استيراد scikit-learn، أو تكوينه في Python باستخدام :func:`set_config`. للسماح بالتحكم أكثر من هذه الإعدادات العالمية، يتيح لك :func:`config_context` تعيين هذا التكوين ضمن سياق محدد::

  >>> import sklearn
  >>> with sklearn.config_context(assume_finite=True):
  ...     pass  # do learning/prediction here with reduced validation

لاحظ أن هذا سيؤثر على جميع استخدامات :func:`~utils.assert_all_finite` داخل السياق.

تأثير عدد الميزات
....................................

من الواضح أنه عندما يزداد عدد الميزات يزداد استهلاك الذاكرة لكل مثال. في الواقع، بالنسبة لمصفوفة من :math:`M` الحالات مع :math:`N` الميزات، يكون تعقيد المساحة في :math:`O(NM)`. من منظور الحوسبة، فهذا يعني أيضًا أن عدد العمليات الأساسية (على سبيل المثال، الضربات لمنتجات المتجهات-المصفوفات في النماذج الخطية) تزداد أيضًا. هنا رسم بياني لتطور فترة الكمون للتنبؤ بعدد الميزات:

.. |influence_of_n_features_on_latency| image::  ../auto_examples/applications/images/sphx_glr_plot_prediction_latency_003.png
    :target: ../auto_examples/applications/plot_prediction_latency.html
    :scale: 80

.. centered:: |influence_of_n_features_on_latency|

بشكل عام، يمكنك توقع أن يزداد وقت التنبؤ على الأقل بشكل خطي مع عدد الميزات (يمكن أن تحدث حالات غير خطية اعتمادًا على البصمة الإجمالية للذاكرة والمقدر).

تأثير تمثيل البيانات المدخلة
...........................................

يوفر Scipy هياكل بيانات المصفوفات المتفرقة التي تم تحسينها لتخزين البيانات المتفرقة. الميزة الرئيسية للتنسيقات المتفرقة هي أنك لا تخزن الأصفار، لذلك إذا كانت بياناتك متفرقة فإنك تستخدم ذاكرة أقل بكثير. ستأخذ قيمة غير صفرية في تمثيل متفرق (`CSR أو CSC <https://docs.scipy.org/doc/scipy/reference/sparse.html>`_) في المتوسط موضع واحد ذو رقم صحيح 32 بت + قيمة نقطة عائمة 64 بت + 32 بت إضافية لكل صف أو عمود في المصفوفة. يمكن أن يؤدي استخدام مدخلات متفرقة على نموذج خطي كثيف (أو متفرق) إلى تسريع التنبؤ بشكل كبير نظرًا لأن الميزات ذات القيمة غير الصفرية فقط تؤثر على حاصل الضرب النقطي وبالتالي توقعات النموذج. وبالتالي إذا كان لديك 100 غير صفرية في مساحة أبعادها 1e6، فأنت تحتاج فقط إلى 100 عملية ضرب وإضافة بدلاً من 1e6.

ومع ذلك، قد تستفيد الحسابات على التمثيل الكثيف من عمليات متجهية محسّنة للغاية ومتعددة الخيوط في BLAS، وتميل إلى التسبب في عدد أقل من حالات فشل ذاكرة التخزين المؤقت لوحدة المعالجة المركزية. لذا يجب أن تكون التفرق عالية جدًا (بنسبة 10٪ كحد أقصى، يجب التحقق منها حسب الجهاز) للتأكد من أن التمثيل المدخلي المتفرق أسرع من التمثيل المدخلي الكثيف على جهاز به العديد من وحدات المعالجة المركزية وتنفيذ BLAS محسّن.

فيما يلي رمزًا نموذجيًا لاختبار تفرق مدخلاتك::

    def sparsity_ratio(X):
        return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])
    print("input sparsity ratio:", sparsity_ratio(X))

كقاعدة عامة، يمكنك اعتبار أنه إذا كانت نسبة التفرق أكبر من 90٪، فمن المحتمل أن تستفيد من التنسيقات المتفرقة. تحقق من توثيق تنسيقات المصفوفات المتفرقة لـ Scipy `documentation <https://docs.scipy.org/doc/scipy/reference/sparse.html>`_ لمزيد من المعلومات حول كيفية بناء (أو تحويل بياناتك إلى) تنسيقات المصفوفات المتفرقة. في معظم الحالات، تعمل تنسيقات ``CSR`` و``CSC`` بشكل أفضل.

تأثير تعقيد النموذج
..................................

بشكل عام، عندما يزداد تعقيد النموذج، من المفترض أن تزداد القوة التنبؤية وفترة الكمون. زيادة القوة التنبؤية عادة ما تكون مثيرة للاهتمام، ولكن بالنسبة للعديد من التطبيقات، من الأفضل ألا نزيد فترة الكمون للتنبؤ كثيرًا. سنراجع الآن هذه الفكرة لمختلف أسر النماذج الخاضعة للإشراف.

لنموذج :mod:`sklearn.linear_model` (مثل Lasso، ElasticNet، SGDClassifier/Regressor، Ridge & RidgeClassifier، PassiveAggressiveClassifier/Regressor، LinearSVC، LogisticRegression...)، يتم تطبيق دالة القرار نفسها وقت التنبؤ (الناتج الدالّي)، لذلك يجب أن تكون الكمونية مكافئة.

فيما يلي مثال باستخدام :class:`~linear_model.SGDClassifier` مع عقوبة ``elasticnet``. يتم التحكم في قوة التقييد عالميًا بواسطة معامل ``alpha``. باستخدام ``alpha`` عالي بما فيه الكفاية، يمكن للمرء بعد ذلك زيادة معامل ``l1_ratio`` لـ ``elasticnet`` لفرض مستويات مختلفة من التفرق في معاملات النموذج. يتم تفسير التفرق العالي هنا على أنه تعقيد أقل للنموذج لأننا نحتاج إلى معاملات أقل لوصفه بشكل كامل. بالطبع يؤثر التفرق بدوره على وقت التنبؤ حيث يستغرق ناتج الدالّي المتفرق وقتًا يتناسب تقريبًا مع عدد المعاملات غير الصفرية.

.. |en_model_complexity| image::  ../auto_examples/applications/images/sphx_glr_plot_model_complexity_influence_001.png
    :target: ../auto_examples/applications/plot_model_complexity_influence.html
    :scale: 80

.. centered:: |en_model_complexity|

لخوارزميات عائلة :mod:`sklearn.svm` مع نواة غير خطية، ترتبط الكمونية بعدد نواقل الدعم (كلما قل عددها، كان ذلك أسرع). يجب أن ينمو (بشكل غير مباشر) الكمونية والإنتاجية بشكل متساوٍ مع عدد نواقل الدعم في نموذج SVC أو SVR. ستؤثر النواة أيضًا على الكمونية حيث يتم استخدامها لحساب إسقاط متجه الإدخال مرة واحدة لكل ناقل دعم. في الرسم البياني التالي، تم استخدام معامل ``nu`` لـ :class:`~svm.NuSVR` للتأثير على عدد نواقل الدعم.

.. |nusvr_model_complexity| image::  ../auto_examples/applications/images/sphx_glr_plot_model_complexity_influence_002.png
    :target: ../auto_examples/applications/plot_model_complexity_influence.html
    :scale: 80

.. centered:: |nusvr_model_complexity|

لـ :mod:`sklearn.ensemble` من الأشجار (مثل RandomForest، GBT، ExtraTrees، إلخ)، يلعب عدد الأشجار وعمقها الدور الأكثر أهمية. يجب أن تتناسب الكمونية والإنتاجية بشكل خطي مع عدد الأشجار. في هذه الحالة، استخدمنا مباشرة معامل ``n_estimators`` لـ :class:`~ensemble.GradientBoostingRegressor`.

.. |gbt_model_complexity| image::  ../auto_examples/applications/images/sphx_glr_plot_model_complexity_influence_003.png
    :target: ../auto_examples/applications/plot_model_complexity_influence.html
    :scale: 80

.. centered:: |gbt_model_complexity|

في أي حالة، يجب عليك أن تكون حذرًا من أن تقليل تعقيد النموذج يمكن أن يؤدي إلى انخفاض الدقة كما هو مذكور أعلاه. على سبيل المثال، يمكن التعامل مع مشكلة غير قابلة للفصل بشكل خطي باستخدام نموذج خطي سريع، ولكن من المحتمل جدًا أن تتأثر قوة التنبؤ في العملية.

كمونية استخراج الميزة
..........................

عادةً ما تكون معظم نماذج scikit-learn سريعة جدًا لأنها يتم تنفيذها إما مع امتدادات Cython المجمعة أو المكتبات الحاسوبية المحسنة. من ناحية أخرى، في العديد من التطبيقات الواقعية، تحكم عملية استخراج الميزة (أي تحويل البيانات الأولية مثل صفوف قاعدة البيانات أو حزم الشبكة إلى صفائف Numpy) وقت التنبؤ الكلي. على سبيل المثال، في مهمة تصنيف النصوص لرويترز، فإن التحضير الكامل (قراءة ملفات SGML وتحليلها، وتجزئة النص وتجميعه في مساحة متجه مشتركة) يستغرق وقتًا من 100 إلى 500 مرة أكثر من وقت التنبؤ الفعلي، اعتمادًا على النموذج المختار.

.. |prediction_time| image::  ../auto_examples/applications/images/sphx_glr_plot_out_of_core_classification_004.png
  :target: ../auto_examples/applications/plot_out_of_core_classification.html
  :scale: 80

.. centered:: |prediction_time|

في العديد من الحالات، يُنصح بتوقيت عملية استخراج الميزة بشكل دقيق للغاية لأنها قد تكون مكانًا جيدًا لبدء التحسين عندما يكون وقت الاستجابة الكلي بطيئًا جدًا لتطبيقك.

إنتاجية التنبؤ

    

  هذا نص بتنسيق RST أريد ترجمته إلى اللغة العربية، مع الحفاظ على الرموز الخاصة، والرموز والمعادلات الرياضية، والروابط، والتاجات، والشفرة البرمجية كما هي:

----------------------

تعد المقياس الآخر المهم الذي يجب الانتباه إليه عند تحديد حجم أنظمة الإنتاج هو معدل الإنتاج، أي عدد التنبؤات التي يمكنك إجراؤها في فترة زمنية معينة. فيما يلي مقياس أداء من مثال :ref:`sphx_glr_auto_examples_applications_plot_prediction_latency.py` الذي يقيس هذه الكمية لعدد من مقدري التنبؤ على بيانات اصطناعية:

.. |throughput_benchmark| image::  ../auto_examples/applications/images/sphx_glr_plot_prediction_latency_004.png
    :target: ../auto_examples/applications/plot_prediction_latency.html
    :scale: 80

.. centered:: |throughput_benchmark|

يتم تحقيق هذه المعدلات الإنتاجية في عملية واحدة. طريقة واضحة لزيادة معدل الإنتاج لتطبيقك هي توليد عمليات إضافية (عادة ما تكون عمليات في بايثون بسبب `GIL <https://wiki.python.org/moin/GlobalInterpreterLock>`_) التي تشارك نفس النموذج. يمكن أيضًا إضافة آلات لتوزيع الحمل. يخرج شرح تفصيلي حول كيفية تحقيق ذلك عن نطاق هذه الوثائق.

نصائح وحيل
------------

مكتبات الجبر الخطي
.........................

نظرًا لأن سكيكيت-ليرن يعتمد بشكل كبير على مكتبات نيمباي/سايباي والجبر الخطي بشكل عام، فمن المنطقي الانتباه بشكل صريح لإصدارات هذه المكتبات. أساسًا، يجب التأكد من أن نيمباي مبني باستخدام مكتبة `BLAS <https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms>`_ / `LAPACK <https://en.wikipedia.org/wiki/LAPACK>`_ محسّنة.

لا تستفيد جميع النماذج من تطبيقات BLAS وLAPACK المحسّنة. على سبيل المثال، لا تعتمد النماذج القائمة على أشجار القرار (العشوائية) عادة على مكالمات BLAS في حلقاتها الداخلية، ولا تعتمد آلات المتجهات الداعمة (SVMs) القائمة على النواة (``SVC``، ``SVR``، ``NuSVC``، ``NuSVR``). من ناحية أخرى، سوف يستفيد نموذج خطي مُنفّذ مع مكالمة BLAS DGEMM (عبر ``numpy.dot``) بشكل كبير من تطبيق BLAS المحسّن، مما يؤدي إلى تسريع كبير مقارنةً بتطبيق BLAS غير المحسّن.

يمكنك عرض تطبيق BLAS / LAPACK الذي تستخدمه عملية تثبيت نيمباي / سايباي / سكيكيت-ليرن باستخدام الأمر التالي::

    python -c "import sklearn; sklearn.show_versions()"

تتضمن تطبيقات BLAS / LAPACK المحسّنة:

- Atlas (تحتاج إلى ضبط خاص بالأجهزة عن طريق إعادة البناء على الجهاز الهدف)
- OpenBLAS
- MKL
- أطر عمل Apple Accelerate و vecLib (OSX فقط)

يمكن العثور على مزيد من المعلومات في صفحة التثبيت `NumPy <https://numpy.org/install/>`_ وهذه `المدونة <https://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/>`_ من دانيال نوري والتي تحتوي على بعض تعليمات التثبيت خطوة بخطوة لـ Debian / Ubuntu.

.. _working_memory:

تحديد ذاكرة العمل
........................

تتضمن بعض الحسابات عند تنفيذها باستخدام عمليات نيمباي النمطية استخدام كمية كبيرة من الذاكرة المؤقتة. هذا قد يؤدي إلى استنزاف ذاكرة النظام. حيث يمكن إجراء العمليات في أجزاء ذات حجم ثابت، نحن نحاول القيام بذلك، ونسمح للمستخدم بالإشارة إلى الحجم الأقصى لذاكرة العمل هذه (مع الافتراضي إلى 1 غيغابايت) باستخدام :func:`set_config` أو :func:`config_context`. يقترح المثال التالي تحديد ذاكرة العمل المؤقتة إلى 128 ميغابايت::

  >>> import sklearn
  >>> with sklearn.config_context(working_memory=128):
  ...     pass  # do chunked work here

مثال على عملية مجزأة تلتزم بهذا الإعداد هو :func:`~metrics.pairwise_distances_chunked`، والذي يسهل حساب التخفيضات الصفية لمصفوفة مسافة زوجية.

ضغط النموذج
..................

ضغط النموذج في سكيكيت-ليرن يتعلق بالنماذج الخطية فقط في الوقت الحالي. في هذا السياق، يعني أننا نريد التحكم في تماثل النموذج (أي عدد الإحداثيات غير الصفرية في متجهات النموذج). من الجيد عمومًا دمج تماثل النموذج مع تمثيل البيانات النادرة.

فيما يلي مثال على الكود يوضح استخدام طريقة ``sparsify()``::

    clf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)
    clf.fit(X_train, y_train).sparsify()
    clf.predict(X_test)

في هذا المثال، نفضل استخدام عقوبة ``elasticnet`` لأنها غالبًا ما تكون حل وسط جيد بين إحكام النموذج وقوة التنبؤ. يمكن أيضًا ضبط معلم ``l1_ratio`` (بالاشتراك مع قوة التعديل ``alpha``) للتحكم في هذا التوازن.

يؤدي اختبار `قياسي <https://github.com/scikit-learn/scikit-learn/blob/main/benchmarks/bench_sparsify.py>`_ على بيانات اصطناعية إلى انخفاض بنسبة 30% في زمن الاستجابة عندما يكون النموذج والمدخل كلاهما نادران (مع نسبة معاملات غير صفرية 0.000024 و 0.027400 على التوالي). قد تختلف النتائج اعتمادًا على تماثل وحجم بياناتك ونموذجك.
علاوة على ذلك، يمكن أن يكون التخفيف مفيدًا جدًا لتقليل استخدام الذاكرة لنماذج التنبؤ المنشورة على خوادم الإنتاج.

إعادة تشكيل النموذج
................

يعني إعادة تشكيل النموذج تحديد جزء فقط من الميزات المتاحة لتناسب النموذج. بعبارة أخرى، إذا تجاهل النموذج الميزات أثناء مرحلة التعلم، فيمكننا بعد ذلك إزالة تلك الميزات من المدخل. لهذا العديد من الفوائد. أولاً، يقلل من زيادة ذاكرة (وبالتالي زمن) النموذج نفسه. يسمح أيضًا بإزالة مكونات تحديد الميزة الصريحة في خط الأنابيب بمجرد معرفة الميزات التي يجب الاحتفاظ بها من تشغيل سابق. أخيرًا، يمكن أن يساعد في تقليل وقت المعالجة واستخدام I/O في طبقات الوصول إلى البيانات واستخراج الميزات عن طريق عدم جمع وبناء ميزات يتجاهلها النموذج. على سبيل المثال، إذا جاءت البيانات الأولية من قاعدة بيانات، فيمكنها تمكين كتابة استعلامات أبسط وأسرع أو تقليل استخدام I/O عن طريق جعل الاستعلامات تعيد سجلات أخف.
في الوقت الحالي، يجب إجراء إعادة التشكيل يدويًا في سكيكيت-ليرن. في حالة الإدخال النادر (خاصةً في تنسيق ``CSR``)، يكون من الكافي عادةً عدم إنشاء الميزات ذات الصلة، تاركًا أعمدتها فارغة.

روابط
......

- :ref:`وثائق أداء مطور سكيكيت-ليرن <performance-howto>`
- `وثائق تنسيقات مصفوفة سايباي النادرة <https://docs.scipy.org/doc/scipy/reference/sparse.html>`_
