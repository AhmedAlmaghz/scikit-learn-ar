.. _feature_extraction:

==================
استخراج الميزات
==================

.. currentmodule:: sklearn.feature_extraction

يمكن استخدام وحدة :mod:`sklearn.feature_extraction` لاستخراج الميزات بتنسيق مدعوم من خوارزميات التعلم الآلي من مجموعات البيانات التي تتكون من تنسيقات مثل النص والصورة.

.. note::

   يختلف استخراج الميزات اختلافًا كبيرًا عن :ref:`feature_selection`:
   الأول يتكون من تحويل البيانات التعسفية، مثل النص أو الصور، إلى ميزات رقمية قابلة للاستخدام للتعلم الآلي.
   الأخير هو تقنية تعلم آلي يتم تطبيقها على هذه الميزات.

.. _dict_feature_extraction:

تحميل الميزات من القواميس
===========================

يمكن استخدام الفئة :class:`DictVectorizer` لتحويل مصفوفات الميزات الممثلة كقوائم من كائنات Python القياسية ``dict`` إلى التمثيل NumPy / SciPy المستخدم من قبل تقديرات scikit-learn.

على الرغم من أنها ليست سريعة بشكل خاص في المعالجة، إلا أن ``dict`` في Python لها مزايا كونها مريحة في الاستخدام، وكونها متفرقة (لا يلزم تخزين الميزات الغائبة) وتخزين أسماء الميزات بالإضافة إلى القيم.

تنفذ :class:`DictVectorizer` ما يسمى برمجة واحد من K أو "one-hot" للميزات التصنيفية (المعروفة أيضًا بالميزات الاسمية أو المنفصلة). الميزات التصنيفية هي أزواج "سمة-قيمة" حيث تكون القيمة مقيدة بقائمة من الإمكانيات المنفصلة دون ترتيب (مثل معرفات الموضوع، أنواع الكائنات، العلامات، الأسماء ...).

في ما يلي، "المدينة" هي سمة تصنيفية بينما "درجة الحرارة" هي سمة رقمية تقليدية::

  >>> measurements = [
  ...     {'city': 'Dubai', 'temperature': 33.},
  ...     {'city': 'London', 'temperature': 12.},
  ...     {'city': 'San Francisco', 'temperature': 18.},
  ... ]

  >>> from sklearn.feature_extraction import DictVectorizer
  >>> vec = DictVectorizer()

  >>> vec.fit_transform(measurements).toarray()
 array([[ 1.,  0.,  0., 33.],
         [ 0.,  1.,  0., 12.],
         [ 0.,  0.,  1., 18.]])

  >>> vec.get_feature_names_out()
 array(['city=Dubai', 'city=London', 'city=San Francisco', 'temperature'], ...)

تقبل :class:`DictVectorizer` قيم نصية متعددة لميزة واحدة، مثل، على سبيل المثال، فئات متعددة لفيلم.

افترض أن قاعدة البيانات تصنف كل فيلم باستخدام بعض الفئات (غير الإلزامية)
وسنة إصداره.

    >>> movie_entry = [{'category': ['thriller', 'drama'], 'year': 2003},
    ...                {'category': ['animation', 'family'], 'year': 2011},
    ...                {'year': 1974}]
    >>> vec.fit_transform(movie_entry).toarray()
    array([[0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 2.003e+03],
           [1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 2.011e+03],
           [0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.974e+03]])
    >>> vec.get_feature_names_out()
    array(['category=animation', 'category=drama', 'category=family',
           'category=thriller', 'year'], ...)
    >>> vec.transform({'category': ['thriller'],
    ...                'unseen_feature': '3'}).toarray()
    array([[0., 0., 0., 1., 0.]])

:class:`DictVectorizer` هي أيضًا تحويل تمثيل مفيد
لتدريب تصنيفات التسلسل في نماذج معالجة اللغة الطبيعية
التي تعمل عادةً عن طريق استخراج نوافذ الميزات حول
كلمة اهتمام معينة.

على سبيل المثال، لنفترض أن لدينا خوارزمية أولى تستخرج علامات جزء من الكلام (Part of Speech)
التي نريد استخدامها كعلامات تكميلية لتدريب
مصنف التسلسل (مثل chunkier). يمكن أن يكون القاموس التالي
نافذة من الميزات المستخرجة حول الكلمة "sat" في الجملة
"The cat sat on the mat."::

  >>> pos_window = [
  ...     {
  ...         'word-2': 'the',
  ...         'pos-2': 'DT',
  ...         'word-1': 'cat',
  ...         'pos-1': 'NN',
  ...         'word+1': 'on',
  ...         'pos+1': 'PP',
  ...     },
  ...     # في تطبيق حقيقي، سيستخرج المرء العديد من هذه القواميس
  ... ]

يمكن تحويل هذا الوصف إلى مصفوفة متفرقة ثنائية الأبعاد
مناسبة للتغذية في مصنف (ربما بعد أن يتم ضخها في
:class:`~text.TfidfTransformer` للتطبيع)::

  >>> vec = DictVectorizer()
  >>> pos_vectorized = vec.fit_transform(pos_window)
  >>> pos_vectorized
  <Compressed Sparse...dtype 'float64'
    with 6 stored elements and shape (1, 6)>
  >>> pos_vectorized.toarray()
 array([[1., 1., 1., 1., 1., 1.]])
  >>> vec.get_feature_names_out()
 array(['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat',
         'word-2=the'], ...)

كما تتخيل، إذا استخرج المرء مثل هذا السياق حول كل فرد
كلمة من مجموعة من المستندات ستكون النتيجة مصفوفة واسعة جدًا
(العديد من الميزات الفريدة) مع معظمها يتم تقييمه على الصفر في معظم الأحيان.
حتى يتمكن هيكل البيانات الناتج من التناسب مع الذاكرة
تستخدم فئة ``DictVectorizer`` مصفوفة ``scipy.sparse`` بشكل افتراضي
بدلاً من ``numpy.ndarray``.


.. _feature_hashing:

تجزئة الميزات
========

.. currentmodule:: sklearn.feature_extraction

فئة :class:`FeatureHasher` هي أداة تحويل عالية السرعة ومنخفضة الذاكرة تستخدم تقنية تعرف باسم "تجزئة الميزة" (`feature hashing <https://en.wikipedia.org/wiki/Feature_hashing>`_)، أو "حيلة التجزئة". بدلاً من إنشاء جدول تجزئة للميزات التي تم مواجهتها في التدريب، كما تفعل أدوات التحويل، فإن مثيلات :class:`FeatureHasher` تطبق دالة تجزئة على الميزات لتحديد مؤشر العمود الخاص بها في مصفوفات العينات مباشرة. والنتيجة هي زيادة السرعة وتقليل استخدام الذاكرة، على حساب قابلية التفتيش؛ لا يتذكر برنامج التجزئة شكل ميزات الإدخال وليس لديه طريقة "inverse_transform".

نظرًا لأن دالة التجزئة قد تسبب تصادمات بين الميزات (غير ذات الصلة)، يتم استخدام دالة تجزئة موقعة ويحدد علامة قيمة التجزئة علامة القيمة المخزنة في المصفوفة الناتجة لميزة ما. بهذه الطريقة، من المرجح أن تلغي التصادمات بعضها البعض بدلاً من تراكم الخطأ، والمتوسط المتوقع لأي قيمة ميزة ناتجة هو صفر. يتم تمكين هذه الآلية بشكل افتراضي مع ``alternate_sign=True`` وهي مفيدة بشكل خاص لأحجام جداول التجزئة الصغيرة (``n_features < 10000``). بالنسبة لأحجام جداول التجزئة الكبيرة، يمكن تعطيلها، للسماح بتمرير الإخراج إلى أدوات التقدير مثل :class:`~sklearn.naive_bayes.MultinomialNB` أو أدوات اختيار الميزات مثل :class:`~sklearn.feature_selection.chi2` التي تتوقع مدخلات غير سالبة.

تقبل فئة :class:`FeatureHasher` إما تعيينات (مثل ``dict`` في Python والمتغيرات الخاصة بها في وحدة ``collections``)، أو أزواج ``(feature, value)``، أو سلاسل أحرف، اعتمادًا على مُعطى البناء ``input_type``. يتم التعامل مع التعيينات على أنها قوائم لأزواج ``(feature, value)``، بينما تحتوي سلاسل الأحرف المفردة على قيمة ضمنية تبلغ 1، لذلك يتم تفسير ``['feat1', 'feat2', 'feat3']`` على أنه ``[('feat1', 1), ('feat2', 1), ('feat3', 1)]``. إذا حدثت ميزة واحدة عدة مرات في عينة، فسيتم جمع القيم المرتبطة بها (لذلك يصبح ``('feat', 2)`` و ``('feat', 3.5)`` ``('feat', 5.5)``). الإخراج من :class:`FeatureHasher` هو دائمًا مصفوفة ``scipy.sparse`` في تنسيق CSR.

يمكن استخدام تجزئة الميزات في تصنيف المستندات، ولكن على عكس :class:`~text.CountVectorizer`، لا تقوم فئة :class:`FeatureHasher` بتقسيم الكلمات أو أي معالجات مسبقة أخرى باستثناء ترميز Unicode-to-UTF-8؛ انظر :ref:`hashing_vectorizer` أدناه، لأداة تجزئة/مفككة مجمعة.

على سبيل المثال، ضع في اعتبارك مهمة معالجة اللغة الطبيعية على مستوى الكلمة والتي تحتاج إلى استخراج الميزات من أزواج ``(token, part_of_speech)``. يمكن استخدام وظيفة مولد Python لاستخراج الميزات::

 def token_features(token, part_of_speech):
      if token.isdigit():
          yield "numeric"
      else:
          yield "token={}".format(token.lower())
          yield "token,pos={},{}".format(token, part_of_speech)
      if token[0].isupper():
          yield "uppercase_initial"
      if token.isupper():
          yield "all_uppercase"
      yield "pos={}".format(part_of_speech)

بعد ذلك، يمكن إنشاء ``raw_X`` المراد تغذيته إلى ``FeatureHasher.transform`` باستخدام::

  raw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)

ويمكن تغذيته إلى أداة تجزئة باستخدام::

 hasher = FeatureHasher(input_type='string')
  X = hasher.transform(raw_X)

للحصول على مصفوفة ``scipy.sparse`` ``X``.

لاحظ استخدام فهم المولد، الذي يقدم الكسل في استخراج الميزة:
يتم معالجة الرموز فقط عند الطلب من أداة التجزئة.

.. dropdown:: تفاصيل التنفيذ

  تستخدم فئة :class:`FeatureHasher` النوع الموقع 32 بت من MurmurHash3. نتيجة لذلك (وبسبب القيود في ``scipy.sparse``)، فإن الحد الأقصى لعدد الميزات المدعوم حاليًا هو :math:`2^{31} - 1`.

  كانت الصياغة الأصلية لحيلة التجزئة بواسطة Weinberger et al. تستخدم دالتي تجزئة منفصلتين :math:`h` و :math:`\xi` لتحديد مؤشر العمود وعلامة الميزة، على التوالي. يعمل التنفيذ الحالي تحت افتراض أن بت علامة MurmurHash3 مستقل عن البتات الأخرى.

  نظرًا لأن وحدة بسيطة تستخدم لتحويل دالة التجزئة إلى فهرس عمود، فمن المستحسن استخدام قوة اثنين كمعلمة ``n_features``؛ وإلا فلن يتم تعيين الميزات بالتساوي إلى الأعمدة.

  .. rubric:: المراجع

  * `MurmurHash3 <https://github.com/aappleby/smhasher>`_.


.. rubric:: المراجع

* كيلان وينبرغر، أنيربان داسجوبتا، جون لانجفورد، أليكس سماولا وجوش أتينبيرغ (2009). `تجزئة الميزة للتعلم متعدد المهام على نطاق واسع <https://alex.smola.org/papers/2009/Weinbergeretal09.pdf>`_. إجراءات ICML.

.. _text_feature_extraction:

استخراج ميزة النص
=======================

.. currentmodule:: sklearn.feature_extraction.text


تمثيل حقيبة الكلمات
-------------------------------

تحليل النص هو مجال تطبيق رئيسي لخوارزميات التعلم الآلي. ومع ذلك، لا يمكن تغذية البيانات الأولية، وهي تسلسل من الرموز، مباشرة إلى الخوارزميات نفسها لأن معظمها يتوقع متجهات ميزات عددية ذات حجم ثابت بدلاً من مستندات النص الأولية ذات الطول المتغير.

لمعالجة هذا الأمر، يوفر sklearn أدوات للأكثر شيوعًا طرق استخراج الميزات العددية من محتوى النص، وهي:

- **تجزئة** السلاسل ومنح معرّف صحيح لكل رمز ممكن، على سبيل المثال باستخدام المسافات البيضاء وعلامات الترقيم كفواصل بين الرموز.

- **عد** مرات حدوث الرموز في كل مستند.

- **تطبيع** وترجيح مع أهمية متناقصة للرموز التي تحدث في أغلبية العينات / المستندات.

في هذا المخطط، يتم تعريف الميزات والعينات على النحو التالي:

- كل **تكرار فردي للرمز** (مُطبع أم لا) يتم التعامل معه على أنه **ميزة**.

- يتم اعتبار متجه كل تكرارات الرمز لمستند معين عينة متعددة المتغيرات.

وبالتالي، يمكن تمثيل مجموعة من المستندات بواسطة مصفوفة تحتوي على صف واحد لكل مستند وعمود واحد لكل رمز (على سبيل المثال، كلمة) موجود في المجموعة.

نحن نسمي ** Vectorizationالنقل إلى متجه** العملية العامة لتحويل مجموعة من المستندات النصية إلى متجهات ميزات رقمية. تسمى هذه الاستراتيجية المحددة (التجزئة والعد والتطبيع) تمثيل **حقيبة الكلمات** أو "حقيبة n-grams". ويتم وصف المستندات من خلال حدوث الكلمات مع تجاهل معلومات الموضع النسبي للكلمات في المستند تمامًا.


التفرق
--------

نظرًا لأن معظم المستندات ستستخدم عادةً مجموعة فرعية صغيرة جدًا من الكلمات المستخدمة في المجموعة، فستحتوي المصفوفة الناتجة على العديد من قيم الميزات التي تكون أصفارًا (عادةً أكثر من 99٪ منها).

على سبيل المثال، ستستخدم مجموعة من 10000 مستند نصي قصير (مثل رسائل البريد الإلكتروني) مفردات بحجم يتراوح بين 100000 كلمة فريدة في المجموع، بينما سيستخدم كل مستند 100 إلى 1000 كلمة فريدة بشكل فردي.

من أجل أن تكون قادرًا على تخزين مثل هذه المصفوفة في الذاكرة ولكن أيضًا لتسريع العمليات الجبرية للمصفوفة / المتجه، ستستخدم عمليات التنفيذ عادةً تمثيلًا متفرقًا مثل عمليات التنفيذ المتوفرة في ``scipy.sparse`` الحزمة.


استخدام Vectorizer الشائع
-----------------------

يقوم :class:`CountVectorizer` بتنفيذ كل من التجزئة وعد مرات الحدوث في فئة واحدة::

  >>> from sklearn.feature_extraction.text import CountVectorizer

هذا النموذج به العديد من المعلمات، ومع ذلك فإن القيم الافتراضية معقولة تمامًا (يرجى الاطلاع على :ref:`reference documentation <feature_extraction_ref-from-text>` للحصول على التفاصيل)::

  >>> vectorizer = CountVectorizer()
  >>> vectorizer
  CountVectorizer()

لنستخدمه في تجزئة وعد حدوث الكلمات لمجموعة نصية بسيطة من المستندات النصية::

  >>> corpus = [
  ...     'هذا هو المستند الأول.',
  ...     'هذا هو المستند الثاني الثاني.',
  ...     'والمستند الثالث.',
  ...     'هل هذا هو المستند الأول؟',
  ... ]
  >>> X = vectorizer.fit_transform(corpus)
  >>> X
  <Compressed Sparse...dtype 'int64'
    with 19 stored elements and shape (4, 9)>

يقوم التكوين الافتراضي بتجزئة السلسلة عن طريق استخراج الكلمات التي لا يقل طولها عن حرفين. يمكن طلب الدالة المحددة التي تقوم بهذه الخطوة صراحةً::

  >>> analyze = vectorizer.build_analyzer()
  >>> analyze("هذا هو نص المستند لتحليله.") == (
  ...     ['هذا', 'هو', 'نص', 'المستند', 'ل', 'تحليله'])
  True

يتم تعيين كل مصطلح تم العثور عليه بواسطة محلل أثناء التخصيص لمؤشر صحيح فريد يتوافق مع عمود في المصفوفة الناتجة. يمكن استرداد هذا التفسير للأعمدة على النحو التالي::

  >>> vectorizer.get_feature_names_out()
 array(['و', 'المستند', 'الأول', 'هو', 'الثالث', 'الثاني', 'ل',
         'الثاني', 'هذا'], ...)

  >>> X.toarray()
 array([[0, 1, 1, 1, 0, 0, 1, 0, 1],
         [0, 1, 0, 1, 0, 2, 1, 0, 1],
         [1, 0, 0, 0, 1, 0, 1, 1, 0],
         [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)

يتم تخزين رسم الخرائط العكسي من اسم الميزة إلى فهرس العمود في السمة ``vocabulary_`` للمحسن::

  >>> vectorizer.vocabulary_.get('المستند')
  1

وبالتالي سيتم تجاهل الكلمات التي لم يتم رؤيتها في مجموعة التدريب تمامًا في المكالمات المستقبلية لطريقة التحويل::

  >>> vectorizer.transform(['شيء جديد تمامًا.']).toarray()
 array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)

لاحظ أنه في المجموعة السابقة، المستند الأول والمستند الأخير بهما نفس الكلمات بالتالي يتم ترميزهما في متجهات متساوية. على وجه الخصوص، نفقد المعلومات التي تفيد بأن المستند الأخير هو شكل استفهامي. للحفاظ على بعض معلومات الترتيب المحلية، يمكننا استخراج 2-grams من الكلمات بالإضافة إلى 1-grams (كلمات فردية)::

  >>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),
  ...                                     token_pattern=r'\b\w+\b', min_df=1)
  >>> analyze = bigram_vectorizer.build_analyzer()
  >>> analyze('Bi-grams are cool!') == (
  ...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])
  True

وبالتالي فإن المفردات التي يستخرجها هذا المحسن أكبر بكثير ويمكنها الآن حل الغموض المشفر في أنماط تحديد المواقع المحلية::

  >>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()
  >>> X_2
 array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],
         [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],
         [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],
         [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)


على وجه الخصوص، شكل الاستفهام "هل هذا" موجود فقط في المستند الأخير::

  >>> feature_index = bigram_vectorizer.vocabulary_.get('هل هذا')
  >>> X_2[:, feature_index]
 array([0, 0, 0, 1]...)

.. _stop_words:

استخدام الكلمات التوقف
----------------

تعتبر الكلمات المتوقفة مثل "و"، "ال"، "هو" كلمات غير مفيدة في تمثيل محتوى النص، ويمكن إزالتها لتجنب اعتبارها إشارة للتنبؤ. ومع ذلك، فإن بعض الكلمات المماثلة قد تكون مفيدة للتنبؤ، مثل تصنيف أسلوب الكتابة أو الشخصية.

توجد عدة مشاكل معروفة في قائمة الكلمات المتوقفة "الإنجليزية" التي نقدمها. إنها لا تهدف إلى أن تكون حلاً عاماً "مقاس واحد يناسب الجميع" لأن بعض المهام قد تتطلب حلاً أكثر تخصيصاً. راجع [NQY18]_ للحصول على مزيد من التفاصيل.

يُرجى توخي الحذر عند اختيار قائمة الكلمات المتوقفة. قد تتضمن قوائم الكلمات المتوقفة الشائعة كلمات تكون مفيدة للغاية في بعض المهام، مثل "كمبيوتر".

يجب أيضاً التأكد من أن قائمة الكلمات المتوقفة قد خضعت لنفس عملية المعالجة المسبقة والتجزئة المستخدمة في أداة التحويل المتجه. على سبيل المثال، يتم تقسيم الكلمة "we've" إلى "we" و "ve" بواسطة أداة التجزئة الافتراضية لـ CountVectorizer، لذلك إذا كانت الكلمة "we've" موجودة في ``stop_words`` ولكن الكلمة "ve" ليست كذلك، فستظل الكلمة "ve" موجودة في النص المحول. ستحاول أدوات التحويل المتجه لدينا تحديد وتحذيرك من بعض أنواع التناقضات.

.. rubric:: المراجع

.. [NQY18] ج. ناثمان، ه. كوين و ر. يورشاك (2018).
   `"قوائم الكلمات المتوقفة في حزم البرامج المجانية مفتوحة المصدر"
   <https://aclweb.org/anthology/W18-2502>`__.
   في *محاضر ورشة عمل لبرامج NLP مفتوحة المصدر*.


.. _tfidf:

وزن المصطلحات بواسطة Tf-idf

    

في مجموعة نصية كبيرة، ستكون بعض الكلمات موجودة بشكل كبير (مثل "the" و "a" و "is" في اللغة الإنجليزية) وبالتالي تحمل معلومات مفيدة قليلة عن المحتوى الفعلي للوثيقة. إذا قمنا بتغذية بيانات العد المباشر مباشرة إلى المصنف، فإن هذه المصطلحات المتكررة جدًا ستظلل تكرار المصطلحات النادرة ولكن الأكثر إثارة للاهتمام.

من أجل إعادة وزن ميزات العد إلى قيم نقطية مناسبة للاستخدام من قبل المصنف، من الشائع جدًا استخدام تحويل tf-idf.

Tf تعني **تكرار المصطلح** بينما tf-idf تعني تكرار المصطلح في **عدد الوثائق العكسية**:
:math:`\text{tf-idf(t,d)}=\text{tf(t,d)} \times \text{idf(t)}`.

باستخدام الإعدادات الافتراضية لـ ``TfidfTransformer`` ،
``TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)``
يتم ضرب تكرار المصطلح، عدد المرات التي يظهر فيها المصطلح في وثيقة معينة، في مكون idf، الذي يتم حسابه على النحو التالي

:math:`\text{idf}(t) = \log{\frac{1 + n}{1+\text{df}(t)}} + 1`,

حيث :math:`n` هو إجمالي عدد الوثائق في مجموعة الوثائق، و
:math:`\text{df}(t)` هو عدد الوثائق في مجموعة الوثائق التي تحتوي على المصطلح :math:`t`. ثم يتم تطبيع متجهات tf-idf الناتجة بواسطةolocation

:math:`v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
v{_2}^2 + \dots + v{_n}^2}}`.

كان هذا في الأصل مخطط وزن مصطلح تم تطويره لاسترجاع المعلومات
(كوظيفة ترتيب لنتائج محركات البحث) والتي وجدت أيضًا استخدامًا جيدًا في تصنيف المستندات وتجميعها.

تحتوي الأقسام التالية على مزيد من التفسيرات والأمثلة التي
توضح كيفية حساب tf-idfs بالضبط وكيف يختلف tf-idfs المحسوب في :class:`TfidfTransformer`
و :class:`TfidfVectorizer` في Scikit-learn اختلافًا طفيفًا عن معيار كتاب
النص الذي يعرّف idf على أنه

:math:`\text{idf}(t) = \log{\frac{n}{1+\text{df}(t)}}.`

في :class:`TfidfTransformer` و :class:`TfidfVectorizer`
مع ``smooth_idf=False``، يتم إضافة عدد "1" إلى idf بدلاً من مقام idf:

:math:`\text{idf}(t) = \log{\frac{n}{\text{df}(t)}} + 1`

يتم تنفيذ هذا التوحيد من قبل فئة :class:`TfidfTransformer`::

  >>> from sklearn.feature_extraction.text import TfidfTransformer
  >>> transformer = TfidfTransformer(smooth_idf=False)
  >>> transformer
  TfidfTransformer(smooth_idf=False)

مرة أخرى، يرجى الاطلاع على :ref:`reference documentation
<feature_extraction_ref-from-text>` للحصول على التفاصيل حول جميع المعلمات.

.. dropdown:: مثال رقمي لمصفوفة tf-idf

  دعونا نأخذ مثالاً مع التهم التالية. المصطلح الأول موجود
  بنسبة 100٪ من الوقت وبالتالي لا يكون مثيرًا للاهتمام. يوجد المصطلحان الآخران فقط
  في أقل من 50٪ من الوقت وبالتالي من المحتمل أن يكونا أكثر تمثيلا
  لمحتوى المستندات::

    >>> counts = [[3, 0, 1],
    ...           [2, 0, 0],
    ...           [3, 0, 0],
    ...           [4, 0, 0],
    ...           [3, 2, 0],
    ...           [3, 0, 2]]
    ...
    >>> tfidf = transformer.fit_transform(counts)
    >>> tfidf
    <Compressed Sparse...dtype 'float64'
      with 9 stored elements and shape (6, 3)>

    >>> tfidf.toarray()
    array([[0.81940995, 0.        , 0.57320793],
          [1.        , 0.        , 0.        ],
          [1.        , 0.        , 0.        ],
          [1.        , 0.        , 0.        ],
          [0.47330339, 0.88089948, 0.        ],
          [0.58149261, 0.        , 0.81355169]])

  يتم تطبيع كل صف بحيث يكون له وحدة القاعدة الأورثوبيدية:

  :math:`v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
  v{_2}^2 + \dots + v{_n}^2}}`

  على سبيل المثال، يمكننا حساب tf-idf للمصطلح الأول في الوثيقة الأولى
  في صفيف `counts` على النحو التالي:

  :math:`n = 6`

  :math:`\text{df}(t)_{\text{term1}} = 6`

  :math:`\text{idf}(t)_{\text{term1}} =
  \log \frac{n}{\text{df}(t)} + 1 = \log(1)+1 = 1`

  :math:`\text{tf-idf}_{\text{term1}} = \text{tf} \times \text{idf} = 3 \times 1 = 3`

  الآن، إذا كررنا هذا الحساب للمصطلحين المتبقيين في المستند،
  نحصل على

  :math:`\text{tf-idf}_{\text{term2}} = 0 \times (\log(6/1)+1) = 0`

  :math:`\text{tf-idf}_{\text{term3}} = 1 \times (\log(6/2)+1) \approx 2.0986`

  ومتجه tf-idfs الخام:

  :math:`\text{tf-idf}_{\text{raw}} = [3, 0, 2.0986].`


  ثم، بتطبيق القاعدة الأورثوبيدية (L2)، نحصل على tf-idfs التالية
  للمستند 1:

  :math:`\frac{[3, 0, 2.0986]}{\sqrt{\big(3^2 + 0^2 + 2.0986^2\big)}}
  = [ 0.819,  0,  0.573].`

  علاوة على ذلك، فإن معامل ``smooth_idf=True`` الافتراضي يضيف "1" إلى البسط
  و المقام كما لو تم رؤية مستند إضافي يحتوي على كل مصطلح في
  المجموعة بالضبط مرة واحدة، مما يمنع الانقسامات الصفرية:

  :math:`\text{idf}(t) = \log{\frac{1 + n}{1+\text{df}(t)}} + 1`

  باستخدام هذا التعديل، يتغير tf-idf المصطلح الثالث في المستند 1 إلى
  1.8473:

  :math:`\text{tf-idf}_{\text{term3}} = 1 \times \log(7/3)+1 \approx 1.8473`

  ويتغير tf-idf L2-normalized إلى

  :math:`\frac{[3, 0, 1.8473]}{\sqrt{\big(3^2 + 0^2 + 1.8473^2\big)}}
  = [0.8515, 0, 0.5243]`::

    >>> transformer = TfidfTransformer()
    >>> transformer.fit_transform(counts).toarray()
    array([[0.85151335, 0.        , 0.52433293],
          [1.        , 0.        
على الرغم من أن تطبيع tf-idf يكون مفيدًا جدًا في كثير من الأحيان، إلا أنه قد تكون هناك حالات حيث توفر علامات الحدوث الثنائية ميزات أفضل. يمكن تحقيق ذلك باستخدام معامل "binary" الخاص بـ :class:`CountVectorizer`. على وجه الخصوص، بعض المقدرات مثل :ref:`bernoulli_naive_bayes` تقوم بشكل صريح بنمذجة متغيرات عشوائية boolean منفصلة. أيضًا، من المرجح أن تحتوي النصوص القصيرة جدًا على قيم tf-idf صاخبة بينما تكون معلومات الحدوث الثنائي أكثر استقرارًا.

كما هو معتاد، فإن أفضل طريقة لضبط معلمات استخراج الميزات هي استخدام بحث شبكة تم التحقق من صحتها عبر التحقق من الصحة، على سبيل المثال عن طريق توصيل مستخرج الميزة مع المصنف:

* :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`


فك تشفير ملفات النص

-------------------

يتكون النص من أحرف، ولكن الملفات تتكون من وحدات بايت. تمثل هذه الوحدات الأحرف وفقًا لبعض *التشفير*. للعمل مع ملفات النص في Python، يجب *فك تشفير* وحدات البايت هذه إلى مجموعة أحرف تسمى Unicode.

التشفير الشائع هو ASCII و Latin-1 (أوروبا الغربية) و KOI8-R (الروسية) والتشفير العالمي UTF-8 و UTF-16. يوجد العديد من الترميزات الأخرى.

.. note::
    يمكن أيضًا تسمية التشفير "مجموعة الأحرف"، ولكن هذا المصطلح أقل دقة: يمكن أن توجد عدة ترميزات لمجموعة أحرف واحدة.

تعرف مستخرجات ميزات النص في scikit-learn كيفية فك تشفير ملفات النص، ولكن فقط إذا أخبرتهم عن الترميز الذي تستخدمه الملفات. يأخذ :class:`CountVectorizer` معلم ``encoding`` لهذا الغرض.

بالنسبة إلى ملفات النص الحديثة، من المحتمل أن يكون الترميز الصحيح هو UTF-8، وهو الترميز الافتراضي (``encoding="utf-8"``).

ومع ذلك، إذا لم يكن النص الذي تقوم بتحميله مشفرًا بالفعل باستخدام UTF-8، فستحصل على ``UnicodeDecodeError``.

يمكن إخبار vectorizers بعدم الإبلاغ عن أخطاء فك التشفير عن طريق تعيين معلمة ``decode_error`` إما ``"ignore"`` أو ``"replace"``. راجع التوثيق لوظيفة Python ``bytes.decode`` للحصول على مزيد من التفاصيل (اكتب ``help(bytes.decode)`` في موجه Python).

.. dropdown:: استكشاف أخطاء فك تشفير النص وإصلاحها

  إذا كنت تواجه مشكلة في فك تشفير النص، فإليك بعض الأشياء التي يمكنك تجربتها:

  - معرفة الترميز الفعلي للنص. قد يأتي الملف مع رأس أو README يخبرك بالترميز، أو قد يكون هناك بعض الترميز القياسي الذي يمكنك افتراضه استنادًا إلى مصدر النص.

  - قد تتمكن من معرفة نوع الترميز بشكل عام باستخدام أمر UNIX ``file``. تأتي وحدة ``chardet`` في Python مع برنامج نصي يسمى ``chardetect.py`` سيرجح الترميز المحدد، على الرغم من أنه لا يمكنك الاعتماد على تخمينه الصحيح.

  - يمكنك تجربة UTF-8 وتجاهل الأخطاء. يمكنك فك تشفير سلاسل البايت باستخدام ``bytes.decode(errors='replace')`` لاستبدال جميع أخطاء فك التشفير بحرف عديم المعنى، أو تعيين ``decode_error='replace'`` في vectorizer. قد يؤدي هذا إلى إتلاف فائدة ميزاتك.

  - قد يأتي النص الحقيقي من مجموعة متنوعة من المصادر التي ربما استخدمت ترميزات مختلفة، أو حتى تم فك ترميزها بشكل سيئ في ترميز مختلف عن الترميز الذي تم ترميزه به. هذا شائع في النص المسترد من الويب. يمكن للحزمة Python `ftfy <https://github.com/LuminosoInsight/python-ftfy>`__ فرز بعض فئات أخطاء فك التشفير تلقائيًا، حتى تتمكن من تجربة فك تشفير النص غير المعروف كـ ``latin-1`` ثم استخدام ``ftfy`` لإصلاح الأخطاء.

  - إذا كان النص عبارة عن مزيج من الترميزات يصعب فرزها ببساطة (وهو الحال بالنسبة لمجموعة بيانات 20 Newsgroups)، فيمكنك الرجوع إلى ترميز بسيط مكون من بايت واحد مثل ``latin-1``. قد يتم عرض بعض النص بشكل غير صحيح، ولكن على الأقل ستمثل نفس سلسلة البايتات نفس الميزة دائمًا.

  على سبيل المثال، تستخدم المقتطف التالي ``chardet`` (لا يتم شحنه مع scikit-learn، يجب تثبيته بشكل منفصل) لمعرفة ترميز ثلاثة نصوص. ثم تقوم بنقل النصوص وطباعة المفردات المستفادة. لا يظهر الإخراج هنا.

    >>> import chardet    # doctest: +SKIP
    >>> text1 = b"Sei mir gegr\xc3\xbc\xc3\x9ft mein Sauerkraut"
    >>> text2 = b"holdselig sind deine Ger\xfcche"
    >>> text3 = b"\xff\xfeA\x00u\x00f\x00 \x00F\x00l\x0

...

    >>> decoded = [x.decode(chardet.detect(x)['encoding'])
    ...            for x in (text1, text2, text3)]        # doctest: +SKIP
    >>> v = CountVectorizer().fit(decoded).vocabulary_    # doctest: +SKIP
    >>> for term in v: print(v)                           # doctest: +SKIP

  (اعتمادًا على إصدار ``chardet``، قد يخطئ في الترميز الأول.)

  لمقدمة إلى Unicode وترميزات الأحرف بشكل عام، راجع `الحد الأدنى المطلق الذي يجب على كل مطور برامج معرفته حول Unicode <https://www.joelonsoftware.com/articles/Unicode.html>`_ بواسطة Joel Spolsky.


التطبيقات والأمثلة
-------------------------

تمثيل حقيبة الكلمات بسيط للغاية ولكنه مفيد بشكل مدهش في الممارسة العملية.

على وجه الخصوص في **إعداد خاضع للإشراف**، يمكن دمجه بنجاح مع النماذج الخطية السريعة والقابلة للتطوير لتدريب **مصنفات المستندات**، على سبيل المثال:

* :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`

في **إعداد غير خاضع للإشراف**، يمكن استخدامه لتجميع المستندات المماثلة معًا عن طريق تطبيق خوارزميات التجميع مثل :ref:`k_means`:

* :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`

أخيرًا، من الممكن اكتشاف الموضوعات الرئيسية لمجموعة من خلال الاسترخاء في قيد التعيين الصعب للتجميع، على سبيل المثال باستخدام :ref:`NMF`:

* :ref:`sphx_glr_auto_examples_applications_plot_topics_extraction_with_nmf_lda.py`


قيود تمثيل Bag of Words
----------------------------------------------

لا يمكن لمجموعة الأنيجرامات (وهو ما تعنيه "bag of words") التقاط العبارات والتعبيرات المكونة من عدة كلمات، وبالتالي تتجاهل أي اعتماد على ترتيب الكلمات. بالإضافة إلى ذلك، لا يأخذ نموذج "bag of words" في الاعتبار الإملاءات الخاطئة المحتملة أو اشتقاقات الكلمات.

هنا يأتي دور الن-جرامات! بدلاً من بناء مجموعة بسيطة من الأنيجرامات (n=1)، قد يفضل المرء مجموعة من البيجرامات (n=2)، حيث يتم حساب تكرارات أزواج الكلمات المتتالية.

بدلاً من ذلك، يمكن اعتبار مجموعة من الن-جرامات الحرفية، وهي تمثيل مرن ضد الإملاءات الخاطئة والاشتقاقات.

على سبيل المثال، لنفترض أننا نتعامل مع مجموعة من اثنين من المستندات: ``['words', 'wprds']``. يحتوي المستند الثاني على إملاء خاطئ لكلمة 'words'.

سوف يعتبر التمثيل البسيط لـ "bag of words" هذين المستندين مختلفين تماماً، حيث يختلفان في كلتي الخاصيتين المحتملتين.

ولكن التمثيل باستخدام الحرفين 2-جرام، سوف يجد المستندين متطابقين في 4 من أصل 8 خصائص، مما قد يساعد المصنف المفضل على اتخاذ قرار أفضل::

  >>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))
  >>> counts = ngram_vectorizer.fit_transform(['words', 'wprds'])
  >>> ngram_vectorizer.get_feature_names_out()
 array([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...)
  >>> counts.toarray().astype(int)
 array([[1, 1, 1, 0, 1, 1, 1, 0],
         [1, 1, 0, 1, 1, 1, 0, 1]])

في المثال أعلاه، يتم استخدام محلل 'char_wb'، والذي ينشئ الن-جرامات فقط من الحروف الموجودة داخل حدود الكلمات (مبطنة بمسافة على كل جانب). بدلاً من ذلك، ينشئ محلل 'char' الن-جرامات التي تمتد عبر الكلمات::

  >>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))
  >>> ngram_vectorizer.fit_transform(['jumpy fox'])
  <Compressed Sparse...dtype 'int64'
    with 4 stored elements and shape (1, 4)>

  >>> ngram_vectorizer.get_feature_names_out()
 array([' fox ', ' jump', 'jumpy', 'umpy '], ...)

  >>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5))
  >>> ngram_vectorizer.fit_transform(['jumpy fox'])
  <Compressed Sparse...dtype 'int64'
    with 5 stored elements and shape (1, 5)>
  >>> ngram_vectorizer.get_feature_names_out()
 array(['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'], ...)

النسخة الواعية بحدود الكلمات 'char_wb' مثيرة للاهتمام بشكل خاص للغات التي تستخدم المسافات لفصل الكلمات، لأنها تولد ميزات أقل ضوضاء بكثير من النسخة الأولية 'char' في هذه الحالة. بالنسبة لهذه اللغات، يمكنها زيادة كل من الدقة التنبؤية وسرعة التقارب للمصنفات المدربة باستخدام هذه الميزات مع الحفاظ على الصلابة فيما يتعلق بالإملاءات الخاطئة واشتقاقات الكلمات.

في حين أنه يمكن الحفاظ على بعض المعلومات الموضعية المحلية عن طريق استخراج الن-جرامات بدلاً من الكلمات الفردية، فإن "bag of words" و "bag of n-grams" يدمران معظم البنية الداخلية للمستند وبالتالي معظم المعنى الذي تحمله هذه البنية الداخلية.

ولمعالجة المهمة الأوسع المتمثلة في فهم اللغة الطبيعية، يجب مراعاة البنية المحلية للجمل والفقرات. وبالتالي، سيتم صياغة العديد من هذه النماذج كمشاكل "إخراج منظم" والتي تقع حالياً خارج نطاق scikit-learn.


.. _hashing_vectorizer:

تجهيز مجموعة نصوص كبيرة باستخدام خدعة التجزئة

    

النظام المتجهي المذكور أعلاه هو نظام بسيط، ولكن حقيقة أنه يحتفظ **بخريطة في الذاكرة من رموز السلسلة إلى مؤشرات الميزات الصحيحة** (السمة ``vocabulary_``) تسبب العديد من **المشكلات عند التعامل مع مجموعات البيانات الكبيرة**:

- كلما زاد حجم المجموعة، زاد حجم المفردات وبالتالي استخدام الذاكرة أيضًا،

- يتطلب التركيب تخصيص هياكل بيانات وسيطة بحجم متناسب مع حجم المجموعة الأصلية.

- يتطلب بناء خريطة الكلمات المرور الكامل على المجموعة وبالتالي لا يمكن تركيب تصنيفات النص بطريقة متصلة بالإنترنت تمامًا.

- يمكن أن يكون التخليل وإزالة التخليل للبرامج المتجهية ذات ``vocabulary_`` كبيرًا بطيئًا جدًا (عادةً ما يكون أبطأ بكثير من التخليل / إزالة التخليل لهياكل البيانات المسطحة مثل صفيف NumPy بنفس الحجم)،

- ليس من السهل تقسيم عمل المتجهات إلى مهام فرعية متزامنة حيث يجب أن تكون سمة ``vocabulary_`` حالة مشتركة مع حاجز مزامنة دقيق الحبيبات: يعتمد إنشاء الخريطة من سلسلة الرمز إلى مؤشر الميزة على ترتيب حدوث كل رمز لأول مرة وبالتالي يجب مشاركته، مما قد يضر بأداء العمال المتزامنين إلى نقطة جعلهم أبطأ من البديل التسلسلي.

من الممكن التغلب على تلك القيود من خلال الجمع بين "خدعة التجزئة" (:ref:`Feature_hashing`) التي نفذتها :class:`~sklearn.feature_extraction.FeatureHasher` فئة وميزات معالجة النصوص المسبقة والتسميات لفئة :class:`CountVectorizer`.

يتم تنفيذ هذا المزيج في :class:`HashingVectorizer`، وهو فئة المحولات التي تتوافق بشكل أساسي مع واجهة برمجة التطبيقات :class:`CountVectorizer`. :class:`HashingVectorizer` هو عديم الحالة، مما يعني أنك لست بحاجة إلى استدعاء ``fit`` عليه::

  >>> من sklearn.feature_extraction.text استيراد HashingVectorizer
  >>> hv = HashingVectorizer(n_features=10)
  >>> hv.transform(corpus)
  <Compressed Sparse ... dtype 'float64'
    with 16 stored elements and shape (4, 10)>

يمكنك أن ترى أنه تم استخراج 16 رمز ميزة غير صفرية في الناتج المتجه: هذا أقل من 19 غير الصفرية التي استخرجتها سابقًا :class:`CountVectorizer` على نفس مجموعة اللعب. يأتي التباين من تصادمات دالة التجزئة بسبب القيمة المنخفضة لمعلمة ``n_features``.

في إعداد العالم الحقيقي، يمكن ترك معلمة ``n_features`` إلى قيمتها الافتراضية ``2 ** 20`` (حوالي مليون ميزة ممكنة). إذا كانت الذاكرة أو حجم النماذج في اتجاه التيار مشكلة، فإن اختيار قيمة أقل مثل ``2 ** 18`` قد يساعد دون إدخال الكثير من التصادمات الإضافية في مهام تصنيف النصوص النموذجية.

لاحظ أن الأبعاد لا تؤثر على وقت تدريب وحدة المعالجة المركزية للخوارزميات التي تعمل على مصفوفات CSR (``LinearSVC(dual=True)``, ``Perceptron``, ``SGDClassifier``, ``PassiveAggressive``) ولكنها تؤثر على الخوارزميات التي تعمل مع مصفوفات CSC (``LinearSVC(dual=False)``, ``Lasso()``، إلخ.).

دعونا نحاول مرة أخرى مع الإعداد الافتراضي::

  >>> hv = HashingVectorizer()
  >>> hv.transform(corpus)
  <Compressed Sparse ... dtype 'float64'
    with 19 stored elements and shape (4, 1048576)>

لم نعد نحصل على التصادمات، ولكن هذا يأتي على حساب أبعاد أكبر بكثير لمساحة الإخراج.
بالطبع، قد تتصادم مصطلحات أخرى غير الـ 19 المستخدمة هنا مع بعضها البعض.

يأتي :class:`HashingVectorizer` أيضًا مع القيود التالية:

- من غير الممكن عكس النموذج (لا توجد طريقة ``inverse_transform``)، ولا الوصول إلى التمثيل السلسلة الأصلي للميزات، نظرًا لطبيعة اتجاه واحد لوظيفة التجزئة التي تؤدي التعيين.

- لا يوفر ترجيح IDF لأن ذلك سيقدم الحالة في النموذج. يمكن إلحاق :class:`TfidfTransformer` به في خط أنابيب إذا لزم الأمر.

.. dropdown:: أداء التحجيم خارج النواة باستخدام HashingVectorizer

  تطوير مثير للاهتمام لاستخدام :class:`HashingVectorizer` هو القدرة على إجراء التحجيم `خارج النواة`_. هذا يعني أنه يمكننا التعلم من البيانات التي لا تتناسب مع الذاكرة الرئيسية للكمبيوتر.

  .. _خارج النواة: https://en.wikipedia.org/wiki/Out-of-core_algorithm

  تتمثل إستراتيجية تنفيذ التحجيم خارج النواة في دفق البيانات إلى ارزیاب في مجموعات صغيرة. يتم متجه كل مجموعة صغيرة باستخدام :class:`HashingVectorizer` وذلك لضمان أن مساحة الإدخال للارزیاب لها دائمًا نفس الأبعاد. وبالتالي، يتم تحديد مقدار الذاكرة المستخدمة في أي وقت بحجم المجموعة الصغيرة. على الرغم من عدم وجود حد لكمية البيانات التي يمكن تناولها باستخدام مثل هذا النهج، إلا أن وقت التعلم من وجهة نظر عملية غالبًا ما يقتصر على وقت وحدة المعالجة المركزية الذي تريد إنفاقه على المهمة.

  للحصول على مثال كامل للتحجيم خارج النواة في مهمة تصنيف النص، راجع :ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`.


تخصيص فئات المتجهات

من الممكن تخصيص السلوك عن طريق تمرير وسيط استدعاء إلى منشئ الناقل::

  >>> def my_tokenizer(s):
  ...     return s.split()
  ...
  >>> vectorizer = CountVectorizer(tokenizer=my_tokenizer)
  >>> vectorizer.build_analyzer()(u"Some... punctuation!") == (
  ...     ['some...', 'punctuation!'])
  True

على وجه الخصوص ، نسمي:

* ``preprocessor``: عبارة عن استدعاء يأخذ مستندًا كاملاً كمدخل (كسلسلة واحدة) ، ويعيد إصدارًا محولًا ربما من المستند ، لا يزال كسلسلة كاملة. يمكن استخدام هذا لإزالة علامات HTML ، أو تحويل المستند بأكمله إلى أحرف صغيرة ، إلخ.

* ``tokenizer``: استدعاء يأخذ الإخراج من المعالج المسبق ويقسمه إلى رموز ، ثم يعيد قائمة بهذه الرموز.

* ``analyzer``: استدعاء يحل محل المعالج المسبق والراسم. تقوم أدوات التحليل الافتراضية جميعها باستدعاء المعالج المسبق والمحلل ، ولكن أدوات التحليل المخصصة ستتخطى هذا. يحدث استخراج N-gram وتصفية الكلمات المتوقفة على مستوى المحلل ، لذلك قد يضطر المحلل المخصص إلى إعادة إنتاج هذه الخطوات.

(قد يتعرف مستخدمو Lucene على هذه الأسماء ، ولكن يجب أن تدرك أن مفاهيم scikit-learn قد لا تتوافق واحد لواحد مع مفاهيم Lucene.)

لجعل المعالج المسبق والراسم وأدوات التحليل على دراية بمعلمات النموذج ، من الممكن الاشتقاق من الفصل والفئة وتجاوز ``build_preprocessor`` ، ``build_tokenizer`` و ``build_analyzer`` طرق المصنع بدلاً من تمرير وظائف مخصصة.

.. dropdown:: نصائح وحيل
  :color: success

  * إذا كانت المستندات مسبقة الرمز من خلال حزمة خارجية ، فاحفظها في ملفات (أو سلاسل) مع فصل الرموز بمسافة بيضاء واجتياز ``analyzer=str.split``
  * لا يتم تضمين تحليلات المستوى الرمزي الفائق مثل التشعب ، أو التلميح ، أو الانقسام المركب ، أو التصفية بناءً على جزء من الكلام ، وما إلى ذلك في شفرة scikit-learn ، ولكن يمكن إضافتها عن طريق تخصيص المميز أو المحلل.
    فيما يلي ``CountVectorizer`` مع مميز ومميز باستخدام
    `NLTK <https://www.nltk.org/>`_::

        >>> from nltk import word_tokenize          # doctest: +SKIP
        >>> from nltk.stem import WordNetLemmatizer # doctest: +SKIP
        >>> class LemmaTokenizer:
        ...     def __init__(self):
        ...         self.wnl = WordNetLemmatizer()
        ...     def __call__(self, doc):
        ...         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]
        ...
        >>> vect = CountVectorizer(tokenizer=LemmaTokenizer())  # doctest: +SKIP

    (لاحظ أن هذا لن يزيل علامات الترقيم.)

    على سبيل المثال ، سوف يحول المثال التالي بعض الهجاء البريطاني إلى الهجاء الأمريكي::

        >>> import re
        >>> def to_british(tokens):
        ...     for t in tokens:
        ...         t = re.sub(r"(...)our$", r"\1or", t)
        ...         t = re.sub(r"([bt])re$", r"\1er", t)
        ...         t = re.sub(r"([iy])s(e$|ing|ation)", r"\1z\2", t)
        ...         t = re.sub(r"ogue$", "og", t)
        ...         yield t
        ...
        >>> class CustomVectorizer(CountVectorizer):
        ...     def build_tokenizer(self):
        ...         tokenize = super().build_tokenizer()
        ...         return lambda doc: list(to_british(tokenize(doc)))
        ...
        >>> print(CustomVectorizer().build_analyzer()(u"color colour"))
        [...'color', ...'color']

    لأنماط أخرى من المعالجة المسبقة ؛ تتضمن الأمثلة إزالة التشعب ، أو التلميح ، أو تطبيع الرموز العددية ، مع توضيح الأخير في:

    * :ref:`sphx_glr_auto_examples_bicluster_plot_bicluster_newsgroups.py`

  يمكن أيضًا أن يكون تخصيص الناقل مفيدًا عند التعامل مع اللغات الآسيوية التي لا تستخدم فاصل كلمة صريح مثل المسافة البيضاء.

.. _image_feature_extraction:

استخراج ميزة الصورة
========================

.. currentmodule:: sklearn.feature_extraction.image

استخراج التصحيحات
----------------

تستخرج الدالة :func:`extract_patches_2d` التصحيحات من صورة مخزنة كصفيف ثنائي الأبعاد، أو ثلاثي الأبعاد مع معلومات اللون على طول المحور الثالث. لإعادة بناء صورة من جميع تصحيحاتها، استخدم :func:`reconstruct_from_patches_2d`. على سبيل المثال، لننشئ صورة بحجم 4x4 بكسل مع 3 قنوات لونية (مثل تنسيق RGB)::

    >>> import numpy as np
    >>> from sklearn.feature_extraction import image

    >>> one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))
    >>> one_image[:, :, 0]  # قناة R لصورة RGB مزيفة
    array([[ 0,  3,  6,  9],
           [12, 15, 18, 21],
           [24, 27, 30, 33],
           [36, 39, 42, 45]])

    >>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,
    ...     random_state=0)
    >>> patches.shape
    (2, 2, 2, 3)
    >>> patches[:, :, :, 0]
    array([[[ 0,  3],
            [12, 15]],
    <BLANKLINE>
           [[15, 18],
            [27, 30]]])
    >>> patches = image.extract_patches_2d(one_image, (2, 2))
    >>> patches.shape
    (9, 2, 2, 3)
    >>> patches[4, :, :, 0]
    array([[15, 18],
           [27, 30]])

لنحاول الآن إعادة بناء الصورة الأصلية من التصحيحات عن طريق المتوسط ​​على المناطق المتداخلة::

    >>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))
    >>> np.testing.assert_array_equal(one_image, reconstructed)

تعمل فئة :class:`PatchExtractor` بنفس طريقة :func:`extract_patches_2d`، إلا أنها تدعم صورًا متعددة كمدخلات. يتم تنفيذها كمعالج scikit-learn، لذلك يمكن استخدامها في خطوط الأنابيب. انظر::

    >>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)
    >>> patches = image.PatchExtractor(patch_size=(2, 2)).transform(five_images)
    >>> patches.shape
    (45, 2, 2, 3)

رسم بياني لوصلات الصورة
------------------------

يمكن للعديد من المقدرين في scikit-learn استخدام معلومات الربط بين الميزات أو العينات. على سبيل المثال، يمكن لتجميع Ward (:ref:`hierarchical_clustering`) تجميع وحدات البكسل المجاورة للصورة فقط، وبالتالي تشكيل بقع متجاورة:

.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_coin_ward_segmentation_001.png
   :target: ../auto_examples/cluster/plot_coin_ward_segmentation.html
   :align: center
   :scale: 40

لهذا الغرض، يستخدم المقدرون مصفوفة "اتصال"، والتي تعطي أي العينات متصلة.

تعيد الدالة :func:`img_to_graph` مثل هذه المصفوفة من صورة ثنائية أو ثلاثية الأبعاد. وبالمثل، تبني :func:`grid_to_graph` مصفوفة اتصال للصور بالنظر إلى شكل هذه الصورة.

يمكن استخدام هذه المصفوفات لفرض الاتصال في المقدرين الذين يستخدمون معلومات الاتصال، مثل تجميع Ward (:ref:`hierarchical_clustering`)، ولكن أيضًا لبناء نواة محسوبة مسبقًا، أو مصفوفات التشابه.

.. note:: **أمثلة**

   * :ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`

   * :ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`

   * :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`
    
    
