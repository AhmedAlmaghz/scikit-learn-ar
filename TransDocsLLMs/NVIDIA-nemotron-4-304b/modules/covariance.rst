\label{covariance}

====================================================
تقدير التباين المشترك (Covariance estimation)
====================================================

.. currentmodule:: sklearn.covariance


تتطلب العديد من المشاكل الإحصائية تقدير مصفوفة التباين المشترك (covariance matrix) لسكان، والتي يمكن اعتبارها تقديرًا لشكل مخطط التشتت لمجموعة البيانات. في معظم الأحيان، يجب إجراء مثل هذا التقدير على عينة لها خصائص (الحجم، الهيكل، التجانس) لها تأثير كبير على جودة التقدير. يوفر حزمة :mod:`sklearn.covariance` أدوات لتقدير دقيق لمصفوفة التباين المشترك لسكان تحت إعدادات مختلفة.

نفترض أن الملاحظات مستقلة ومتطابقة التوزيع (i.i.d.).


التباين المشترك التجريبي (Empirical covariance)
========================================

من المعروف أن مصفوفة التباين المشترك لمجموعة بيانات تقارب بشكل جيد بواسطة *مقدر الاحتمال الأقصى* الكلاسيكي (أو "التباين التجريبي")، بشرط أن يكون عدد الملاحظات كبيرًا بما فيه الكفاية مقارنة بعدد الميزات (المتغيرات التي تصف الملاحظات). بشكل أكثر دقة، فإن مقدر الاحتمال الأقصى لعينة هو مقدر غير متحيز بشكل غير متحيز لمصفوفة التباين المشترك للسكان المقابلة.

يمكن حساب مصفوفة التباين التجريبي لعينة باستخدام الدالة :func:`empirical_covariance` للحزمة، أو عن طريق تركيب كائن :class:`EmpiricalCovariance` لعينة البيانات باستخدام الطريقة :meth:`EmpiricalCovariance.fit`. يرجى الانتباه إلى أن النتائج تتوقف على ما إذا كانت البيانات مركزية، لذا قد يرغب المستخدم في استخدام معلمة ``assume_centered`` بشكل دقيق. على وجه التحديد، إذا كان ``assume_centered=False``، فإن مجموعة الاختبار من المفترض أن يكون لها نفس متجه المتوسط لمجموعة التدريب. إذا لم يكن الأمر كذلك، فيجب على المستخدم تركيز كليهما، ويجب استخدام ``assume_centered=True``.

.. rubric:: أمثلة

* انظر :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` للحصول على مثال حول كيفية تركيب كائن :class:`EmpiricalCovariance` للبيانات.


.. _shrunk_covariance:

تباين مشترك منكمش (Shrunk Covariance)

    

  الانكماش الأساسي
    --------------

    على الرغم من أن تقدير المعاملات الراجعة للانحدار يكون غير متحيز بشكل مقارب لمصفوفة التباين، إلا أنه ليس مقدرًا جيدًا لقيم eigenvalues الخاصة بمصفوفة التباين، وبالتالي فإن مصفوفة الدقة الناتجة عن انقلابها ليست دقيقة. في بعض الأحيان، يحدث حتى أن مصفوفة التباين التجريبية لا يمكن انقلابها لأسباب رقمية. لتجنب مشكلة الانقلاب هذه، تم تقديم تحويل لمصفوفة التباين التجريبية: "الانكماش".

    في سكيت-ليرن، يمكن تطبيق هذا التحويل (مع معامل انكماش يحدده المستخدم) مباشرة على مصفوفة التباين المحسوبة مسبقًا باستخدام الدالة :func:`shrunk_covariance`. بالإضافة إلى ذلك، يمكن تركيب مقدر منكمش للتباين للبيانات باستخدام كائن :class:`ShrunkCovariance` وطريقة :meth:`ShrunkCovariance.fit`. مرة أخرى، تعتمد النتائج على ما إذا كانت البيانات تتمحور، لذلك قد يرغب المرء في استخدام معامل "assume_centered" بشكل دقيق.

    رياضيًا، يتكون هذا الانكماش من تقليل النسبة بين أصغر وأكبر eigenvalues لمصفوفة التباين التجريبية. يمكن القيام بذلك ببساطة عن طريق تحويل كل eigenvalue وفقًا لإزاحة معينة، والتي تعادل إيجاد مقدر Maximum Likelihood المنظم l2 لمصفوفة التباين. في الممارسة العملية، يتلخص الانكماش في تحويل محدب بسيط: :math:`\Sigma_{\rm shrunk} = (1-\alpha)\hat{\Sigma} + \alpha\frac{{\rm Tr}\hat{\Sigma}}{p}\rm Id`.

    إن اختيار كمية الانكماش، :math:`\alpha` يحدد تجارة التحيز / التباين، ويتم مناقشته أدناه.

    .. rubric:: أمثلة

    * انظر :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` للحصول على مثال حول كيفية تركيب كائن :class:`ShrunkCovariance` للبيانات.


    انكماش Ledoit-Wolf
    ------------------

    في ورقتهم لعام 2004 [1]_، يقترح O. Ledoit و M. Wolf صيغة لحساب معامل الانكماش الأمثل :math:`\alpha` الذي يقلل متوسط الخطأ المربع بين المقدر الحقيقي ومصفوفة التباين الحقيقية.

    يمكن حساب مقدر Ledoit-Wolf لمصفوفة التباين على عينة باستخدام دالة :meth:`ledoit_wolf` لحزمة :mod:`sklearn.covariance`، أو يمكن الحصول عليها عن طريق تركيب كائن :class:`LedoitWolf` لنفس العينة.

    .. note:: **الحالة عندما تكون مصفوفة التباين السكاني متناحية**

        من المهم ملاحظة أنه عندما يكون عدد العينات أكبر بكثير من عدد الميزات، يتوقع المرء ألا يكون أي انكماش ضروريًا. الحدس وراء هذا هو أنه إذا كانت مصفوفة التباين السكاني كاملة الرتبة، عندما يزداد عدد العينات، ستصبح مصفوفة التباين العينة أيضًا موجبة التحديد. نتيجة لذلك، لن يكون أي انكماش ضروريًا ويجب أن تقوم الطريقة بذلك تلقائيًا.

        هذا، مع ذلك، ليس هو الحال في إجراء Ledoit-Wolf عندما يحدث أن تكون مصفوفة التباين السكانية مضاعفة لهوية المصفوفة. في هذه الحالة، يقترب تقدير Ledoit-Wolf للانكماش من 1 مع زيادة عدد العينات. يشير هذا إلى أن التقدير الأمثل لمصفوفة التباين في معنى Ledoit-Wolf هو مضاعف لهوية المصفوفة. نظرًا لأن مصفوفة التباين السكاني هي بالفعل مضاعف لهوية المصفوفة، فإن حل Ledoit-Wolf هو في الواقع تقدير معقول.

    .. rubric:: أمثلة

    * انظر :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` للحصول على مثال حول كيفية تركيب كائن :class:`LedoitWolf` للبيانات ولتصور أداء مقدر Ledoit-Wolf من حيث الاحتمال.

    .. rubric:: المراجع

    .. [1] O. Ledoit و M. Wolf، "مقدر جيد التهيئة لمصفوفات التباين الكبيرة الأبعاد"، مجلة التحليل متعدد المتغيرات، المجلد 88، العدد 2، فبراير 2004، الصفحات 365-411.

    .. _oracle_approximating_shrinkage:

    تقريب انكماش Oracle
    -------------------

    بافتراض أن البيانات موزعة توزيعاً طبيعياً، استنتج تشين وآخرون [2]_ صيغة تهدف إلى اختيار معامل انكماش ينتج عنه متوسط خطأ مربع أصغر من الخطأ المقدم بواسطة صيغة Ledoit و Wolf. يسمى المقدر الناتج مقدر تقريبي للانكماش Oracle لمصفوفة التباين.

    يمكن حساب مقدر OAS لمصفوفة التباين على عينة باستخدام دالة :meth:`oas` لحزمة :mod:`sklearn.covariance`، أو يمكن الحصول عليها عن طريق تركيب كائن :class:`OAS` لنفس العينة.

    .. figure:: ../auto_examples/covariance/images/sphx_glr_plot_covariance_estimation_001.png
       :target: ../auto_examples/covariance/plot_covariance_estimation.html
       :align: center
       :scale: 65%

       تجارة التحيز / التباين عند ضبط الانكماش: مقارنة خيارات مقدري Ledoit-Wolf و OAS

    .. rubric:: المراجع

    .. [2] :arxiv:`"خوارزميات الانكماش لتقدير التباين MMSE.",
           تشين، وايزل، الدار، هيرو.
           معاملات IEEE على معالجة الإشارات، 58(10)، 5016-5029، 201

    .. rubric:: أمثلة

    * انظر :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` للحصول على مثال حول كيفية تركيب كائن :class:`OAS` للبيانات.

    * انظر :ref:`sphx_glr_auto_examples_covariance_plot_lw_vs_oas.py` لتصور فرق متوسط الخطأ المربع بين مقدر :class:`LedoitWolf` ومقدر :class:`OAS` للتباين.


    .. figure:: ../auto_examples/covariance/images/sphx_glr_plot_lw_vs_oas_001.png
       :target: ../auto_examples/covariance/plot_lw_vs_oas.html
       :align: center
       :scale: 7


    .. _sparse_inverse_covariance:

    عكس التباين المتناثر
    
 إن مصفوفة العكسية لمصفوفة التباين، والتي غالباً ما تسمى مصفوفة الدقة، تتناسب طردياً مع مصفوفة الارتباط الجزئي. إنها تعطينا علاقة الاستقلال الجزئي. بمعنى آخر، إذا كانت هناك صفتان مستقلتان شرطياً عن الأخرى، فإن المعامل المناظر في مصفوفة الدقة سيكون صفراً. لهذا السبب، من المنطقي تقدير مصفوفة الدقة النادرة: يتم تحسين تقدير مصفوفة التباين من خلال تعلم علاقات الاستقلال من البيانات. وهذا ما يعرف باختيار التباين.

في حالة العينات الصغيرة، حيث يكون "عدد العينات" من مرتبة "عدد الميزات" أو أصغر، فإن مقاييس مصفوفة الدقة العكسية النادرة تعمل بشكل أفضل من مقاييس مصفوفة التباين المنكمشة. ومع ذلك، في الحالة المعاكسة، أو للبيانات المترابطة للغاية، يمكن أن تكون غير مستقرة رقمياً. بالإضافة إلى ذلك، على عكس مقاييس الانكماش، يمكن لمقاييس التقدير النادرة استرداد بنية خارج القطر.

يستخدم مقوم :class:`GraphicalLasso` عقوبة l1 لفرض الندرة على مصفوفة الدقة: كلما زادت قيمة معامل "ألفا" الخاص به، زادت ندرة مصفوفة الدقة. يستخدم كائن :class:`GraphicalLassoCV` المقابل التحقق المتقاطع لتعيين معامل "ألفا" تلقائياً.

.. figure:: ../auto_examples/covariance/images/sphx_glr_plot_sparse_cov_001.png
   :target: ../auto_examples/covariance/plot_sparse_cov.html
   :align: center
   :scale: 60%

   *مقارنة بين أقصى احتمال، الانكماش والتقديرات النادرة لمصفوفة التباين والدقة في إعدادات العينات الصغيرة جداً.*

.. note:: **استعادة البنية**

   استعادة بنية رسومية من الارتباطات في البيانات أمر صعب. إذا كنت مهتماً بمثل هذه الاستعادة، فضع في اعتبارك أن:

   * الاستعادة أسهل من مصفوفة ارتباط من مصفوفة التباين: قم بتوحيد ملاحظاتك قبل تشغيل :class:`GraphicalLasso`

   * إذا كان المخطط الأساسي يحتوي على عقد ذات اتصالات أكثر بكثير من العقدة المتوسطة، فسيفوت الخوارزمية بعض هذه الاتصالات.

   * إذا لم يكن عدد ملاحظاتك كبيراً مقارنة بعدد الحواف في الرسم البياني الأساسي، فلن تستعيده.

   * حتى لو كنت في ظروف استعادة مواتية، فإن معامل "ألفا" المختار بالتحقق المتقاطع (على سبيل المثال باستخدام كائن :class:`GraphicalLassoCV`) سيؤدي إلى اختيار الكثير من الحواف. ومع ذلك، فإن الحواف ذات الصلة سيكون لها أوزان أثقل من الحواف غير ذات الصلة.

الصياغة الرياضية هي كما يلي:

.. math::

    \hat{K} = \mathrm{argmin}_K \big(
                \mathrm{tr} S K - \mathrm{log} \mathrm{det} K
                + \alpha \|K\|_1
                \big)

حيث :math:`K` هي مصفوفة الدقة المراد تقديرها، و :math:`S` هي مصفوفة التباين للعينة. :math:`\|K\|_1` هو مجموع القيم المطلقة للمعاملات خارج القطر لـ :math:`K`. الخوارزمية المستخدمة لحل هذه المشكلة هي خوارزمية GLasso، من ورقة Friedman 2008 Biostatistics. وهي نفس الخوارزمية الموجودة في حزمة R "glasso".


.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_covariance_plot_sparse_cov.py`: مثال على بيانات صناعية تظهر بعض استعادة البنية، وتقارن بمقدرات التباين الأخرى.

* :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py`: مثال على بيانات سوق الأسهم الحقيقية، وإيجاد الرموز الأكثر ارتباطاً.

.. rubric:: المراجع

* Friedman et al, `"Sparse inverse covariance estimation with the
  graphical lasso" <https://biostatistics.oxfordjournals.org/content/9/3/432.short>`_,
  Biostatistics 9, pp 432, 2

.. _robust_covariance:

تقدير التباين القوي

    

غالبًا ما تكون مجموعات البيانات الحقيقية عرضة للقياس أو أخطاء التسجيل. قد تظهر أيضًا ملاحظات منتظمة ولكن غير شائعة لمجموعة متنوعة من الأسباب. تسمى الملاحظات النادرة جدًا بالنقاط الشاذة.

يكون مُقدّر التباين التجريبي ومقدرات التباين المنكمشة المذكورة أعلاه حساسة للغاية لوجود نقاط شاذة في البيانات. لذلك، يجب استخدام مقدرات تباين قوية لتقدير تباين مجموعات البيانات الحقيقية. بدلاً من ذلك، يمكن استخدام مقدرات التباين القوية لإجراء الكشف عن الشذوذ والتخلص من بعض الملاحظات أو تقليل وزنها وفقًا لمعالجة إضافية للبيانات.

يُنفّذ حزمة "sklearn.covariance" مُقدّرًا قويًا للتباين، وهو مُقدّر التباين الأدنى المحدد [3].


مُقدّر التباين الأدنى المحدد
------------------------------

مُقدّر التباين الأدنى المحدد هو مُقدّر قوي لتباين مجموعة البيانات الذي قدمه بي. جي. روسويو في [3]. الفكرة هي إيجاد نسبة معينة (ه) من الملاحظات "الجيدة" التي ليست شاذة وحساب مصفوفة التباين التجريبية الخاصة بها. ثم يتم إعادة ضبط مصفوفة التباين التجريبية لتعويض التحديد الذي تم إجراؤه للملاحظات ("خطوة الاتساق"). بعد حساب مُقدّر التباين الأدنى المحدد، يمكن إعطاء أوزان للملاحظات وفقًا لمسافة ماهالانوبيس الخاصة بها، مما يؤدي إلى إعادة تقدير مصفوفة التباين لمجموعة البيانات ("خطوة إعادة الترجيح").

طور روسويو وفان دريسين خوارزمية FastMCD لحساب التباين الأدنى المحدد [4]. يتم استخدام هذه الخوارزمية في Scikit-learn عند تركيب كائن MCD على البيانات. تحسب خوارزمية FastMCD أيضًا تقديرًا قويًا لموقع مجموعة البيانات في نفس الوقت.

يمكن الوصول إلى التقديرات الأولية كمعلمات "raw_location\_" و"raw_covariance\_" لكائن مُقدّر التباين القوي :class:`MinCovDet`.

.. rubric:: مراجع

.. [3] بي. جي. روسويو. الأقل وسطًا لتراجعات المربعات.
       J. Am Stat Ass، 79: 871، 1984.
.. [4] خوارزمية سريعة لمُقدّر التباين الأدنى المحدد،
       1999، الجمعية الأمريكية للإحصاء والجمعية الأمريكية للجودة، تكنوميتركس.

.. rubric:: أمثلة

* انظر :ref:`sphx_glr_auto_examples_covariance_plot_robust_vs_empirical_covariance.py` للحصول على مثال حول كيفية تركيب كائن :class:`MinCovDet` للبيانات ومعرفة كيف يظل التقدير دقيقًا على الرغم من وجود نقاط شاذة.

* انظر :ref:`sphx_glr_auto_examples_covariance_plot_mahalanobis_distances.py` لتصور الفرق بين مُقدّر التباين التجريبي :class:`EmpiricalCovariance` ومُقدّر التباين الأدنى المحدد :class:`MinCovDet` من حيث مسافة ماهالانوبيس (حتى نحصل على تقدير أفضل لمصفوفة الدقة أيضًا).

.. |robust_vs_emp| صورة:: ../auto_examples/covariance/images/sphx_glr_plot_robust_vs_empirical_covariance_001.png
   :الهدف: ../auto_examples/covariance/plot_robust_vs_empirical_covariance.html
   :مقياس: 49%

.. |mahalanobis| صورة:: ../auto_examples/covariance/images/sphx_glr_plot_mahalanobis_distances_001.png
   :الهدف: ../auto_examples/covariance/plot_mahalanobis_distances.html
   :مقياس: 49%



____

.. قائمة الجدول::
    :عنوان الصفوف: 1

    * - تأثير القيم المتطرفة على تقديرات الموقع والتباين
      - فصل القيم الداخلية عن القيم الشاذة باستخدام مسافة ماهالانوبيس

    * - |robust_vs_emp|
      - |mahalanobis|

    
