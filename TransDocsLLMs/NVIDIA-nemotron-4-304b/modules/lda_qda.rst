==========================================
التحليل التمييزي الخطي والتحليل التمييزي التربيعي
==========================================

.. currentmodule:: sklearn

التحليل التمييزي الخطي (:class:`~discriminant_analysis.LinearDiscriminantAnalysis`) والتحليل التمييزي التربيعي (:class:`~discriminant_analysis.QuadraticDiscriminantAnalysis`) هما تصنيفان كلاسيكيان، بأسطح قرار خطية وتربيعية على التوالي، كما توحي أسماؤهما.

هذه التصنيفات جذابة لأنها تحتوي على حلول مغلقة يمكن حسابها بسهولة، وهي متعددة الطبقات بطبيعتها، وقد ثبت أنها تعمل بشكل جيد في الممارسة العملية، وليس لديها أي فرط-معلمات لضبطها.

.. |ldaqda| image:: ../auto_examples/classification/images/sphx_glr_plot_lda_qda_001.png
        :target: ../auto_examples/classification/plot_lda_qda.html
        :scale: 80

.. centered:: |ldaqda|

يظهر الرسم البياني حدود القرار للتحليل التمييزي الخطي والتحليل التمييزي التربيعي. يوضح الصف السفلي أن التحليل التمييزي الخطي يمكنه فقط تعلم الحدود الخطية، في حين أن التحليل التمييزي التربيعي يمكنه تعلم الحدود التربيعية وبالتالي فهو أكثر مرونة.

.. rubric:: الأمثلة

* :ref:`sphx_glr_auto_examples_classification_plot_lda_qda.py`: مقارنة بين LDA و QDA على البيانات الاصطناعية.

تخفيض الأبعاد باستخدام التحليل التمييزي الخطي
===========================================================

يمكن استخدام :class:`~discriminant_analysis.LinearDiscriminantAnalysis` لإجراء تخفيض الأبعاد الخاضع للإشراف، عن طريق إسقاط بيانات الإدخال على الفضاء الخطي الفرعي الذي يتكون من الاتجاهات التي تزيد من الفصل بين الفئات (بمعنى دقيق تمت مناقشته في قسم الرياضيات أدناه). بالضرورة، يكون بعد المخرجات أقل من عدد الفئات، لذلك هذا بشكل عام تخفيض أبعاد قوي إلى حد ما، ولا معنى له إلا في إعداد متعدد الفئات.

يتم تنفيذ هذا في طريقة `transform`. يمكن تعيين الأبعاد المطلوبة باستخدام معامل ``n_components``. هذا المعامل ليس له أي تأثير على طريقتي `fit` و `predict`.

.. rubric:: الأمثلة

* :ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`: مقارنة بين LDA و PCA لتخفيض أبعاد مجموعة بيانات Iris

.. _lda_qda_math:

الصياغة الرياضية لتصنيفات LDA و QDA
=======================================================

    كل من LDA و QDA يمكن اشتقاقهما من نماذج احتمالية بسيطة والتي تقوم بنمذجة التوزيع الشرطي للبيانات :math:`P(X|y=k)` لكل فئة :math:`k`. يمكن بعد ذلك الحصول على التنبؤات باستخدام قاعدة بايز، لكل عينة تدريب :math:`x \in \mathcal{R}^d`:

    .. math::
        P(y=k | x) = \frac{P(x | y=k) P(y=k)}{P(x)} = \frac{P(x | y=k) P(y = k)}{ \sum_{l} P(x | y=l) \cdot P(y=l)}

    ونختار الفئة :math:`k` التي تزيد من احتمال هذا التوزيع البعدي.

    بشكل أكثر تحديدًا، بالنسبة لتحليل التمييز الخطي والتمييز التربيعي، يتم نمذجة :math:`P(x|y)` كتوزيع غاوسي متعدد المتغيرات بكثافة:

    .. math:: P(x | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k)\right)

    حيث :math:`d` هو عدد الميزات.

    QDA
    ---

    وفقًا للنموذج أعلاه، فإن لوغاريتم التوزيع البعدي هو:

    .. math::

        \log P(y=k | x) &= \log P(x | y=k) + \log P(y = k) + Cst \\
        &= -\frac{1}{2} \log |\Sigma_k| -\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k) + \log P(y = k) + Cst,

    حيث المصطلح الثابت :math:`Cst` يتوافق مع المقام :math:`P(x)`, بالإضافة إلى مصطلحات ثابتة أخرى من التوزيع الغاوسي. الفئة المتوقعة هي الفئة التي تزيد من لوغاريتم التوزيع البعدي هذا.

    .. note:: **العلاقة مع Gaussian Naive Bayes**

	  إذا افترضنا في نموذج QDA أن مصفوفات التباين قطرية،
	  فإن المدخلات يفترض أنها مستقلة شرطيًا في كل فئة،
	  ويكون المصنف الناتج مكافئًا لمصنف Gaussian Naive Bayes
	  :class:`naive_bayes.GaussianNB`.

    LDA
    ---

    LDA هي حالة خاصة من QDA، حيث يفترض أن التوزيعات الغاوسية لكل فئة تشترك في نفس مصفوفة التباين: :math:`\Sigma_k = \Sigma` لجميع :math:`k`. هذا يقلل من لوغاريتم التوزيع البعدي إلى:

    .. math:: \log P(y=k | x) = -\frac{1}{2} (x-\mu_k)^t \Sigma^{-1} (x-\mu_k) + \log P(y = k) + Cst.

    المصطلح :math:`(x-\mu_k)^t \Sigma^{-1} (x-\mu_k)` يتوافق مع
    `المسافة الماهالانوبية <https://en.wikipedia.org/wiki/Mahalanobis_distance>`_
    بين العينة :math:`x` والمتوسط :math:`\mu_k`. تخبرنا المسافة الماهالانوبية عن مدى قرب :math:`x` من :math:`\mu_k`, مع مراعاة تباين كل ميزة أيضًا. وبالتالي يمكننا تفسير LDA على أنه تعيين :math:`x` إلى الفئة التي يكون متوسطها هو الأقرب من حيث المسافة الماهالانوبية، مع مراعاة احتمالات الفئة المسبقة أيضًا.

    يمكن أيضًا كتابة لوغاريتم التوزيع البعدي لـ LDA [3]_ على أنه:

    .. math::

        \log P(y=k | x) = \omega_k^t x + \omega_{k0} + Cst.

    حيث :math:`\omega_k = \Sigma^{-1} \mu_k` و :math:`\omega_{k0} =
    -\frac{1}{2} \mu_k^t\Sigma^{-1}\mu_k + \log P (y = k)`. هذه الكميات
    تتوافق مع `coef_` و `intercept_` على التوالي.

    من الصيغة أعلاه، من الواضح أن LDA لديه سطح قرار خطي.
    في حالة QDA، لا توجد افتراضات على مصفوفات التباين
    :math:`\Sigma_k` للتوزيعات الغاوسية، مما يؤدي إلى أسطح قرار تربيعي.
    انظر [1]_ لمزيد من التفاصيل.

    الصياغة الرياضية لخفض أبعاد LDA
    =================================

    أولاً لاحظ أن المتوسطات :math:`\mu_k` هي متجهات في
    :math:`\mathcal{R}^d`, وهي تقع في فضاء afin :math:`H` من
    البعد على الأكثر :math:`K - 1` (نقطتان تقعان على خط، 3 نقاط تقع على مستوى، إلخ).

    كما ذكرنا أعلاه، يمكننا تفسير LDA على أنه تعيين :math:`x` إلى الفئة
    التي يكون متوسطها :math:`\mu_k` هو الأقرب من حيث المسافة الماهالانوبية،
    مع مراعاة احتمالات الفئة المسبقة أيضًا. بدلاً من ذلك، LDA
    يعادل أولاً *تكوير* البيانات بحيث تكون مصفوفة التباين هي الهوية،
    ثم تعيين :math:`x` إلى المتوسط الأقرب من حيث المسافة الإقليدية
    (لا يزال مع مراعاة احتمالات الفئة المسبقة).

    حساب المسافات الإقليدية في هذا الفضاء d-dimensional يعادل
    أولاً إسقاط نقاط البيانات في :math:`H`, وحساب المسافات
    هناك (نظرًا لأن الأبعاد الأخرى ستساهم بالتساوي في كل فئة من حيث المسافة). بمعنى آخر، إذا كان :math:`x` الأقرب إلى :math:`\mu_k`
    في الفضاء الأصلي، فسيكون الأمر كذلك في :math:`H`.
    هذا يوضح أنه، ضمنيًا في مصنف LDA،
    هناك تقليل للأبعاد عن طريق الإسقاط الخطي على
    فضاء :math:`K-1` البعدي.

    يمكننا تقليل البعد أكثر، إلى :math:`L` المختار، عن طريق الإسقاط
    على الفضاء الخطي :math:`H_L` الذي يزيد من تباين
    :math:`\mu^*_k` بعد الإسقاط (في الواقع، نحن نقوم بشكل من أشكال PCA للمتوسطات المحولة للفئة :math:`\mu^*_k`). هذا :math:`L` يتوافق مع
    المعلمة ``n_components`` المستخدمة في
    :func:`~discriminant_analysis.LinearDiscriminantAnalysis.transform` الطريقة. انظر
    [1]_ لمزيد من التفاصيل.

    الانكماش وتقدير التباين
    
    -------------------------------------------------------

    المراجع
    ---------

    .. [1] "The Elements of Statistical Learning", Trevor Hastie, Robert
           Tibshirani and Jerome Friedman, Springer, 2009.

    .. [2] "Pattern recognition and machine learning", Christopher M. Bishop,
           Springer, 2006.

    .. [3] "Linear Discriminant Analysis"
           https://en.wikipedia.org/wiki/Linear_discriminant_analysis
    
    .. [4] "Quadratic Discriminant Analysis"
           https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis
    
    .. [5] "Mahalanobis distance"
           https://en.wikipedia.org/wiki/Mahalanobis_distance
    
    .. [6] "Naive Bayes classifier"
           https://en.wikipedia.org/wiki/Naive_Bayes_classifier
    
    .. [7] "Principal component analysis"
           https://en.wikipedia.org/wiki/Principal_component_analysis

    انظر أيضًا

==================================

يستخدم الانكماش كشكل من أشكال التنظيم لتحسين تقدير مصفوفات التباين في الحالات التي يكون فيها عدد عينات التدريب صغيرًا مقارنة بعدد الميزات. في هذا السيناريو، يكون تقدير تباين العينة التجريبية ضعيفًا، ويساعد الانكماش على تحسين أداء تعميم المصنف.

يمكن استخدام انكماش LDA عن طريق تعيين معامل ``shrinkage`` في الفئة :class:`~discriminant_analysis.LinearDiscriminantAnalysis` إلى 'auto'. هذا يحدد تلقائيًا معامل الانكماش الأمثل بطريقة تحليلية بعد اللزمة التي قدمها Ledoit و Wolf [2]_. لاحظ أن الانكماش يعمل حاليًا فقط عند تعيين معامل ``solver`` إلى 'lsqr' أو 'eigen'.

يمكن أيضًا تعيين معامل ``shrinkage`` يدويًا بين 0 و 1. على وجه الخصوص، تتوافق القيمة 0 مع عدم وجود انكماش (مما يعني أنه سيتم استخدام مصفوفة التباين التجريبية) وتتوافق القيمة 1 مع الانكماش الكامل (مما يعني أنه سيتم استخدام المصفوفة القطرية للتباينات كمعيار لتقدير مصفوفة التباين). سيؤدي تعيين هذا المعامل إلى قيمة بين هذين الطرفين إلى تقدير نسخة منكمشة من مصفوفة التباين.

قد لا يكون مقدر التباين المنكمش Ledoit و Wolf هو الخيار الأفضل دائمًا. على سبيل المثال، إذا كان توزيع البيانات طبيعيًا، فإن مقدر الانكماش التقريبي Oracle :class:`sklearn.covariance.OAS` ينتج خطأ مربعيًا متوسطًا أصغر من الخطأ الذي تعطيه صيغة Ledoit و Wolf المستخدمة مع shrinkage="auto". في LDA، من المفترض أن تكون البيانات غاوسية بشكل شرطي للفئة. إذا كانت هذه الافتراضات صحيحة، فإن استخدام LDA مع مقدر التباين OAS سيعطي دقة تصنيف أفضل مما لو تم استخدام Ledoit و Wolf أو مقدر التباين التجريبي.

يمكن اختيار مقدر التباين باستخدام معامل ``covariance_estimator`` في الفئة :class:`discriminant_analysis.LinearDiscriminantAnalysis`. يجب أن يكون لدى مقدر التباين طريقة :term:`fit` وخاصية ``covariance_`` مثل جميع مقدرات التباين في وحدة :mod:`sklearn.covariance`.

.. |shrinkage| image:: ../auto_examples/classification/images/sphx_glr_plot_lda_001.png
        :target: ../auto_examples/classification/plot_lda.html
        :scale: 75

.. centered:: |shrinkage|

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_classification_plot_lda.py`: مقارنة مصنفات LDA مع مقدر التباين التجريبي، Ledoit Wolf و OAS.

خوارزميات التقدير
=====================

يتطلب استخدام LDA و QDA حساب اللوغاريتم الخلفي الذي يعتمد على الاحتمالات المسبقة للفئة :math:`P(y=k)`, والمتوسطات الفئوية :math:`\mu_k`, ومصفوفات التباين.

إن 'svd' هو الحل الافتراضي المستخدم لـ :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`, وهو الحل الوحيد المتاح لـ :class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`. يمكنه إجراء كل من التصنيف والتحويل (لـ LDA). نظرًا لأنه لا يعتمد على حساب مصفوفة التباين، فقد يكون الحل 'svd' مفضلًا في المواقف التي يكون فيها عدد الميزات كبيرًا. لا يمكن استخدام الحل 'svd' مع الانكماش.

بالنسبة لـ QDA، يعتمد استخدام حل SVD على حقيقة أن مصفوفة التباين :math:`\Sigma_k` تساوي، بحكم التعريف، :math:`\frac{1}{n - 1} X_k^tX_k = \frac{1}{n - 1} V S^2 V^t` حيث :math:`V` يأتي من SVD للمصفوفة (المركزة): :math:`X_k = U S V^t`. اتضح أنه يمكننا حساب اللوغاريتم الخلفي أعلاه دون الحاجة إلى حساب :math:`\Sigma` صراحةً: حساب :math:`S` و :math:`V` عبر SVD لـ :math:`X` يكفي. بالنسبة لـ LDA، يتم حساب SVDs: SVD للمصفوفة المدخلة المركزة :math:`X` و SVD لمتوسطات الفئة.

إن 'lsqr' هو خوارزمية فعالة تعمل فقط للتصنيف. يحتاج إلى حساب مصفوفة التباين :math:`\Sigma` صراحةً، ويدعم الانكماش ومقدرات التباين المخصصة. يحسب هذا الحل المعاملات :math:`\omega_k = \Sigma^{-1}\mu_k` عن طريق الحل لـ :math:`\Sigma \omega = \mu_k`, وبالتالي تجنب الحساب الصريح للمعكوس :math:`\Sigma^{-1}`.

يعتمد حل 'eigen' على تحسين نسبة التشتت بين الفئات إلى التشتت داخل الفئة. يمكن استخدامه لكل من التصنيف والتحويل، ويدعم الانكماش. ومع ذلك، يحتاج حل 'eigen' إلى حساب مصفوفة التباين، لذلك قد لا يكون مناسبًا للمواقف التي تحتوي على عدد كبير من الميزات.

.. rubric:: المراجع

.. [1] "The Elements of Statistical Learning", Hastie T., Tibshirani R.,
    Friedman J., Section 4.3, p.106-119, 2008.

.. [2] Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix.
    The Journal of Portfolio Management 30(4), 110-119, 2004.

.. [3] R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification
    (Second Edition), section 2.6.2.
    
