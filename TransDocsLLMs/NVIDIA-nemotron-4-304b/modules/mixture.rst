    .. _mixture:

    .. _gmm:

    =======================
    نماذج خليط غاوسي
    =======================

    .. currentmodule:: sklearn.mixture

    ``sklearn.mixture`` هي حزمة تمكن المستخدم من تعلم نماذج خليط غاوسي (دعم مصفوفات التباين القطرية، الكروية، المرتبطة والكاملة)، أخذ عينات منها، وتقديرها من البيانات. كما يتم توفير تسهيلات للمساعدة في تحديد عدد المكونات المناسبة.

    .. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_pdf_001.png
      :target: ../auto_examples/mixture/plot_gmm_pdf.html
      :align: center
      :scale: 50%

      **نموذج خليط غاوسي مكون من عنصرين:** *نقاط البيانات، والأسطح ذات الاحتمالية المتساوية للنموذج.*

    نموذج الخليط الغاوسي هو نموذج احتمالي يفترض أن جميع نقاط البيانات يتم توليدها من خليط من عدد محدود من التوزيعات الغاوسية ذات المعلمات غير المعروفة. يمكن التفكير في نماذج الخليط على أنها تعميم لتجميع k-means لإدراج معلومات حول بنية التباين للبيانات بالإضافة إلى مراكز التوزيعات الغاوسية الكامنة.

    تقوم سكيكيت-ليرن بتنفيذ فئات مختلفة لتقدير نماذج الخليط الغاوسي، والتي تتوافق مع استراتيجيات التقدير المختلفة، المفصلة أدناه.

    خليط غاوسي
        
================

يقوم كائن :class:`GaussianMixture` بتنفيذ خوارزمية :ref:`expectation-maximization <expectation_maximization>` (EM) لتناسب نماذج خليط غاوسي. يمكنه أيضًا رسم قطع ناقص الثقة للنماذج متعددة المتغيرات ، وحساب معيار معلومات Bayesian لتقييم عدد العناقيد في البيانات. يتم توفير طريقة :meth:`GaussianMixture.fit` التي تتعلم نموذج خليط غاوسي من بيانات التدريب. بالنظر إلى بيانات الاختبار ، يمكنه تعيين كل عينة إلى غاوسي الذي ينتمي إليه على الأرجح باستخدام طريقة :meth:`GaussianMixture.predict`.

..
    بدلاً من ذلك ، يمكن استرداد احتمال كل عينة تنتمي إلى مختلف غاوسي باستخدام
    طريقة :meth:`GaussianMixture.predict_proba`.

يأتي :class:`GaussianMixture` مع خيارات مختلفة لتقييد
التباين في فئات الاختلاف المقدرة: كروية ، قطرية ، مربوطة أو
تباين كامل.

.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_covariances_001.png
   :target: ../auto_examples/mixture/plot_gmm_covariances.html
   :align: center
   :scale: 75%

.. rubric:: أمثلة

* انظر :ref:`sphx_glr_auto_examples_mixture_plot_gmm_covariances.py` للحصول على مثال عن
  استخدام خليط غاوسي كتجميع على مجموعة بيانات القزحية.

* انظر :ref:`sphx_glr_auto_examples_mixture_plot_gmm_pdf.py` للحصول على مثال حول رسم
  تقدير الكثافة.

.. dropdown:: إيجابيات وسلبيات فئة GaussianMixture

  .. rubric:: إيجابيات

  :السرعة: إنها أسرع خوارزمية لتعلم نماذج الخليط

  :لا ديني: نظرًا لأن هذه الخوارزمية تزيد من الاحتمال فقط ، فلن
    تحيز الوسائل نحو الصفر ، أو تحيز أحجام العناقيد إلى
    لديها هياكل محددة قد تنطبق أو لا تنطبق.

  .. rubric:: سلبيات

  :التفردات: عندما يكون لدى المرء نقاط غير كافية لكل
    خليط ، يصبح تقدير مصفوفات التباين أمرًا صعبًا ،
    من المعروف أن الخوارزمية تتباعد وتجد حلولًا ذات احتمالية لا نهائية ما لم
    يقوم المرء بتنظيم التباينات بشكل مصطنع.

  :عدد المكونات: ستستخدم هذه الخوارزمية دائمًا جميع
    المكونات التي يمكنها الوصول إليها ، وتحتاج إلى بيانات محجوزة
    أو معايير نظرية المعلومات لتحديد عدد المكونات التي يجب استخدامها
    في غياب الإشارات الخارجية.

.. dropdown:: اختيار عدد المكونات في نموذج خليط غاوسي الكلاسيكي

  يمكن استخدام معيار BIC لاختيار عدد المكونات في خليط غاوسي
  بطريقة فعالة. من الناحية النظرية ، فإنه يستعيد العدد الحقيقي للمكونات فقط في
  النظام اللامتناهي (أي إذا كانت هناك الكثير من البيانات المتاحة و
  على افتراض أن البيانات تم إنشاؤها بالفعل بشكل مستقل من خليط من التوزيع الغاوسي).
  لاحظ أن استخدام :ref:`خليط غاوسي بايزي المتغير <bgmm>`
  يتجنب تحديد عدد المكونات لنموذج خليط غاوسي.

  .. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_selection_002.png
    :target: ../auto_examples/mixture/plot_gmm_selection.html
    :align: center
    :scale: 50%

  .. rubric:: أمثلة

  * انظر :ref:`sphx_glr_auto_examples_mixture_plot_gmm_selection.py` للحصول على مثال
    عن اختيار النموذج الذي تم إجراؤه باستخدام خليط غاوسي الكلاسيكي.

.. _expectation_maximization:

.. dropdown:: خوارزمية تقدير التوقع والتعظيم

  الصعوبة الرئيسية في تعلم نماذج خليط غاوسي من غير المسمى
  البيانات هي أن المرء عادة لا يعرف النقاط التي جاءت من
  أي مكون كامن (إذا كان المرء لديه حق الوصول إلى هذه المعلومات يصبح من السهل جدًا ملاءمة
  توزيع غاوسي منفصل لكل مجموعة من
  النقاط). `توقع-تعظيم
  <https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm>`_
  هي خوارزمية إحصائية جيدة التأسيس
  لتجاوز هذه المشكلة من خلال عملية تكرارية. أولاً
  يفترض المرء مكونات عشوائية (مركزة عشوائيًا على نقاط البيانات ،
  تتعلم من k-means ، أو حتى موزعة بشكل طبيعي حول
  الأصل) وتحسب لكل نقطة احتمال أن يتم إنشاؤها بواسطة
  كل مكون من مكونات النموذج. ثم ، يقوم المرء بضبط
  المعلمات لزيادة احتمالية البيانات بالنظر إلى تلك
  التعيينات. تكرار هذه العملية مضمون دائمًا للتقارب
  إلى الأمثل المحلي.

.. dropdown:: اختيار طريقة التهيئة

  هناك خيار من أربع طرق تهيئة (بالإضافة إلى إدخال وسائط محددة من قبل المستخدم
  الوسائل الأولية) لتوليد المراكز الأولية لمكونات النموذج:

  k-means (افتراضي)
    هذا يطبق خوارزمية تجميع k-means التقليدية.
    يمكن أن يكون هذا مكلفًا حسابيًا مقارنة بطرق التهيئة الأخرى.

  k-means++
    هذا يستخدم طريقة التهيئة لتجميع k-means: k-means++.
    سيختار هذا المركز الأول عشوائيًا من البيانات. سيتم اختيار المراكز اللاحقة من
    توزيع مرجح للبيانات يفضل النقاط الأبعد عن
    المراكز الحالية. k-means ++ هي التهيئة الافتراضية لـ k-means لذلك ستكون
    أسرع من تشغيل k-means الكامل ولكن لا يزال بإمكانها أخذ كمية كبيرة من
    الوقت لمجموعات البيانات الكبيرة مع العديد من المكونات.
    

random_from_data
    سيتم اختيار نقاط بيانات عشوائية من بيانات الإدخال كمراكز أولية. هذه طريقة سريعة جدًا للتحضيرات الأولية ولكن يمكن أن تنتج نتائج غير متقاربة إذا كانت النقاط المختارة قريبة جدًا من بعضها البعض.

random
    يتم اختيار المراكز كاضطراب صغير بعيدًا عن متوسط جميع البيانات. هذه الطريقة بسيطة ولكن يمكن أن تؤدي إلى استغراق النموذج وقتًا أطول للتقارب.

.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_init_001.png
    :target: ../auto_examples/mixture/plot_gmm_init.html
    :align: center
    :scale: 50%

.. rubric:: أمثلة

* انظر :ref:`sphx_glr_auto_examples_mixture_plot_gmm_init.py` للحصول على مثال عن استخدام تحضيرات أولية مختلفة في خليط غاوسي.

.. _bgmm:

خليط غاوسي بايزي التبديلي
    
=====================================

يُنفذ كائن :class:`BayesianGaussianMixture` نموذجًا بديلاً لنموذج الخليط الغاوسي مع خوارزميات الاستدلال التبايني. يشبه واجهة برمجة التطبيقات تلك التي حددها :class:`GaussianMixture`.

.. _variational_inference:

**خوارزمية التقدير: الاستدلال التبايني**

الاستدلال التبايني هو امتداد لخوارزمية التوقعات والتعظيم التي تزيد من الحد الأدنى لبيانات نموذج الأدلة (بما في ذلك التوزيعات الأولية) بدلاً من احتمالية البيانات. المبدأ وراء الطرق التباينية هو نفسه مبدأ التوقعات والتعظيم (أي كلاهما خوارزميات تكرارية تتناوب بين إيجاد الاحتمالات لكل نقطة يتم إنشاؤها بواسطة كل خليط وتناسب الخليط مع هذه النقاط المخصصة)، ولكن الطرق التباينية تضيف التنظيم من خلال دمج المعلومات من التوزيعات الأولية. هذا يتجنب التفردات التي غالبًا ما توجد في حلول التوقعات والتعظيم ولكنها تقدم بعض التحيزات الدقيقة للنموذج. غالبًا ما يكون الاستدلال أبطأ بشكل ملحوظ، ولكن ليس عادةً بقدر ما يجعل الاستخدام غير عملي.

نظرًا لطبيعته البيزية، يحتاج الخوارزمية التباينية إلى المزيد من المعلمات الفوقية أكثر من التوقعات والتعظيم، وأهمها معامل التركيز ``weight_concentration_prior``. تحديد قيمة منخفضة لمعامل التركيز المسبق سيجعل النموذج يضع معظم الوزن على بعض المكونات ويعيّن أوزان المكونات المتبقية قريبة جدًا من الصفر. ستسمح القيم العالية لمعامل التركيز المسبق لعدد أكبر من المكونات بأن تكون نشطة في الخليط.

تقترح معلمات تنفيذ فئة :class:`BayesianGaussianMixture` نوعين من التوزيعات الأولية لتوزيع الأوزان: نموذج خليط محدود مع توزيع ديريتشليت ونموذج خليط لانهائي مع عملية ديريتشليت. في الواقع، يتم تقريب خوارزمية استدلال عملية ديريتشليت وتستخدم توزيعًا مقطوعًا بعدد ثابت من المكونات (يُسمى تمثيل كسر العصا). عدد المكونات المستخدمة بالفعل يعتمد دائمًا على البيانات.

تقارن الصورة التالية النتائج التي تم الحصول عليها لأنواع مختلفة من معامل التركيز المسبق للوزن (المعلمة ``weight_concentration_prior_type``) لقيم مختلفة من ``weight_concentration_prior``. هنا، يمكننا أن نرى أن قيمة معامل التركيز المسبق للوزن لها تأثير قوي على عدد المكونات النشطة الفعلي الذي تم الحصول عليه. يمكننا أيضًا أن نلاحظ أن القيم الكبيرة لمعامل التركيز المسبق للوزن تؤدي إلى أوزان أكثر تجانسًا عندما يكون نوع التوزيع الأولي 'dirichlet_distribution' بينما هذا ليس بالضرورة هو الحال بالنسبة لنوع 'dirichlet_process' (المستخدم افتراضيًا).

.. |plot_bgmm| image:: ../auto_examples/mixture/images/sphx_glr_plot_concentration_prior_001.png
   :target: ../auto_examples/mixture/plot_concentration_prior.html
   :scale: 48%

.. |plot_dpgmm| image:: ../auto_examples/mixture/images/sphx_glr_plot_concentration_prior_002.png
   :target: ../auto_examples/mixture/plot_concentration_prior.html
   :scale: 48%

.. centered:: |plot_bgmm| |plot_dpgmm|

تقارن الأمثلة أدناه نماذج الخليط الغاوسي بعدد ثابت من المكونات، مع نماذج الخليط الغاوسي التبايني مع توزيع أولي لعملية ديريتشليت. هنا، يتم تركيب خليط غاوسي كلاسيكي مع 5 مكونات على مجموعة بيانات تتكون من مجموعتين. يمكننا أن نرى أن الخليط الغاوسي التبايني مع توزيع أولي لعملية ديريتشليت قادر على الحد من نفسه إلى مكونين فقط بينما يلائم الخليط الغاوسي البيانات بعدد ثابت من المكونات التي يجب تعيينها مسبقًا من قبل المستخدم. في هذه الحالة، حدد المستخدم ``n_components=5`` والتي لا تتطابق مع التوزيع التوليدي الحقيقي لهذه البيانات التجريبية. لاحظ أنه مع القليل جدًا من الملاحظات، يمكن لنماذج الخليط الغاوسي التبايني مع توزيع أولي لعملية ديريتشليت أن تتخذ موقفًا متحفظًا، وتلائم مكونًا واحدًا فقط.

.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_001.png
   :target: ../auto_examples/mixture/plot_gmm.html
   :align: center
   :scale: 70%


في الشكل التالي، نقوم بتركيب مجموعة بيانات لا يتم تصويرها جيدًا بواسطة خليط غاوسي. يؤدي ضبط ``weight_concentration_prior``، معلمة :class:`BayesianGaussianMixture`، إلى التحكم في عدد المكونات المستخدمة لتركيب هذه البيانات. نقدم أيضًا في آخر رسمين بيانيين عينات عشوائية تم إنشاؤها من الخليطين الناتجين.

.. figure:: ../auto_examples/mixture/images/sphx_glr_plot_gmm_sin_001.png
   :target: ../auto_examples/mixture/plot_gmm_sin.html
   :align: center
   :scale: 65%



.. rubric:: أمثلة

* انظر :ref:`sphx_glr_auto_examples_mixture_plot_gmm.py` للحصول على مثال حول
  رسم بيضاويات الثقة لكل من :class:`GaussianMixture`
  و :class:`BayesianGaussianMixture`.

* :ref:`sphx_glr_auto_examples_mixture_plot_gmm_sin.py` يوضح استخدام
  :class:`GaussianMixture` و :class:`BayesianGaussianMixture` لتركيب موجة جيبية.
    

* انظر :ref:`sphx_glr_auto_examples_mixture_plot_concentration_prior.py` للحصول على مثال يوضح كيفية رسم الإهليلجيات الثقة لـ :class:`BayesianGaussianMixture` مع أنواع مختلفة من ``weight_concentration_prior_type`` لقيم مختلفة من معامل ``weight_concentration_prior``.

.. dropdown:: إيجابيات وسلبيات الاستدلال المتغير مع BayesianGaussianMixture

  .. rubric:: الإيجابيات

  :الاختيار التلقائي: عندما يكون ``weight_concentration_prior`` صغيرًا بدرجة كافية و ``n_components`` أكبر من ما هو ضروري حسب النموذج، فإن نموذج الخليط البايزي المتغير لديه ميل طبيعي لتعيين بعض قيم أوزان الخليط قريبة من الصفر. هذا يجعل من الممكن السماح للنموذج باختيار عدد مناسب من المكونات الفعالة تلقائيًا. تحتاج فقط إلى توفير حد أعلى لهذا الرقم. ومع ذلك، تجدر الإشارة إلى أن "العدد المثالي" للمكونات النشطة يعتمد بشكل كبير على التطبيق وغالبًا ما يكون غير محدد جيدًا في إعداد استكشاف البيانات.

  :أقل حساسية لعدد المعلمات: على عكس النماذج المحدودة، التي ستستخدم تقريبًا جميع المكونات قدر الإمكان، وبالتالي ستنتج حلولًا مختلفة تمامًا لأعداد مختلفة من المكونات، فإن الاستدلال المتغير مع عملية Dirichlet المسبقة (``weight_concentration_prior_type='dirichlet_process'``) لن يتغير كثيرًا مع التغييرات في المعلمات، مما يؤدي إلى مزيد من الاستقرار وأقل ضبطًا.

  :التنظيم: نظرًا لدمج المعلومات المسبقة، فإن الحلول المتغيرة لها حالات خاصة مرضية أقل من حلول تعظيم التوقع.

  .. rubric:: السلبيات

  :السرعة: المعلمات الإضافية اللازمة للاستدلال المتغير تجعل الاستدلال أبطأ، وإن لم يكن بكثير.

  :المعلمات الفائقة: تحتاج هذه الخوارزمية إلى معلمة فائقة إضافية قد تحتاج إلى ضبط تجريبي عبر التحقق المتقاطع.

  :الانحياز: هناك العديد من التحيزات الضمنية في خوارزميات الاستدلال (وكذلك في عملية Dirichlet إذا تم استخدامها)، وكلما كان هناك عدم تطابق بين هذه التحيزات والبيانات، قد يكون من الممكن تركيب نماذج أفضل باستخدام خليط محدود.

.. _dirichlet_process:

عملية Dirichlet
    
فيما يلي وصف لخوارزميات الاستدلال التبايني على خليط العملية الديريشليه. تعد العملية الديريشليه توزيع احتمالي مسبق على *التجمعات بعدد غير محدود من الأقسام*. تتيح لنا التقنيات التباينية دمج هذا الهيكل المسبق على نماذج الخليط الغاوسي دون أي عقوبة تقريبًا في وقت الاستدلال، مقارنة بنموذج الخليط الغاوسي المحدود.

سؤال مهم هو كيف يمكن لعملية الديريشليه استخدام عدد غير محدود من التجمعات ولا تزال متسقة. على الرغم من أن التفسير الكامل لا يناسب هذا الدليل، يمكن للمرء أن يفكر في تشبيه "عملية كسر العصا" لمساعدته على فهمها. عملية كسر العصا هي قصة مولدة لعملية الديريشليه. نبدأ بعصا طولها وحدة واحدة وفي كل خطوة نكسر جزءًا من العصا المتبقية. في كل مرة، نربط طول قطعة العصا بنسبة النقاط التي تقع في مجموعة من الخليط. في النهاية، لتمثيل الخليط اللانهائي، نربط آخر قطعة متبقية من العصا بنسبة النقاط التي لا تقع في جميع المجموعات الأخرى. طول كل قطعة هو متغير عشوائي باحتمالية تتناسب مع معامل التركيز. ستقسم القيم الأصغر للتركيز وحدة الطول إلى قطع أكبر من العصا (محددة توزيعًا أكثر تركيزًا). ستخلق قيم التركيز الأكبر قطعًا أصغر من العصا (مما يزيد من عدد المكونات ذات الأوزان غير الصفرية).

لا تزال تقنيات الاستدلال التبايني لعملية الديريشليه تعمل مع تقريب محدود لنموذج الخليط اللانهائي هذا، ولكن بدلاً من الاضطرار إلى تحديد عدد المكونات مسبقًا، يحدد المرء فقط معامل التركيز والحد الأعلى لعدد مكونات الخليط (هذا الحد الأعلى، على افتراض أنه أعلى من "الحقيقي" عدد المكونات، يؤثر فقط على التعقيد الخوارزمي، وليس العدد الفعلي للمكونات المستخدمة).
