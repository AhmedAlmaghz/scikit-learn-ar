.. _sgd:

===========================
انحدار تدرج الاحتمالية العشوائي
===========================

.. currentmodule:: sklearn.linear_model

**انحدار تدرج الاحتمالية العشوائي (SGD)** هو نهج بسيط ولكنه فعال للغاية
لملاءمة المصنفات والمنقبات الخطية تحت
دالات خسارة محدبة مثل (الخطية) `Support Vector Machines
<https://en.wikipedia.org/wiki/Support_vector_machine>`_ و `Logistic
Regression <https://en.wikipedia.org/wiki/Logistic_regression>`_.
على الرغم من أن SGD موجود منذ فترة طويلة في مجتمع تعلم الآلة،
إلا أنه حظي مؤخرًا باهتمام كبير في سياق التعلم واسع النطاق.

تم تطبيق SGD بنجاح على مشكلات التعلم الآلي واسعة النطاق والمفرطة
التي يتم مواجهتها غالبًا في تصنيف النصوص ومعالجة اللغات الطبيعية.
ونظرًا لأن البيانات مفرطة، فإن المصنفات في هذا النموذج تتوسع بسهولة
لتشمل مشكلات بها أكثر من 10^5 مثال تدريبي وأكثر من 10^5 خاصية.

وبالحديث الدقيق، فإن SGD هو مجرد تقنية تحسين ولا يتوافق مع عائلة
محددة من نماذج التعلم الآلي. إنها مجرد
*طريقة* لتدريب نموذج. وغالبًا ما يكون لدى مثيل من :class:`SGDClassifier` أو
:class:`SGDRegressor` مكافئًا في أداة تقدير
واجهة برمجة التطبيقات scikit-learn، باستخدام تقنية تحسين مختلفة.
على سبيل المثال، يؤدي استخدام `SGDClassifier(loss='log_loss')` إلى الانحدار اللوجستي،
أي نموذج مكافئ لـ :class:`~sklearn.linear_model.LogisticRegression`
الذي يتم ملاؤه عبر SGD بدلاً من ملؤه بواسطة أحد المحللين الآخرين
في :class:`~sklearn.linear_model.LogisticRegression`. وبالمثل،
`SGDRegressor(loss='squared_error', penalty='l2')` و
:class:`~sklearn.linear_model.Ridge` يحلان نفس مشكلة التحسين، بوسائل مختلفة.

مزايا انحدار تدرج الاحتمالية العشوائي هي:

+ الكفاءة.

+ سهولة التنفيذ (الكثير من الفرص لضبط الكود).

وتشمل عيوب انحدار تدرج الاحتمالية العشوائي ما يلي:

+ يتطلب SGD عددًا من فرط المعلمات مثل معامل التنظيم
وعدد التكرارات.

+ SGD حساس لمقياس الخاصية.

.. warning::

  تأكد من أنك تقوم بتبديل (خلط) بيانات التدريب الخاصة بك قبل ملاءمة النموذج
  أو استخدام ``shuffle=True`` للخلط بعد كل تكرار (يتم استخدامه بشكل افتراضي).
  أيضًا، من الناحية المثالية، يجب توحيد المقاييس باستخدام e.g.
  `make_pipeline(StandardScaler(), SGDClassifier())` (راجع: :ref:`Pipelines
  <combining_estimators>`).

التصنيف
==============


تنفذ الفئة :class:`SGDClassifier` روتين تعلم الانحدار التدريجي العشوائي العادي
الذي يدعم دالات الخسارة والعقوبات المختلفة للتصنيف. وفيما يلي حد القرار
لمصنف :class:`SGDClassifier` تم تدريبه بخسارة المفصل، وهو ما يعادل SVM الخطي.

.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_sgd_separating_hyperplane_001.png
   :target: ../auto_examples/linear_model/plot_sgd_separating_hyperplane.html
   :align: center
   :scale: 75

وكما هو الحال مع المصنفات الأخرى، يجب ملاءمة SGD باستخدام صفيفين: صفيف `X`
بشكل (n_samples، n_features) يحتوي على عينات التدريب، وصفيف y بشكل (n_samples،)
يحتوي على القيم المستهدفة (ملصقات الفئات)
لعينات التدريب::

    >>> from sklearn.linear_model import SGDClassifier
    >>> X = [[0., 0.], [1., 1.]]
    >>> y = [0, 1]
    >>> clf = SGDClassifier(loss="hinge", penalty="l2", max_iter=5)
    >>> clf.fit(X, y)
    SGDClassifier(max_iter=5)


بعد الملاءمة، يمكن بعد ذلك استخدام النموذج للتنبؤ بقيم جديدة::

    >>> clf.predict([[2., 2.]])
    array([1])

يحدد SGD نموذجًا خطيًا لبيانات التدريب. يحتوي معامل ``coef_`` على
معلمات النموذج::

    >>> clf.coef_
    array([[9.9..., 9.9...]])

يحتوي معامل ``intercept_`` على المقاطعة (المعروفة أيضًا باسم الإزاحة أو الانحياز)::

    >>> clf.intercept_
    array([-9.9...])

ما إذا كان يجب على النموذج استخدام مقاطعة، أي فراغ متحيز،
يتم التحكم فيه بواسطة معامل ``fit_intercept``.

تعطي دالة :meth:`SGDClassifier.decision_function` المسافة الموقعة إلى الفراغ
(المحسوبة على أنها جداء داخلي بين المعاملات والعينة المدخلة، بالإضافة إلى المقاطعة)::

    >>> clf.decision_function([[2., 2.]])
    array([29.6...])

يمكن تعيين دالة الخسارة الملموسة عبر معامل ``loss``. تدعم الفئة :class:`SGDClassifier`
دالات الخسارة التالية:

* ``loss="hinge"``: (soft-margin) linear Support Vector Machine،
* ``loss="modified_huber"``: smoothed hinge loss،
* ``loss="log_loss"``: logistic regression،
* and all regression losses below. In this case the target is encoded as -1
  or 1, and the problem is treated as a regression problem. The predicted
  class then correspond to the sign of the predicted target.

يرجى الرجوع إلى القسم الرياضي أدناه
<sgd_mathematical_formulation> للصيغ.
دالتا الخسارة الأوليتان كسولتان، فهما لا تحدثان تحديثات لمعلمات النموذج
إلا إذا انتهك مثال ما قيد الهامش، مما يجعل التدريب فعالاً للغاية وقد يؤدي إلى
نماذج أكثر ندرة (أي بمعاملات صفرية أكثر)، حتى عند استخدام عقوبة L2.

يتيح استخدام ``loss="log_loss"`` أو ``loss="modified_huber"`` طريقة
``predict_proba``، والتي تعطي متجهًا من تقديرات الاحتمالية
:math:`P(y|x)` لكل عينة :math:`x`::

    >>> clf = SGDClassifier(loss="log_loss", max_iter=5).fit(X, y)
    >>> clf.predict_proba([[1., 1.]]) # doctest: +SKIP
    array([[0.00..., 0.99...]])

يمكن تعيين العقوبة الملموسة عبر معامل ``penalty``.
يدعم SGD العقوبات التالية:

* ``penalty="l2"``: L2 norm penalty on ``coef_``.
* ``penalty="l1"``: L1 norm penalty on ``coef_``.
* ``penalty="elasticnet"``: Convex combination of L2 and L1;
  ``(1 - l1_ratio) * L2 + l1_ratio * L1``.

الإعداد الافتراضي هو ``penalty="l2"``. تؤدي عقوبة L1 إلى حلول نادرة،
مما يؤدي إلى جعل معظم المعاملات صفرية. وتعالج شبكة المطاط بعض أوجه القصور
في عقوبة L1 في وجود سمات شديدة الارتباط. يتحكم معامل ``l1_ratio`` في
الدمج المحدب للعقوبتين L1 وL2.

تدعم الفئة :class:`SGDClassifier` التصنيف متعدد الفئات عن طريق الجمع
بين مصنفات ثنائية متعددة في مخطط "واحد مقابل الجميع" (OVA). لكل
من الفئات :math:`K`، يتم تعلم مصنف ثنائي يميز تلك الفئة عن جميع الفئات الأخرى
:math:`K-1`. وفي وقت الاختبار، نحسب درجة الثقة (أي المسافات الموقعة إلى الفراغ)
لكل مصنف ونختار الفئة ذات الثقة الأعلى. توضح الصورة أدناه نهج OVA
على مجموعة بيانات Iris. تمثل الخطوط المتقطعة المصنفات الثلاثة OVA؛
توضح ألوان الخلفية سطح القرار الذي تسببه المصنفات الثلاثة.

.. figure:: ../auto_examples/linear_model/images/sphx_glr_plot_sgd_iris_001.png
   :target: ../auto_examples/linear_model/plot_sgd_iris.html
   :align: center
   :scale: 75

في حالة التصنيف متعدد الفئات، يكون ``coef_`` مصفوفة ثنائية الأبعاد
بشكل (n_classes، n_features) و ``intercept_`` مصفوفة أحادية البعد
بشكل (n_classes،). يحتوي الصف i-th من ``coef_`` على
متجه الأوزان لمصنف OVA للفئة i-th؛ يتم فهرسة الفئات بترتيب تصاعدي
(راجع معامل "classes_").
لاحظ أنه، من حيث المبدأ، نظرًا لأنها تسمح بإنشاء نموذج احتمالي،
فإن ``loss="log_loss"`` و ``loss="modified_huber"`` أكثر ملاءمة
للتصنيف أحادي مقابل الجميع.

تدعم الفئة :class:`SGDClassifier` كلاً من الفئات المرجحة والعينات المرجحة
عبر معلمي الملاءمة ``class_weight`` و ``sample_weight``. راجع
الأمثلة أدناه ووثائق سلسلة طرق :meth:`SGDClassifier.fit` لمزيد من المعلومات.

تدعم الفئة :class:`SGDClassifier` متوسط SGD (ASGD) [#4]_. يمكن تمكين المتوسط
بوضع `average=True`. يؤدي ASGD نفس التحديثات مثل
SGD العادي (راجع: :ref:`sgd_mathematical_formulation`)، ولكن بدلاً من استخدام
القيمة الأخيرة للمعاملات كمعامل ``coef_`` (أي قيم
التحديث الأخير)، يتم تعيين ``coef_`` بدلاً من ذلك إلى متوسط
قيم المعاملات عبر جميع التحديثات. ويتم الشيء نفسه بالنسبة لمعامل ``intercept_``.
عند استخدام ASGD، يمكن أن يكون معدل التعلم أكبر وحتى ثابتًا، مما يؤدي في بعض
مجموعات البيانات إلى تسريع وقت التدريب.

بالنسبة للتصنيف بخسارة لوجستية، هناك متغير آخر من SGD مع استراتيجية
المتوسط متاح بخوارزمية متوسط التدرج العشوائي (SAG)،
متاح كمُحسِّن في :class:`LogisticRegression`.

.. rubric:: أمثلة

- :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_separating_hyperplane.py`
- :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_iris.py`
- :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_weighted_samples.py`
- :ref:`sphx_glr_auto_examples_linear_model_plot_sgd_comparison.py`
- :ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane_unbalanced.py`
  (راجع الملاحظة في المثال)

الانحدار
==========

تنفذ الفئة :class:`SGDRegressor` روتين تعلم الانحدار التدريجي العشوائي العادي
الذي يدعم دالات الخسارة والعقوبات المختلفة لملاءمة نماذج الانحدار الخطي.
الفئة :class:`SGDRegressor` مناسبة لمشكلات الانحدار التي بها عدد كبير من
عينات التدريب (> 10.000)، وبالنسبة للمشكلات الأخرى، نوصي باستخدام الفئات
:class:`Ridge`، أو :class:`Lasso`، أو :class:`ElasticNet`.

يمكن تعيين دالة الخسارة الملموسة عبر معامل ``loss``. تدعم الفئة :class:`SGDRegressor`
دالات الخسارة التالية:

* ``loss="squared_error"``: Ordinary least squares،
* ``loss="huber"``: Huber loss for robust regression،
* ``loss="epsilon_insensitive"``: linear Support Vector Regression.

يرجى الرجوع إلى القسم الرياضي أدناه
<sgd_mathematical_formulation> للصيغ.
يمكن استخدام دالتَي الخسارة Huber وepsilon-insensitive للانحدار القوي.
يجب تحديد عرض المنطقة غير الحساسة عبر معامل ``epsilon``. يعتمد هذا المعامل
على مقياس المتغيرات المستهدفة.

يحدد معامل `penalty` طريقة التنظيم التي سيتم استخدامها (راجع
الوصف أعلاه في قسم التصنيف).

تدعم الفئة :class:`SGDRegressor` أيضًا متوسط SGD [#4]_ (هنا مرة أخرى، راجع
الوصف أعلاه في قسم التصنيف).

بالنسبة للانحدار بخسارة مربعة وعقوبة L2، هناك متغير آخر من SGD مع استراتيجية
المتوسط متاح بخوارزمية متوسط التدرج العشوائي (SAG)،
متاح كمُحسِّن في :class:`Ridge`.

.. _sgd_online_one_class_svm:

Online One-Class SVM
ينفذ الصنف :class:`sklearn.linear_model.SGDOneClassSVM` إصدارًا خطيًا عبر الإنترنت لخوارزمية One-Class SVM باستخدام تدرج تنازلي احتمالي. وبدمجه مع تقنيات تقريب النواة، يمكن استخدام :class:`sklearn.linear_model.SGDOneClassSVM` لتقريب حل خوارزمية One-Class SVM المعممة باستخدام النواة، المنفذة في الصنف :class:`sklearn.svm.OneClassSVM`، بتعقيد خطي في عدد العينات. لاحظ أن تعقيد خوارزمية One-Class SVM المعممة باستخدام النواة هو في أفضل الأحوال تربيعي في عدد العينات.

لذا، فإن الصنف :class:`sklearn.linear_model.SGDOneClassSVM` مناسب تمامًا لمجموعات البيانات التي تحتوي على عدد كبير من عينات التدريب (> 10000) والتي يمكن أن يكون فيها الإصدار المتغير أسرع بعدة درجات من حيث المقدار.

يدعم كل من :class:`SGDClassifier` و:class:`SGDRegressor` و:class:`SGDOneClassSVM` خوارزمية SGD ذات المتوسط الحسابي. ويمكن تمكين المتوسط الحسابي من خلال تعيين "average=True".

خوارزمية التدرج التنازلي العشوائي للبيانات المتناثرة
===========================================================

.. note::

   ينتج التنفيذ المتناثر نتائج مختلفة قليلاً عن التنفيذ الكثيف، وذلك بسبب تقلص معدل التعلم للمقاطعة. راجع :ref: `implementation_details` لمزيد من التفاصيل.

هناك دعم مدمج للبيانات المتناثرة المعطاة في أي مصفوفة بتنسيق مدعوم من قبل `scipy.sparse <https://docs.scipy.org/doc/scipy/reference/sparse.html>_`. ومع ذلك، فمن أجل تحقيق الكفاءة القصوى، استخدم تنسيق مصفوفة CSR كما هو محدد في `scipy.sparse.csr_matrix <https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html>_`.

.. rubric:: الأمثلة

- :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`

التعقيد
==========

الميزة الرئيسية لخوارزمية SGD هي كفاءتها، والتي تكون بشكل أساسي خطية في عدد الأمثلة التدريبية. إذا كانت X مصفوفة ذات حجم (n، p)، فإن التدريب يكون بتكلفة :math:`O(k n \bar p)`، حيث k هو عدد التكرارات (الحقبات) و:math:`\bar p` هو متوسط عدد السمات غير الصفرية لكل عينة.

ومع ذلك، فقد أظهرت النتائج النظرية الحديثة أن وقت التشغيل للحصول على دقة تحسين مرغوبة لا يزيد مع زيادة حجم مجموعة التدريب.

معيار التوقف
==================

يوفر الصنفان :class:`SGDClassifier` و:class:`SGDRegressor` معيارين لوقف الخوارزمية عند الوصول إلى مستوى معين من التقارب:

* مع تعيين "early_stopping=True"، يتم تقسيم بيانات الإدخال إلى مجموعة تدريب ومجموعة تحقق. بعد ذلك، يتم ملاءمة النموذج على مجموعة التدريب، ويتم استناد معيار التوقف على نتيجة التنبؤ (باستخدام طريقة "score") المحسوبة على مجموعة التحقق. يمكن تغيير حجم مجموعة التحقق باستخدام معامل "validation_fraction".

* مع تعيين "early_stopping=False"، يتم ملاءمة النموذج على جميع بيانات الإدخال، ويتم استناد معيار التوقف على الدالة الهدف المحسوبة على بيانات التدريب.

في كلتا الحالتين، يتم تقييم المعيار مرة واحدة لكل حقبة، ويتوقف الخوارزم عند عدم تحسن المعيار "n_iter_no_change" مرات متتالية. يتم تقييم التحسن باستخدام تسامح مطلق "tol"، ويتوقف الخوارزم في جميع الحالات بعد عدد أقصى من التكرارات "max_iter".

نصائح للاستخدام العملي
=====================

* خوارزمية التدرج التنازلي العشوائي حساسة لمقياس الميزة، لذا يوصى بشدة بضبط مقياس بياناتك. على سبيل المثال، قم بضبط كل سمة في متجه الإدخال X إلى المجال [0,1] أو [-1,+1]، أو قم بتوحيدها ليكون متوسطها 0 وانحرافها المعياري 1. لاحظ أنه يجب تطبيق *نفس* الضبط على متجه الاختبار للحصول على نتائج ذات معنى. يمكن القيام بذلك بسهولة باستخدام :class:`~sklearn.preprocessing.StandardScaler`::

    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    scaler.fit(X_train)  # لا تغش - قم بالضبط فقط على بيانات التدريب
    X_train = scaler.transform(X_train)
    X_test = scaler.transform(X_test)  # تطبيق نفس التحويل على بيانات الاختبار

    # أو بشكل أفضل: استخدم خط أنابيب!
    from sklearn.pipeline import make_pipeline
    est = make_pipeline(StandardScaler(), SGDClassifier())
    est.fit(X_train)
    est.predict(X_test)

  إذا كانت سماتك لها مقياس جوهري (مثل تكرارات الكلمات أو السمات المؤشرة)، فلن تكون هناك حاجة للضبط.

* أفضل طريقة لإيجاد معامل ضبط مناسب :math:`\alpha` هي باستخدام البحث التلقائي عن أفضل المعاملات، مثل :class:`~sklearn.model_selection.GridSearchCV` أو :class:`~sklearn.model_selection.RandomizedSearchCV`، عادة في النطاق ``10.0**-np.arange(1,7)``.

* وجدنا من الناحية التجريبية أن خوارزمية SGD تتقارب بعد ملاحظة حوالي 10^6 عينة تدريبية. وبالتالي، فإن التخمين المعقول لأول مرة لعدد التكرارات هو ``max_iter = np.ceil(10**6 / n)``، حيث "n" هو حجم مجموعة التدريب.

* إذا كنت تطبق خوارزمية SGD على سمات مستخرجة باستخدام التحليل العاملي التمييزي (PCA)، فقد وجدنا أنه من الحكمة غالبًا ضبط قيم السمات بواسطة ثابت 'c' بحيث يكون متوسط القيمة L2 المعيارية لبيانات التدريب يساوي واحدًا.

* وجدنا أن خوارزمية SGD ذات المتوسط الحسابي تعمل بشكل أفضل مع عدد أكبر من السمات وقيمة أعلى لـ "eta0".

.. rubric:: المراجع

* `"Efficient BackProp" <http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf>_`
  Y. LeCun, L. Bottou, G. Orr, K. Müller - في شبكات عصبية: حيل المهنة 1998.

.. _sgd_mathematical_formulation:

الصيغة الرياضية
فيما يلي نعرض التفاصيل الرياضية لإجراء "سجود" (SGD). يمكن الاطلاع على نظرة عامة جيدة مع معدلات التقارب في [6].

بالنسبة لمجموعة من الأمثلة التدريبية: (x1, y1)، ...، (xn, yn) حيث xi ∈ R^m و- y_i \in\mathcal{R} (y_i ∈ {-1, 1} للتصنيف)، فإن هدفنا هو تعلم دالة تسجيل خطية: f(x) = w^T x + b مع معلمات النموذج: w ∈ R^m ومعامل الاعتراض: b ∈ R. من أجل عمل تنبؤات للتصنيف الثنائي، ما علينا سوى النظر في علامة f(x). ولإيجاد معلمات النموذج، نقوم بتقليل خطأ التدريب المنتظم المعطى بواسطة:

E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w)

حيث L هي دالة خسارة تقيس مدى ملاءمة (أو عدم ملاءمة) النموذج، وR هي مصطلح المنتظم (المعروف أيضًا باسم العقوبة) الذي يعاقب على تعقيد النموذج؛ α > 0 هو فرط معلم غير سالب يتحكم في قوة التنظيم.

تفاصيل دالات الخسارة:

تؤدي خيارات مختلفة لـ L إلى مصنفات أو مرجحات مختلفة:

- هينج (هامش ناعم): ما يعادل تصنيف المتجهات الداعمة.
  L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i)).

- بيرسيبترون:
  L(y_i, f(x_i)) = \max(0, - y_i f(x_i)).

- هابر المعدل:
  L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))^2 إذا كانت y_i f(x_i) > -1، و L(y_i, f(x_i)) = -4 y_i f(x_i) في ما عدا ذلك.

- سجل الخسارة: ما يعادل الانحدار اللوجستي.
  L(y_i, f(x_i)) = \log(1 + \exp (-y_i f(x_i))).

- الخطأ التربيعي: الانحدار الخطي (Ridge أو Lasso اعتمادًا على R).
  L(y_i, f(x_i)) = \frac{1}{2}(y_i - f(x_i))^2.

- هابر: أقل حساسية للقيم الشاذة من طريقة المربعات الصغرى. وهو ما يعادل المربعات الصغرى عندما |y_i - f(x_i)| ≤ ϵ، و L(y_i, f(x_i)) = ϵ |y_i - f(x_i)| - \frac{1}{2} ϵ^2 في ما عدا ذلك.

- إبسيلون-غير حساس: (هامش ناعم) ما يعادل الانحدار باستخدام المتجهات الداعمة.
  L(y_i, f(x_i)) = \max(0, |y_i - f(x_i)| - ϵ).

يمكن اعتبار جميع دالات الخسارة المذكورة أعلاه حدًا أعلى لخطأ التصنيف (خسارة الصفر-واحد) كما هو موضح في الشكل أدناه.

يمكن أن تشمل الخيارات الشائعة لمصطلح التنظيم R (معلمة "العقوبة") ما يلي:

- معيار L2: R(w) := \frac{1}{2} \sum_{j=1}^{m} w_j^2 = ||w||_2^2،

- معيار L1: R(w) := \sum_{j=1}^{m} |w_j|، والذي يؤدي إلى حلول متفرقة.

- الشبكة المرنة: R(w) := \frac{\rho}{2} \sum_{j=1}^{n} w_j^2 + (1-\rho) \sum_{j=1}^{m} |w_j|، وهو مزيج محدب من L2 وL1، حيث يتم إعطاء ρ بواسطة "1 - l1_ratio".

يوضح الشكل أدناه حدود المصطلحات التنظيمية المختلفة في مساحة المعلمات ثنائية الأبعاد (m=2) عندما R(w) = 1.

سجود
-----

التدرج النسبي العشوائي هو طريقة للتحسين لمشكلات التحسين غير المقيدة. على عكس الانحدار التدرجي (Batch)، يقرب "سجود" التدرج الحقيقي لـ E(w,b) من خلال النظر في مثال تدريبي واحد في كل مرة.

تنفذ فئة SGDClassifier روتين تعلم "سجود" من الدرجة الأولى. يقوم الخوارزمية بالمرور عبر الأمثلة التدريبية، ولكل مثال، يقوم بتحديث معلمات النموذج وفقًا لقاعدة التحديث المعطاة بواسطة:

w \leftarrow w - \eta \left[\alpha \frac{\partial R(w)}{\partial w} + \frac{\partial L(w^T x_i + b, y_i)}{\partial w}\right]

حيث η هو معدل التعلم الذي يتحكم في حجم الخطوة في مساحة المعلمات. يتم تحديث معامل الاعتراض b بشكل مشابه ولكن بدون تنظيم (وبانخفاض إضافي للمصفوفات المتناثرة، كما هو مفصل في قسم "تفاصيل التنفيذ").

يمكن أن يكون معدل التعلم η ثابتًا أو متناقصًا تدريجيًا. بالنسبة للتصنيف، يكون جدول معدلات التعلم الافتراضي (learning_rate='optimal') كما يلي:

\eta^{(t)} = \frac {1}{\alpha  (t_0 + t)}

حيث t هي خطوة الوقت (هناك ما مجموعه n_samples * n_iter خطوات زمنية)، ويتم تحديد t_0 بناءً على قاعدة إبهام مقترحة بواسطة Léon Bottou بحيث تكون التحديثات الأولية المتوقعة قابلة للمقارنة مع الحجم المتوقع للأوزان (هذا بافتراض أن معيار عينات التدريب يساوي تقريبًا 1). ويمكن العثور على التعريف الدقيق في _init_t في BaseSGD.

بالنسبة للانحدار، يكون جدول معدلات التعلم الافتراضي هو التدرج العكسي (learning_rate='invscaling')، كما هو موضح أدناه:

\eta^{(t)} = \frac{eta_0}{t^{power\_t}}

حيث eta_0 وpower_t هما معلمات فائقة يختارها المستخدم من خلال eta0 وpower_t، على التوالي.

بالنسبة لمعدل تعلم ثابت، استخدم learning_rate='constant' وحدد معدل التعلم باستخدام eta0.

بالنسبة لمعدل تعلم متناقص تكيفيًا، استخدم learning_rate='adaptive' وحدد معدل التعلم الأولي باستخدام eta0. عندما يتم الوصول إلى معيار التوقف، يتم تقسيم معدل التعلم على 5، ولا يتوقف الخوارزم. يتوقف الخوارزم عندما يصبح معدل التعلم أقل من 1e-6.

يمكن الوصول إلى معلمات النموذج من خلال السمات "coef_" و"intercept_": يحتفظ "coef_" بالأوزان w ويحتفظ "intercept_" بـ b.

عند استخدام "سجود" المتوسط (مع معلمة "average")، يتم تعيين "coef_" إلى متوسط الوزن عبر جميع التحديثات:

coef_ := \frac{1}{T} \sum_{t=0}^{T-1} w^{(t)}،

حيث T هو العدد الإجمالي للتحديثات، الموجود في السمة t_.

تفاصيل التنفيذ
==============

تأثر تنفيذ "سجود" بـ "SVM Stochastic Gradient" من [1].

على غرار SvmSGD، يتم تمثيل متجه الوزن كحاصل ضرب كمية ومقدار، مما يسمح بتحديث الوزن بكفاءة في حالة التنظيم L2.

في حالة الإدخال المتناثر X، يتم تحديث معامل الاعتراض بمعدل تعلم أصغر (مضروبًا بـ 0.01) لمراعاة حقيقة أنه يتم تحديثه بشكل متكرر. يتم التقاط أمثلة التدريب بشكل تسلسلي ويتم تقليل معدل التعلم بعد كل مثال تمت ملاحظته. وقد اعتمدنا جدول معدلات التعلم من [2].

بالنسبة للتصنيف متعدد الفئات، يتم استخدام نهج "واحد مقابل الجميع".

نستخدم خوارزمية التدرج المقطوع المقترحة في [3] للتنظيم L1 (والشبكة المرنة).

تمت كتابة الكود في Cython.

المراجع
------

[1] "Stochastic Gradient Descent" <https://leon.bottou.org/projects/sgd> L. Bottou - Website, 2010.

[2] "Pegasos: Primal estimated sub-gradient solver for svm" <https://doi.org/10.1145/1273496.1273598> S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML '07.

[3] "Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty" <https://www.aclweb.org/anthology/P/P09/P09-1054.pdf> Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL'09.

[4] "Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent" <https://arxiv.org/abs/1107.2490v2> Xu, Wei (2011)

[5] "Regularization and variable selection via the elastic net" <https://doi.org/10.1111/j.1467-9868.2005.00503.x> H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B, 67 (2), 301-320.

[6] "Solving large scale linear prediction problems using stochastic gradient descent algorithms" <https://doi.org/10.1145/1015330.1015332> T. Zhang - In Proceedings of ICML '04.