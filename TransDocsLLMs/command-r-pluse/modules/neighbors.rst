الجيران الأقرب

يوفر sklearn.neighbors وظائف للتعلم غير الخاضع للإشراف والتعلم الخاضع للإشراف القائم على الجيران. الجيران الأقرب غير الخاضعين للإشراف هو الأساس للعديد من طرق التعلم الأخرى، وخاصة التعلم المانيطي والتجمع الطيفي. يأتي التعلم الخاضع للإشراف القائم على الجيران بنكهتين: التصنيف للبيانات ذات العلامات المتقطعة، والانحدار للبيانات ذات العلامات المستمرة.

المبدأ وراء طرق أقرب جار هو العثور على عدد محدد مسبقًا من عينات التدريب الأقرب في المسافة إلى النقطة الجديدة، والتنبؤ بالعلامة من هذه. يمكن أن يكون عدد العينات ثابتًا محددًا من قبل المستخدم (تعلم أقرب جار k)، أو يختلف بناءً على الكثافة المحلية للنقاط (التعلم القائم على نصف القطر).

يمكن أن تكون المسافة، بشكل عام، أي مقياس متري: المسافة الإقليدية القياسية هي الخيار الأكثر شيوعًا. تُعرف الطرق القائمة على الجيران باسم طرق التعلم غير التعميمية في التعلم الآلي، حيث إنها ببساطة "تتذكر" جميع بيانات التدريب الخاصة بها (التي يمكن تحويلها إلى بنية فهرسة سريعة مثل شجرة الكرة أو شجرة KD).

على الرغم من بساطته، إلا أن أقرب جيران كان ناجحًا في عدد كبير من مشكلات التصنيف والانحدار، بما في ذلك الأرقام المكتوبة بخط اليد ومشاهد الصور الفضائية. وباعتباره طريقة غير معلمية، فإنه غالبًا ما يكون ناجحًا في مواقف التصنيف التي يكون فيها حد القرار غير منتظم.

يمكن لفئات sklearn.neighbors التعامل مع مصفوفات NumPy أو مصفوفات scipy.sparse كمدخلات. بالنسبة للمصفوفات الكثيفة، يتم دعم عدد كبير من مقاييس المسافة الممكنة. وبالنسبة للمصفوفات المتناثرة، يتم دعم مقاييس مينكوفسكي التعسفية للبحث.

هناك العديد من روتين التعلم التي تعتمد على أقرب جيران في جوهرها. أحد الأمثلة هو تقدير كثافة النواة، والذي تمت مناقشته في قسم تقدير الكثافة.

الجيران الأقرب غير الخاضعين للإشراف

ينفذ NearestNeighbors التعلم غير الخاضع للإشراف للجيران الأقرب. إنه يعمل كواجهة موحدة لثلاثة خوارزميات مختلفة للجيران الأقرب: BallTree وKDTree، وخوارزمية القوة الغاشمة المستندة إلى الروتين في sklearn.metrics.pairwise. يتم التحكم في خيار خوارزمية البحث عن الجيران من خلال الكلمة الأساسية "algorithm"، والتي يجب أن تكون واحدة من ["auto"، "ball_tree"، "kd_tree"، "brute"]. عندما يتم تمرير القيمة الافتراضية "auto"، تحاول الخوارزمية تحديد أفضل نهج من بيانات التدريب. للاطلاع على مناقشة حول نقاط القوة والضعف لكل خيار، راجع خوارزميات أقرب جار.

تحذير

فيما يتعلق بخوارزميات أقرب جيران، إذا كان الجاران k + 1 وk لهما مسافات متطابقة ولكن علامات مختلفة، فستعتمد النتيجة على ترتيب بيانات التدريب.

إيجاد أقرب جيران

لمهمة بسيطة تتمثل في العثور على أقرب جيران بين مجموعتين من البيانات، يمكن استخدام الخوارزميات غير الخاضعة للإشراف داخل sklearn.neighbors:

من sklearn.neighbors استيراد NearestNeighbors

استيراد numpy كالنبات

X = array ([[-1، -1]، [-2، -1]، [-3، -2]، [1، 1]، [2، 1]، [3، 2]])

nbrs = NearestNeighbors (n_neighbors = 2، algorithm = 'ball_tree'). fit (X)

المسافات، المؤشرات = nbrs.kneighbors (X)

المؤشرات

المصفوفة ([[0، 1]،

[1، 0]،

[2، 1]،

[3، 4]،

[4، 3]،

[5، 4]]...)

المسافات

المصفوفة ([[0.، 0.]،

[0.، 1.]،

[0.، 1.41421356]،

[0.، 1.]،

[0.، 1.]،

[0.، 1.41421356]])

لأن مجموعة الاستعلام تتطابق مع مجموعة التدريب، فإن أقرب جار لكل نقطة هو النقطة نفسها، بمسافة تساوي صفر.

من الممكن أيضًا إنتاج رسم بياني متفرق بكفاءة يوضح الاتصالات بين النقاط المجاورة:

nbrs.kneighbors_graph (X). toarray ()

المصفوفة ([[1.، 1.، 0.، 0.، 0.، 0.]،

[1.، 1.، 0.، 0.، 0.، 0.]،

[0.، 1.، 1.، 0.، 0.، 0.]،

[0.، 0.، 0.، 1.، 1.، 0.]،

[0.، 0.، 0.، 1.، 1.، 0.]،

[0.، 0.، 0.، 0.، 1.، 1.]])

يتم تنظيم مجموعة البيانات بحيث تكون النقاط القريبة من ترتيب المؤشر قريبة في مساحة المعلمة، مما يؤدي إلى مصفوفة تقريبية ذات كتلة قطرية من أقرب الجيران K. مثل هذا الرسم البياني المتناثر مفيد في مجموعة متنوعة من الظروف التي تستخدم العلاقات المكانية بين النقاط للتعلم غير الخاضع للإشراف: على وجه الخصوص، راجع sklearn.manifold.Isomap، sklearn.manifold.LocallyLinearEmbedding، وsklearn.cluster.SpectralClustering.

فئات KDTree وBallTree

بدلاً من ذلك، يمكن للمرء استخدام فئات KDTree أو BallTree مباشرة للعثور على أقرب جيران. هذه هي الوظيفة التي يتم تغليفها بواسطة فئة NearestNeighbors المستخدمة أعلاه. تحتوي شجرة الكرة وشجرة KD على نفس الواجهة؛ سنعرض مثالًا على استخدام شجرة KD هنا:

من sklearn.neighbors استيراد KDTree

استيراد numpy كالنبات

X = array ([[-1، -1]، [-2، -1]، [-3، -2]، [1، 1]، [2، 1]، [3، 2]])

kdt = KDTree (X، leaf_size = 30، metric = 'euclidean')

kdt.query (X، k = 2، return_distance = False)

المصفوفة ([[0، 1]،

[1، 0]،

[2، 1]،

[3، 4]،

[4، 3]،

[5، 4]]...)

راجع توثيق فئات KDTree وBallTree لمزيد من المعلومات حول الخيارات المتاحة لعمليات البحث عن أقرب جيران، بما في ذلك تحديد استراتيجيات الاستعلام، ومقاييس المسافة، وما إلى ذلك. للحصول على قائمة بالمقاييس الصالحة، استخدم KDTree.valid_metrics وBallTree.valid_metrics:

من sklearn.neighbors استيراد KDTree، BallTree

KDTree.valid_metrics

['euclidean'، 'l2'، 'minkowski'، 'p'، 'manhattan'، 'cityblock'، 'l1'، 'chebyshev'، 'infinity']

BallTree.valid_metrics

['euclidean'، 'l2'، 'minkowski'، 'p'، 'manhattan'، 'cityblock'، 'l1'، 'chebyshev'، 'infinity'، 'seuclidean'، 'mahalanobis'، 'hamming'، 'canberra'، 'braycurtis'، 'jaccard'، 'dice'، 'rogerstanimoto'، 'russellrao'، 'sokalmichener'، 'sokalsneath'، 'haversine'، 'pyfunc']

التصنيف القائم على أقرب جيران

التصنيف القائم على الجيران هو نوع من التعلم القائم على الحالات أو التعلم غير التعميمي: فهو لا يحاول بناء نموذج داخلي عام، ولكنه ببساطة يقوم بتخزين حالات بيانات التدريب. يتم حساب التصنيف من تصويت الأغلبية البسيطة لأقرب جيران كل نقطة: يتم تعيين فئة الاستعلام نقطة فئة البيانات التي لديها معظم الممثلين داخل أقرب جيران النقطة.

ينفذ sklearn-learn مصنفين مختلفين للجيران الأقرب: ينفذ KNeighborsClassifier التعلم القائم على أقرب جيران k من نقطة الاستعلام، حيث k هي قيمة صحيحة يحددها المستخدم. ينفذ RadiusNeighborsClassifier التعلم القائم على عدد الجيران داخل نصف قطر محدد r من كل نقطة تدريب، حيث r هي قيمة ذات نقطة عائمة يحددها المستخدم.

تصنيف k-الجيران في KNeighborsClassifier هو أكثر التقنيات استخدامًا. الخيار الأمثل لقيمة k يعتمد بشدة على البيانات: بشكل عام، يؤدي k الأكبر إلى قمع آثار الضوضاء، ولكنه يجعل حدود التصنيف أقل تميزًا.

في الحالات التي لا يتم فيها أخذ عينات البيانات بشكل موحد، يمكن أن يكون التصنيف القائم على نصف القطر في RadiusNeighborsClassifier خيارًا أفضل. يحدد المستخدم نصف قطرًا ثابتًا r، بحيث تستخدم النقاط الموجودة في الأحياء الأقل كثافة عددًا أقل من أقرب الجيران للتصنيف. بالنسبة لمساحات المعلمات عالية الأبعاد، تصبح هذه الطريقة أقل فعالية بسبب ما يسمى "لعنة الأبعاد".

يستخدم التصنيف القائم على أقرب جيران الأساسي أوزانًا موحدة: أي أن القيمة المعينة لنقطة استعلام يتم حسابها من تصويت الأغلبية البسيطة لأقرب الجيران. في بعض الظروف، من الأفضل وزن الجيران بحيث تساهم الجيران الأقرب أكثر في التصنيف. يمكن تحقيق ذلك من خلال كلمة المرور الأوزان. القيمة الافتراضية، الأوزان = 'موحدة'، تعيين أوزان موحدة لكل جار. الأوزان = 'المسافة' تعيين الأوزان تتناسب عكسيا مع المسافة من نقطة الاستعلام. بدلاً من ذلك، يمكن توفير دالة محددة من قبل المستخدم للمسافة، والتي سيتم استخدامها لحساب الأوزان.

| التصنيف _1 | صورة :: ../auto_examples/neighbors/images/sphx_glr_plot_classification_001.png

: target: ../auto_examples/neighbors/plot_classification.html

: scale: 75

| التصنيف _1 |

| أمثلة |

* sphx_glr_auto_examples_neighbors_plot_classification.py: مثال على
  التصنيف باستخدام أقرب الجيران.

الانحدار القائم على أقرب جيران

يمكن استخدام الانحدار القائم على الجيران في الحالات التي تكون فيها علامات البيانات متغيرات مستمرة بدلاً من متغيرات منفصلة. يتم حساب العلامة المعينة لنقطة الاستعلام بناءً على متوسط علامات أقرب جيرانها.

ينفذ sklearn-learn منظمين مختلفين للجيران: ينفذ KNeighborsRegressor التعلم القائم على أقرب جيران k من نقطة الاستعلام، حيث k هي قيمة صحيحة يحددها المستخدم. ينفذ RadiusNeighborsRegressor التعلم القائم على الجيران داخل نصف قطر محدد r من نقطة الاستعلام، حيث r هي قيمة ذات نقطة عائمة يحددها المستخدم.

يستخدم الانحدار القائم على أقرب جيران الأساسي أوزانًا موحدة: أي أن كل نقطة في الحي المحلي تساهم بشكل موحد في تصنيف نقطة الاستعلام. في بعض الظروف، قد يكون من المفيد وزن النقاط بحيث تساهم النقاط القريبة أكثر في الانحدار من النقاط البعيدة. يمكن تحقيق ذلك من خلال كلمة المرور الأوزان. القيمة الافتراضية، الأوزان = 'موحدة'، تعيين أوزان متساوية لجميع النقاط. الأوزان = 'المسافة' تعيين الأوزان تتناسب عكسيا مع المسافة من نقطة الاستعلام. بدلاً من ذلك، يمكن توفير دالة محددة من قبل المستخدم للمسافة، والتي سيتم استخدامها لحساب الأوزان.

الشكل :: ../auto_examples/neighbors/images/sphx_glr_plot_regression_001.png

: target: ../auto_examples/neighbors/plot_regression.html

: align: center

: scale: 75

يتم توضيح استخدام الجيران الأقرب متعدد الإخراج للانحدار فيsphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py. في هذا المثال، تكون المدخلات X هي بكسلات النصف العلوي من الوجوه والمخرجات Y هي بكسلات النصف السفلي من تلك الوجوه.

الشكل :: ../auto_examples/miscellaneous/images/sphx_glr_plot_multioutput_face_completion_001.png

: target: ../auto_examples/miscellaneous/plot_multioutput_face_completion.html

: scale: 75

: align: center

| أمثلة |

* sphx_glr_auto_examples_neighbors_plot_regression.py: مثال على الانحدار
  باستخدام أقرب الجيران.

* sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py:
  مثال على الانحدار متعدد الإخراج باستخدام أقرب الجيران.

خوارزميات أقرب جار
حساب أقرب الجيران بسرعة هو مجال نشط للبحث في التعلم الآلي. أكثر تنفيذات البحث عن الجار بسيطة تنطوي على الحساب القسري للمسافات بين جميع أزواج النقاط في مجموعة البيانات: بالنسبة لـ N عينة في D أبعاد، هذا النهج يتوسع على النحو O [DN^2]. يمكن أن تكون عمليات البحث عن الجيران ذات القوة الغاشمة ذات الكفاءة تنافسية للغاية لعينات البيانات الصغيرة. ومع ذلك، مع زيادة عدد العينات N، يصبح النهج القسري غير عملي بسرعة. في الفصول الموجودة ضمن sklearn.neighbors، يتم تحديد عمليات البحث عن الجيران ذات القوة الغاشمة باستخدام الكلمة الأساسية "algorithm = 'brute'"، ويتم حسابها باستخدام الروتينات المتوفرة في sklearn.metrics.pairwise.

لمعالجة أوجه القصور الحسابية لنهج القوة الغاشمة، تم اختراع مجموعة متنوعة من هياكل البيانات المستندة إلى الشجرة. بشكل عام، تحاول هذه الهياكل تقليل عدد حسابات المسافة عن طريق الترميز الفعال لمعلومات المسافة الإجمالية للعينة. الفكرة الأساسية هي أنه إذا كانت النقطة A بعيدة جدًا عن النقطة B، والنقطة B قريبة جدًا من النقطة C، فحينئذٍ نعرف أن النقطتين A وC بعيدتان جدًا، *دون الحاجة إلى حساب مسافتهما بشكل صريح*. بهذه الطريقة، يمكن تقليل التكلفة الحسابية لعملية البحث عن أقرب جار إلى O [DN log(N)] أو أفضل. هذا تحسن كبير على القوة الغاشمة لـ N كبيرة.

كان أحد النهج المبكرة للاستفادة من هذه المعلومات المجمعة هو هيكل بيانات "KD tree" (اختصار لـ "K-dimensional tree")، والذي يعمم شجرة "Quad-trees" ثنائية الأبعاد وشجرة "Oct-trees" ثلاثية الأبعاد إلى أي عدد من الأبعاد. شجرة KD هي هيكل شجرة ثنائي يقسم مساحة المعلمات بشكل متكرر على طول محاور البيانات، ويقسمها إلى مناطق متعامدة متداخلة يتم فيها حفظ نقاط البيانات. إن بناء شجرة KD سريع للغاية: لأن التقسيم يتم فقط على طول محاور البيانات، فلا يلزم حساب أي مسافات D-dimensional. بمجرد البناء، يمكن تحديد أقرب جار لنقطة استعلام باستخدام حسابات المسافة O [log(N)] فقط. على الرغم من أن نهج شجرة KD سريع جدًا لعمليات البحث عن الجيران منخفضة الأبعاد (D <20)، إلا أنه يصبح غير فعال عندما يصبح D كبيرًا جدًا: هذه هي إحدى مظاهر ما يسمى "لعنة الأبعاد". في Scikit-learn، يتم تحديد عمليات بحث شجرة KD باستخدام الكلمة الأساسية "algorithm = 'kd_tree'"، ويتم حسابها باستخدام الفئة KDTree.

لمعالجة أوجه القصور في أشجار KD في أبعاد أعلى، تم تطوير هيكل بيانات "ball tree". في حين أن أشجار KD تقسم البيانات على طول المحاور الديكارتية، فإن أشجار الكرات تقسم البيانات في سلسلة من كرات التضمين. يجعل هذا بناء الشجرة أكثر تكلفة من شجرة KD، ولكنه يؤدي إلى هيكل بيانات يمكن أن يكون فعالًا جدًا في البيانات المنظمة للغاية، حتى في أبعاد عالية جدًا.

تقسم شجرة الكرة البيانات بشكل متكرر إلى عقد يتم تحديدها بواسطة مركز C ونصف القطر r، بحيث تقع كل نقطة في العقدة داخل الكرة التي يحددها r وC. يتم تقليل عدد نقاط المرشح لعملية البحث عن الجار من خلال استخدام "متباينة المثلث":

|x+y| ≤ |x| + |y|

مع هذا الإعداد، يكفي حساب مسافة واحدة بين نقطة الاختبار والمركز لتحديد حد أدنى وحد أقصى للمسافة إلى جميع النقاط داخل العقدة. نظرًا للهندسة الكروية لعقد شجرة الكرة، يمكن أن تتفوق على شجرة KD في أبعاد عالية، على الرغم من أن الأداء الفعلي يعتمد بشدة على بنية بيانات التدريب. في Scikit-learn، يتم تحديد عمليات البحث عن الجيران المستندة إلى شجرة الكرة باستخدام الكلمة الأساسية "algorithm = 'ball_tree'"، ويتم حسابها باستخدام الفئة BallTree. بدلاً من ذلك، يمكن للمستخدم العمل مباشرة مع فئة BallTree.

يعتمد الخوارزم الأمثل لمجموعة بيانات معينة على عدد من العوامل:

- عدد العينات N (أي n_samples) والأبعاد D (أي n_features).

  - يزداد وقت استعلام القوة الغاشمة مع O [DN].
  - يزداد وقت استعلام شجرة الكرة مع O [D log(N)] تقريبًا.
  - يتغير وقت استعلام شجرة KD مع D بطريقة يصعب توصيفها بدقة. بالنسبة لـ D صغيرة (أقل من 20 أو نحو ذلك)، تكون التكلفة حوالي O [D log(N)]، ويمكن أن يكون استعلام شجرة KD فعالًا جدًا. بالنسبة لـ D الأكبر، تزداد التكلفة إلى ما يقرب من O [DN]، ويمكن أن يؤدي التخطيط بسبب هيكل الشجرة إلى استعلامات أبطأ من القوة الغاشمة.

  بالنسبة لمجموعات البيانات الصغيرة (N أقل من 30 أو نحو ذلك)، يكون log(N) مشابهًا لـ N، ويمكن أن تكون خوارزميات القوة الغاشمة أكثر كفاءة من النهج القائم على الشجرة. كل من KDTree وBallTree يعالجان ذلك من خلال توفير معلمة "حجم الورقة": يتحكم هذا في عدد العينات التي يتحول عندها الاستعلام إلى القوة الغاشمة. يسمح هذا لكل من الخوارزميات بالاقتراب من كفاءة الحساب القسري لـ N الصغيرة.

- هيكل البيانات: البعد الداخلي للبيانات و/أو ندرة البيانات. يشير البعد الداخلي إلى البعد d ≤ D لمنحنى يقع عليه البيانات، والذي يمكن تضمينه خطيًا أو غير خطي في مساحة المعلمات. تشير الندرة إلى الدرجة التي تملأ بها البيانات مساحة المعلمات (يجب التمييز بين هذا المفهوم كما هو مستخدم في المصفوفات "المتناثرة". قد لا تحتوي مصفوفة البيانات على أي إدخالات صفرية، ولكن يمكن أن يكون "هيكلها" "مبعثرًا" بهذا المعنى).

  - لا يتأثر وقت استعلام القوة الغاشمة بهيكل البيانات.
  - يمكن أن يتأثر وقت استعلام شجرة الكرة وشجرة KD بشكل كبير بهيكل البيانات. بشكل عام، تؤدي البيانات الأكثر ندرة ذات البعد الداخلي الأصغر إلى أوقات استعلام أسرع. نظرًا لأن التمثيل الداخلي لشجرة KD متوافق مع محاور المعلمات، فلن يظهر عادةً تحسنًا كبيرًا مثل شجرة الكرة للبيانات ذات البنية التعسفية.

  تميل مجموعات البيانات المستخدمة في التعلم الآلي إلى أن تكون منظمة للغاية، وهي مناسبة تمامًا للاستعلامات المستندة إلى الشجرة.

- عدد الجيران k المطلوب لنقطة الاستعلام.

  - لا يتأثر وقت استعلام القوة الغاشمة بشكل كبير بقيمة k.
  - سيصبح وقت استعلام شجرة الكرة وشجرة KD أبطأ مع زيادة k. ويرجع ذلك إلى تأثيرين: أولاً، تؤدي قيمة k الأكبر إلى ضرورة البحث في جزء أكبر من مساحة المعلمات. ثانيًا، يتطلب استخدام k > 1 التصفية الداخلية للنتائج أثناء التنقل في الشجرة.

  عندما تصبح k كبيرة مقارنة بـ N، تقل قدرة الاستعلام القائم على الشجرة على تقليم الفروع. في هذه الحالة، يمكن أن تكون استعلامات القوة الغاشمة أكثر كفاءة.

- عدد نقاط الاستعلام. تتطلب كل من شجرة الكرة وشجرة KD مرحلة بناء. تصبح تكلفة هذا البناء غير مهمة عندما يتم استرداد تكلفتها على العديد من الاستعلامات. ومع ذلك، إذا كان سيتم إجراء عدد قليل فقط من الاستعلامات، فيمكن أن يشكل البناء جزءًا كبيرًا من التكلفة الإجمالية. إذا كان مطلوبًا عدد قليل جدًا من نقاط الاستعلام، فإن القوة الغاشمة أفضل من الطريقة المستندة إلى الشجرة.

حاليًا، يحدد "algorithm = 'auto'" "brute" إذا تم التحقق من أي من الشروط التالية:

- البيانات المدخلة متناثرة
- "metric = 'precomputed'"
- D > 15
- k ≥ N/2
- "effective_metric_" غير موجود في قائمة "VALID_METRICS" لكل من "kd_tree" أو "ball_tree"

وإلا، فإنه يحدد الأول من "kd_tree" و"ball_tree" الذي يحتوي على "effective_metric_" في قائمة "VALID_METRICS" الخاصة به. تستند هذه القاعدة إلى الافتراضات التالية:

- عدد نقاط الاستعلام هو على الأقل بنفس ترتيب نقاط التدريب
- "leaf_size" قريب من قيمته الافتراضية 30
- عندما يكون D > 15، يكون البعد الداخلي للبيانات مرتفعًا جدًا بشكل عام للطرق المستندة إلى الشجرة

كما هو مذكور أعلاه، بالنسبة لأحجام العينات الصغيرة، يمكن أن يكون البحث القسري أكثر كفاءة من الاستعلام القائم على الشجرة. يتم التعامل مع هذه الحقيقة في شجرة الكرة وشجرة KD من خلال التبديل داخليًا إلى عمليات بحث قسرية داخل عقد الأوراق. يمكن تحديد مستوى هذا التبديل باستخدام معلمة "leaf_size". لهذا الخيار المعلمة العديد من التأثيرات:

- وقت البناء: تؤدي قيمة "leaf_size" الأكبر إلى وقت بناء شجرة أسرع، لأنه يلزم إنشاء عدد أقل من العقد.
- وقت الاستعلام: يمكن لكل من "leaf_size" الكبير أو الصغير أن يؤدي إلى تكلفة استعلام دون المستوى الأمثل. بالنسبة لـ "leaf_size" الذي يقترب من 1، يمكن أن يؤدي التخطيط المتضمن في التنقل بين العقد إلى إبطاء أوقات الاستعلام بشكل كبير. بالنسبة لـ "leaf_size" الذي يقترب من حجم مجموعة التدريب، تصبح الاستعلامات قسرية بشكل أساسي. حل وسط جيد بين هذين هو "leaf_size = 30"، القيمة الافتراضية للمعلمة.
- الذاكرة: مع زيادة "leaf_size"، تنخفض الذاكرة اللازمة لتخزين هيكل الشجرة. هذا مهم بشكل خاص في حالة شجرة الكرة، والتي تخزن مركزًا D-dimensional لكل عقدة. تبلغ مساحة التخزين المطلوبة لـ BallTree حوالي "1 / leaf_size" من حجم مجموعة التدريب.

لا يتم الرجوع إلى "leaf_size" لاستعلامات القوة الغاشمة.

للاطلاع على قائمة بالمقاييس المتاحة، راجع وثائق فئة sklearn.metrics.DistanceMetric والمقاييس المدرجة في sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS. لاحظ أن مقياس "cosine" يستخدم sklearn.metrics.pairwise.cosine_distances.

يمكن الحصول على قائمة بالمقاييس الصالحة لأي من الخوارزميات المذكورة أعلاه باستخدام سمة "valid_metric" الخاصة بها. على سبيل المثال، يمكن توليد المقاييس الصالحة لـ KDTree على النحو التالي:

>>> from sklearn.neighbors import KDTree
>>> print(sorted(KDTree.valid_metrics))
['chebyshev', 'cityblock', 'euclidean', 'infinity', 'l1', 'l2', 'manhattan', 'minkowski', 'p']

تصنيف أقرب مركزيد
مصنف "نيارستسينتROID" هو خوارزمية بسيطة تمثل كل فئة من خلال مركز كتلة أعضائها. وفي الواقع، هذا يجعلها مشابهة لمرحلة تحديث التسمية في خوارزمية "ك-مينس". كما أنه لا يحتوي على أي معلمات للاختيار، مما يجعله مصنفًا أساسيًا جيدًا. ومع ذلك، فإنه يعاني من الفئات غير محدبة، وكذلك عندما يكون للصفوف تباينات مختلفة بشكل كبير، حيث يتم افتراض التباين المتساوي في جميع الأبعاد. راجع "التحليل التمييزي الخطي" (Linear Discriminant Analysis) و"التحليل التمييزي التربيعي" (Quadratic Discriminant Analysis) للحصول على أساليب أكثر تعقيدًا لا تفترض ذلك. إن استخدام "نيارستسينتROID" الافتراضي أمر بسيط:

يُعد مصنف "نيارستسينتROID" معلمة "شرينك_ثريشولد"، والتي تنفذ مصنف "نيرست شرينكسن تROID". وفي الواقع، يتم تقسيم قيمة كل خاصية لكل مركز إلى التباين داخل الفئة لتلك الخاصية. ثم يتم تقليل قيم الخاصية بواسطة "شرينك_ثريشولد". والأهم من ذلك، إذا عبرت قيمة خاصية معينة عن الصفر، يتم تعيينها إلى الصفر. وفي الواقع، يؤدي هذا إلى إزالة الخاصية من التأثير على التصنيف. وهذا مفيد، على سبيل المثال، لإزالة الميزات الضبابية.

في المثال أدناه، يؤدي استخدام عتبة تقلص صغيرة إلى زيادة دقة النموذج من 0.81 إلى 0.82.

يستند العديد من مصنفات scikit-learn على أقرب جيران: هناك العديد من المصنفات والمقيمات مثل "ك-نيرستسكلاسيفير" و"ك-نيرستريجريسور"، ولكن أيضًا بعض أساليب التجميع مثل "ديبي-كان" و"سبيكترالكلوسترنغ"، وبعض التضمينات المانيفولد مثل "تي-سني" و"إيزوماب".

يمكن لجميع هذه المصنفات حساب أقرب جيران داخليًا، ولكن معظمها يقبل أيضًا أقرب جيران محسوبة مسبقًا: رسم بياني متفرق، كما هو محدد بواسطة "ك-نيرستسجراف" و"راديوسنيرستسجراف". باستخدام الوضع "كونيكتيفيتي"، تعيد هذه الوظائف رسمًا بيانيًا متفرقًا ثنائيًا كما هو مطلوب، على سبيل المثال، في "سبيكترالكلوسترنغ". في حين أنه باستخدام الوضع "المسافة"، فإنها تعيد رسمًا بيانيًا متفرقًا للمسافة كما هو مطلوب، على سبيل المثال، في "ديبي-كان". لإدراج هذه الوظائف في خط أنابيب scikit-learn، يمكن للمرء أيضًا استخدام الفئات المقابلة "ك-نيرسترانسفورمر" و"راديوسنيرسترانسفورمر". فوائد واجهة برمجة التطبيقات هذه متعددة.

أولاً، يمكن إعادة استخدام الرسم البياني المحسوب مسبقًا عدة مرات، على سبيل المثال أثناء تغيير معلمة المصنف. يمكن للمستخدم القيام بذلك يدويًا، أو باستخدام خصائص التخزين المؤقت لخط أنابيب scikit-learn:

المصنف "نيجيستور نيرسترانسفورمر" مفيد بشكل خاص في خط أنابيب scikit-learn، حيث يمكنه الاستفادة من ذاكرة التخزين المؤقت لخط الأنابيب لتجنب إعادة حساب الجيران عند تغيير معلمة المصنف.

ثانيًا، يمكن أن يمنح الحساب المسبق للرسم البياني تحكمًا أدق في تقدير أقرب جيران، على سبيل المثال عن طريق تمكين المعالجة المتعددة من خلال معلمة "ن_جوبس"، والتي قد لا تكون متاحة في جميع المصنفات.

أخيرًا، يمكن أن يؤدي الحساب المسبق إلى تنفيذه بواسطة مصنفات مخصصة لاستخدام تطبيقات مختلفة، مثل أساليب أقرب جيران التقريبية، أو التطبيقات التي تستخدم أنواع بيانات خاصة. يجب تنسيق الرسم البياني للجيران المحسوب مسبقًا على النحو التالي:

- مصفوفة CSR (على الرغم من قبول COO أو CSC أو LIL).

- تخزين الأحياء القريبة صراحةً لكل عينة فيما يتعلق ببيانات التدريب. يجب أن يشمل ذلك تلك التي تكون على مسافة صفر من نقطة الاستعلام، بما في ذلك القطر الرئيسي عند حساب الأحياء القريبة بين بيانات التدريب ونفسها.

- يجب أن تخزن بيانات كل صف المسافة بترتيب تصاعدي (اختياري. سيتم فرز البيانات غير المرتبة بشكل مستقر، مما يضيف عبئًا حسابيًا).

- يجب أن تكون جميع القيم في البيانات غير سالبة.

- لا ينبغي أن تكون هناك مؤشرات مكررة في أي صف (راجع https://github.com/scipy/scipy/issues/5807).

- إذا كان الخوارزم الذي يتم تمرير المصفوفة المحسوبة مسبقًا إليه يستخدم أقرب جيران k (على عكس حي الجوار الشعاعي)، فيجب تخزين k جار على الأقل في كل صف (أو k+1، كما هو موضح في الملاحظة التالية).

ملاحظة: عندما يتم الاستعلام عن عدد محدد من الجيران (باستخدام "ك-نيرسترانسفورمر")، فإن تعريف "ن_نيرستس" غامض منذ ذلك الحين يمكن أن يشمل كل نقطة تدريب كجارتها الخاصة، أو استبعادها. لا يوجد خيار مثالي، نظرًا لأن تضمينها يؤدي إلى عدد مختلف من الجيران غير الذاتيين أثناء التدريب والاختبار، في حين أن استبعادها يؤدي إلى اختلاف بين "فيت (إكس).ترانسفورم (إكس)" و"فيت_ترانسفورم (إكس)"، وهو ما يتعارض مع واجهة برمجة تطبيقات scikit-learn. في "ك-نيرسترانسفورمر"، نستخدم التعريف الذي يتضمن كل نقطة تدريب كجارتها الخاصة في عدد "ن_نيرستس". ومع ذلك، ولأسباب تتعلق بالتوافق مع مصنفات أخرى تستخدم التعريف الآخر، سيتم حساب جار واحد إضافي عند "الوضع == 'المسافة'". لضمان أقصى قدر من التوافق مع جميع المصنفات، فإن الخيار الآمن هو دائمًا تضمين جار واحد إضافي في مصنف أقرب جيران مخصص، نظرًا لأن الجيران غير الضروريين سيتم تصفيتهم بواسطة المصنفات التالية.

أمثلة:

- "سبكس_جلر_أوتو_إكسامبلس_نيرستس_أبروكسينات_نيرست_نيرستس.باي": مثال على خط أنابيب "ك-نيرسترانسفورمر" و"تي-سني". كما يقترح مصنفي أقرب جيران مخصصين يعتمدين على حزم خارجية.

- "سبكس_جلر_أوتو_إكسامبلس_نيرستس_بلوت_كاشينج_نيرست_نيرستس.باي": مثال على خط أنابيب "ك-نيرسترانسفورمر" و"ك-نيرستسكلاسيفير" لتمكين التخزين المؤقت للرسم البياني للجيران أثناء البحث عن الشبكة في الفضاء الفرعي للمعلمات.

تحليل مكونات الحي
تحليل مكونات الجوار (NCA) هو خوارزمية لتعلم مقياس المسافة تهدف إلى تحسين دقة تصنيف أقرب جار مقارنة بالمسافة الإقليدية القياسية. ويهدف الخوارزم إلى تعظيم متغير احتمالي من دالة التصنيف k-nearest neighbors (KNN) على مجموعة التدريب. ويمكنه أيضًا تعلم تمثيل خطي منخفض الأبعاد للبيانات يمكن استخدامه لتصور البيانات والتصنيف السريع.

في الشكل التوضيحي أعلاه، نأخذ في الاعتبار بعض النقاط من مجموعة بيانات تم إنشاؤها عشوائيًا. نركز على التصنيف KNN الاحتمالي للنقطة رقم 3. يتناسب سمك الرابط بين العينة 3 ونقطة أخرى مع المسافة بينهما، ويمكن اعتباره الوزن النسبي (أو الاحتمالية) التي من شأن قاعدة تنبؤ الجار الأقرب أن تعينها لهذه النقطة. في الفراغ الأصلي، تحتوي العينة 3 على العديد من الجيران العشوائيين من فئات مختلفة، لذلك فإن الاحتمالية الصحيحة ليست عالية. ومع ذلك، في الفراغ الذي تم إسقاطه والذي تعلمه NCA، الجيران العشوائيون الوحيدون بوزن غير مهمل هم من نفس فئة العينة 3، مما يضمن تصنيفها بشكل صحيح. راجع الصيغة الرياضية لمزيد من التفاصيل.

التصنيف
--------

بالجمع بين NCA وK-Nearest Neighbors classifier، يصبح NCA جذابًا للتصنيف لأنه يمكنه التعامل مع المشكلات متعددة الفئات بشكل طبيعي دون أي زيادة في حجم النموذج، ولا يقدم أي معلمات إضافية تتطلب الضبط الدقيق من قبل المستخدم.

وقد ثبت أن التصنيف NCA يعمل بشكل جيد في الممارسة العملية لمجموعات البيانات ذات الأحجام ومستويات الصعوبة المختلفة. وعلى عكس الطرق ذات الصلة مثل التحليل التمييزي الخطي، لا يقدم NCA أي افتراضات حول توزيعات الفئات. ويمكن للتصنيف الأقرب إلى الجار أن ينتج بشكل طبيعي حدود قرار غير منتظمة للغاية.

لاستخدام هذا النموذج للتصنيف، يجب دمج مثيل "تحليل مكونات الجوار" الذي يتعلم التحول الأمثل مع مثيل "KNeighborsClassifier" الذي يؤدي التصنيف في الفراغ المسقط. فيما يلي مثال على استخدام الفئتين:

يسمح NCA بالتصنيف المتعدد دون أي زيادة في حجم النموذج، ولا يقدم أي معلمات إضافية تتطلب الضبط الدقيق من قبل المستخدم.

تخفيض الأبعاد
----------------

يمكن استخدام NCA لإجراء تقليل الأبعاد الخاضع للإشراف. يتم إسقاط بيانات الإدخال على فراغ خطي فرعي يتكون من الاتجاهات التي تقلل إلى الحد الأدنى من هدف NCA. يمكن تعيين الأبعاد المرغوبة باستخدام معلمة "n_components". على سبيل المثال، تقارن الأشكال التالية بين تقليل الأبعاد باستخدام التحليل التكويني الرئيسي (PCA)، والتحليل التمييزي الخطي (LDA)، وتحليل مكونات الجوار (NCA) على مجموعة بيانات الأرقام، وهي مجموعة بيانات بحجم 1797 عينة و64 ميزة. يتم تقسيم مجموعة البيانات إلى مجموعتين للتدريب والاختبار متساويتين في الحجم، ثم يتم توحيدها. ولأغراض التقييم، يتم حساب دقة التصنيف لأقرب 3 جيران على النقاط ثنائية الأبعاد التي تم إسقاطها والتي تم العثور عليها بواسطة كل طريقة. تنتمي كل عينة بيانات إلى واحدة من 10 فئات.

الصيغة الرياضية
----------------

هدف NCA هو تعلم مصفوفة تحويل خطي مثالية بحجم "(n_components، n_features)"، والتي تعظم مجموع جميع العينات "i" من الاحتمالية "pi" بأن "i" مصنفة بشكل صحيح، أي:

حيث "N" = "n_samples" و"pi" هي احتمال أن تكون العينة "i" مصنفة بشكل صحيح وفقًا لقاعدة الجار الأقرب العشوائي في الفراغ المدمج المُتعلم:

حيث "Ci" هي مجموعة النقاط الموجودة في نفس فئة العينة "i"، و"pij" هي الدالة الأسية للمسافات الإقليدية في الفراغ المدمج:

يمكن اعتبار NCA على أنه يتعلم مسافة ماهالانوبيس تربيعية:

حيث "M = L^T L" هي مصفوفة نصف مؤكدة إيجابية متماثلة بحجم "(n_features، n_features)".

التطبيق
--------------

يتبع هذا التطبيق ما تم شرحه في الورقة البحثية الأصلية. بالنسبة لطريقة التحسين، يستخدم التطبيق حاليًا L-BFGS-B من مكتبة Scipy مع حساب تدرج كامل في كل تكرار، لتجنب ضبط معدل التعلم وتوفير التعلم المستقر.

راجع الأمثلة أدناه ووثائق طريقة "fit" في فئة "NeighborhoodComponentsAnalysis" لمزيد من المعلومات.

التعقيد
----------

التدريب
^^^^^^^
يخزن NCA مصفوفة للمسافات بين الأزواج، ويستهلك ذاكرة بحجم "n_samples ** 2". تعتمد التعقيد الزمني على عدد التكرارات التي يقوم بها خوارزمية التحسين. ومع ذلك، يمكن تعيين الحد الأقصى لعدد التكرارات باستخدام الحجة "max_iter". بالنسبة لكل تكرار، يكون التعقيد الزمني هو "O(n_components x n_samples x min(n_samples، n_features))".

التحويل
^^^^^^^^^^
تُعيد عملية "التحويل" هنا القيمة "LX^T"، وبالتالي فإن تعقيدها الزمني يساوي "n_components * n_features * n_samples_test". لا توجد تعقيد مكاني مُضاف في العملية.

المراجع
----------

1. "تحليل مكونات الجوار" <http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf>، جيه جولدبرجر، إس رويس، جي هينتون، آر صلاح الدين، في وقائع المؤتمر المتقدم في معالجة المعلومات العصبية، المجلد 17، مايو 2005، الصفحات 513-520.
2. صفحة ويكيبيديا عن تحليل مكونات الجوار <https://en.wikipedia.org/wiki/Neighbourhood_components_analysis>