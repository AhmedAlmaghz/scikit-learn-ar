التدقيق المتقاطع: تقييم أداء أداة التقدير
===================================================

تعلم معلمات دالة التنبؤ واختبارها على نفس البيانات هو خطأ منهجي: فالنموذج الذي يكرر فقط تسميات العينات التي رآها للتو سيحصل على نتيجة مثالية ولكنه سيفشل في التنبؤ بأي شيء مفيد على البيانات التي لم يسبق رؤيتها. يُطلق على هذا الوضع اسم **الإفراط في التكييف**.

لتجنب ذلك، من الشائع عند إجراء تجربة تعلم آلي (مُشرف) الاحتفاظ بجزء من البيانات المتاحة كمجموعة **اختبار** ``X_test, y_test``.

تجدر الإشارة إلى أن كلمة "تجربة" لا يقصد بها الاستخدام الأكاديمي فقط، لأن التعلم الآلي غالبًا ما يبدأ بشكل تجريبي حتى في الإعدادات التجارية.

فيما يلي مخطط تدفق لسير عمل التحقق المتقاطع النموذجي في تدريب النموذج. يمكن تحديد أفضل المعلمات من خلال تقنيات "البحث الشبكي" <grid_search>`.

في سكيت-ليرن، يمكن إجراء تقسيم عشوائي إلى مجموعات تدريب واختبار بسرعة باستخدام دالة المساعدة `train_test_split`.

دعنا نحمل مجموعة بيانات Iris لتناسب آلة المتجهات الداعمة الخطية عليها::

  >>> استيراد نومبي كالنومبي
  >>> من sklearn.model_selection استيراد train_test_split
  >>> من sklearn استيراد مجموعات البيانات
  >>> من sklearn استيراد SVM

  >>> X، y = datasets.load_iris (return_X_y = True)
  >>> X.shape، y.shape
  ((150، 4)، (150،))

الآن يمكننا أخذ عينة من مجموعة التدريب بسرعة مع الاحتفاظ بنسبة 40% من البيانات لاختبار (تقييم) مصنفنا::

  >>> X_train، X_test، y_train، y_test = train_test_split (
  ... X، y، test_size = 0.4، random_state = 0)

  >>> X_train.shape، y_train.shape
  ((90، 4)، (90،))
  >>> X_test.shape، y_test.shape
  ((60، 4)، (60،))

  >>> clf = svm.SVC (kernel = 'linear'، C = 1).fit (X_train، y_train)
  >>> clf.score (X_test، y_test)
  0.96 ...

عند تقييم الإعدادات المختلفة ("المعلمات") لأدوات التقدير، مثل إعداد "C" الذي يجب تعيينه يدويًا لآلة المتجهات الداعمة، لا يزال هناك خطر الإفراط في التكييف *على مجموعة الاختبار* لأن المعلمات يمكن ضبطها حتى تعمل أداة التقدير بشكل مثالي. بهذه الطريقة، يمكن "تسريب" معرفة مجموعة الاختبار إلى النموذج وقد لا تعكس مقاييس التقييم بعد الآن أداء التعميم.

لحل هذه المشكلة، يمكن الاحتفاظ بجزء آخر من مجموعة البيانات كمجموعة "تحقق" بحيث يستمر التدريب على مجموعة التدريب، ثم يتم إجراء التقييم على مجموعة التحقق، وعندما تبدو التجربة ناجحة، يمكن إجراء التقييم النهائي على مجموعة الاختبار.

ومع ذلك، من خلال تقسيم البيانات المتاحة إلى ثلاث مجموعات، فإننا نخفض بشكل كبير عدد العينات التي يمكن استخدامها لتعلم النموذج، وقد تعتمد النتائج على اختيار عشوائي معين لمجموعة (التدريب، التحقق).

حل هذه المشكلة هو إجراء يسمى
`التدقيق المتقاطع <https://en.wikipedia.org/wiki/Cross-validation_(statistics)>`_
(CV اختصارًا). لا تزال هناك حاجة إلى الاحتفاظ بمجموعة اختبار منفصلة للتقييم النهائي، ولكن لم تعد هناك حاجة إلى مجموعة التحقق عند إجراء CV.

في النهج الأساسي، المسمى CV بطي *k*، يتم تقسيم مجموعة التدريب إلى *k* مجموعات أصغر (يتم وصف النهج الأخرى أدناه، ولكنها تتبع نفس المبادئ بشكل عام). يتم اتباع الإجراء التالي لكل من *k* "طي":

* يتم تدريب نموذج باستخدام :math:`k-1` من الطيات كبيانات تدريب؛
* يتم التحقق من صحة النموذج الناتج على الجزء المتبقي من البيانات (أي أنه يستخدم كمجموعة اختبار لحساب مقياس الأداء مثل الدقة).

مقياس الأداء الذي يبلغ عنه التحقق المتقاطع بطي *k* هو متوسط القيم المحسوبة في الحلقة. يمكن أن يكون هذا النهج مكلفًا من الناحية الحسابية، ولكنه لا يهدر الكثير من البيانات (كما هو الحال عند تثبيت مجموعة تحقق تعسفية)، وهي ميزة رئيسية في المشكلات مثل الاستدلال العكسي حيث يكون عدد العينات صغيرًا جدًا.

حساب المقاييس المعتمدة على التدقيق المتقاطع
أبسط طريقة لاستخدام التحقق المتقاطع هي استدعاء دالة المساعدة :func:`cross_val_score` على المحلل والمجموعة البيانات.

يوضح المثال التالي كيفية تقدير دقة آلة المتجهات الداعمة ذات النواة الخطية على مجموعة بيانات Iris من خلال تقسيم البيانات، وتناسب نموذج وحساب النتيجة 5 مرات متتالية (مع تقسيمات مختلفة في كل مرة)::

  >>> from sklearn.model_selection import cross_val_score
  >>> clf = svm.SVC(kernel='linear', C=1, random_state=42)
  >>> scores = cross_val_score(clf, X, y, cv=5)
  >>> scores
  array([0.96..., 1. , 0.96..., 0.96..., 1. ])

وبالتالي، فإن متوسط النتيجة والانحراف المعياري هما::

  >>> print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
  0.98 accuracy with a standard deviation of 0.02

بشكل افتراضي، تكون النتيجة المحسوبة في كل تكرار CV هي طريقة "النتيجة"
من المحلل. من الممكن تغيير هذا باستخدام
معلمة التسجيل::

  >>> from sklearn import metrics
  >>> scores = cross_val_score(
  ...     clf, X, y, cv=5, scoring='f1_macro')
  >>> scores
  array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])

راجع :ref:`scoring_parameter` للحصول على التفاصيل.
في حالة مجموعة بيانات Iris، تكون العينات متوازنة عبر فئات الهدف، وبالتالي فإن الدقة وF1-score متساويان تقريبًا.

عندما تكون حجة "cv" عددًا صحيحًا، يستخدم :func:`cross_val_score`
استراتيجيات :class:`KFold` أو :class:`StratifiedKFold` بشكل افتراضي، ويتم استخدام الأخير
إذا كان المحلل مشتقًا من :class:`ClassifierMixin
<sklearn.base.ClassifierMixin>`.

من الممكن أيضًا استخدام استراتيجيات التحقق من صحة متقاطعة أخرى عن طريق تمرير برنامج تقسيم التحقق من صحة متقاطع بدلاً من ذلك، على سبيل المثال::

  >>> from sklearn.model_selection import ShuffleSplit
  >>> n_samples = X.shape[0]
  >>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
  >>> cross_val_score(clf, X, y, cv=cv)
  array([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])

الخيار الآخر هو استخدام iterable ينتج تقسيمات (train، test) كمصفوفات من
الفهارس، على سبيل المثال::

  >>> def custom_cv_2folds(X):
  ...     n = X.shape[0]
  ...     i = 1
  ...     while i <= 2:
  ...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)
  ...         yield idx, idx
  ...         i += 1
  ...
  >>> custom_cv = custom_cv_2folds(X)
  >>> cross_val_score(clf, X, y, cv=custom_cv)
  array([1.        , 0.973...])

.. dropdown:: تحويل البيانات مع البيانات المحجوزة

  كما أنه من المهم اختبار أداة التنبؤ على البيانات المحجوزة من
  التدريب، فإن المعالجة المسبقة (مثل التوحيد، واختيار الميزات، وما إلى ذلك)
  وتحويلات البيانات المماثلة :ref:`data transformations <data-transforms>` يجب
  أن تتعلم أيضًا من مجموعة بيانات التدريب وتطبق على البيانات المحجوزة للتنبؤ::

    >>> from sklearn import preprocessing
    >>> X_train, X_test, y_train, y_test = train_test_split(
    ...     X, y, test_size=0.4, random_state=0)
    >>> scaler = preprocessing.StandardScaler().fit(X_train)
    >>> X_train_transformed = scaler.transform(X_train)
    >>> clf = svm.SVC(C=1).fit(X_train_transformed, y_train)
    >>> X_test_transformed = scaler.transform(X_test)
    >>> clf.score(X_test_transformed, y_test)
    0.9333...

  يجعل :class:`Pipeline <sklearn.pipeline.Pipeline>` من السهل تكوين
  المحللون، وتوفير هذا السلوك في ظل التحقق من صحة متقاطعة::

    >>> from sklearn.pipeline import make_pipeline
    >>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))
    >>> cross_val_score(clf, X, y, cv=cv)
    array([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])

  راجع :ref:`combining_estimators`.


.. _multimetric_cross_validation:

دالة cross_validate وتقييم المتري المتعدد
----------------------------------------------------------

تختلف دالة :func:`cross_validate` عن :func:`cross_val_score` في
طريقتان:

- يسمح بتحديد مقاييس متعددة للتقييم.

- يعيد قاموسًا يحتوي على أوقات التثبيت والنتيجة
  (وبشكل اختياري درجات التدريب، والمحللون المناسبين، وفصل التدريب-الاختبار
  المؤشرات) بالإضافة إلى النتيجة الاختبارية.

بالنسبة لتقييم المتري الفردي، حيث تكون معلمة التسجيل سلسلة أو
قابل للاستدعاء أو لا شيء، ستكون المفاتيح - ``['test_score', 'fit_time', 'score_time']``

وبالنسبة لتقييم المتري المتعدد، تكون قيمة الإرجاع عبارة عن قاموس بالمفاتيح التالية -
``['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']``

يتم تعيين ``return_train_score`` إلى ``False`` بشكل افتراضي لتوفير وقت الحساب.
لتقييم الدرجات على مجموعة التدريب أيضًا، يجب تعيينها على
``True``. يمكنك أيضًا الاحتفاظ بالمحلل المناسب على كل مجموعة تدريب عن طريق
تعيين ``return_estimator=True``. وبالمثل، يمكنك تعيين
`return_indices=True` للاحتفاظ بالمؤشرات التدريب والاختبار المستخدمة لتقسيم
مجموعة البيانات إلى مجموعات بيانات التدريب والاختبار لكل تقسيم cv.

يمكن تحديد المقاييس المتعددة إما على أنها قائمة أو مجموعة أو مجموعة من
أسماء المسجلين المحددين مسبقًا::

    >>> from sklearn.model_selection import cross_validate
    >>> from sklearn.metrics import recall_score
    >>> scoring = ['precision_macro', 'recall_macro']
    >>> clf = svm.SVC(kernel='linear', C=1, random_state=0)
    >>> scores = cross_validate(clf, X, y, scoring=scoring)
    >>> sorted(scores.keys())
    ['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']
    >>> scores['test_recall_macro']
    array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])

أو كقاموس يرسم اسم المسجل إلى دالة تسجيل محددة مسبقًا أو مخصصة::

    >>> from sklearn.metrics import make_scorer
    >>> scoring = {'prec_macro': 'precision_macro',
    ...            'rec_macro': make_scorer(recall_score, average='macro')}
    >>> scores = cross_validate(clf, X, y, scoring=scoring,
    ...                         cv=5, return_train_score=True)
    >>> sorted(scores.keys())
    ['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',
     'train_prec_macro', 'train_rec_macro']
    >>> scores['train_rec_macro']
    array([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])

فيما يلي مثال على استخدام ``cross_validate`` باستخدام مقياس واحد::

    >>> scores = cross_validate(clf, X, y,
    ...                         scoring='precision_macro', cv=5,
    ...                         return_estimator=True)
    >>> sorted(scores.keys())
    ['estimator', 'fit_time', 'score_time', 'test_score']


الحصول على تنبؤات عن طريق التحقق من صحة متقاطعة
-----------------------------------------

لدالة :func:`cross_val_predict` واجهة مماثلة لـ :func:`cross_val_score`، ولكنها تعيد، لكل عنصر في الإدخال،
التنبؤ الذي تم الحصول عليه لذلك العنصر عندما كان في مجموعة الاختبار. يمكن فقط
استخدام استراتيجيات التحقق من الصحة المتقاطعة التي تعين جميع العناصر إلى مجموعة اختبار مرة واحدة
(وإلا، يتم إلقاء استثناء).


.. warning:: ملاحظة حول الاستخدام غير المناسب لـ cross_val_predict

    قد تختلف نتيجة :func:`cross_val_predict` عن تلك
    التي تم الحصول عليها باستخدام :func:`cross_val_score` نظرًا لأن العناصر مجمعة بطرق مختلفة.
    تقوم دالة :func:`cross_val_score` بإجراء متوسط
    عبر طيات التحقق من الصحة المتقاطعة، في حين أن :func:`cross_val_predict` ببساطة
    إرجاع التسميات (أو الاحتمالات) من عدة نماذج متميزة. وبالتالي، فإن :func:`cross_val_predict` ليس مقياسًا مناسبًا لخطأ التعميم.


دالة :func:`cross_val_predict` مناسبة لما يلي:
  - تصور التنبؤات التي تم الحصول عليها من نماذج مختلفة.
  - مزج النماذج: عندما يتم استخدام تنبؤات أحد المحللين الخاضعين للإشراف لتدريب
    محلل آخر في أساليب التجميع.


يتم تقديم برامج تحقق صحة متقاطعة المتاحة في القسم التالي.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_model_selection_plot_roc_crossval.py`،
* :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`،
* :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`،
* :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`،
* :ref:`sphx_glr_auto_examples_model_selection_plot_cv_predict.py`،
* :ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`.

برامج تحقق صحة متقاطعة
فيما يلي أقسام تسرد المرافق لإنشاء مؤشرات يمكن استخدامها لإنشاء تقسيمات لمجموعة البيانات وفقًا لاستراتيجيات التحقق من الصلاحية المتقاطعة المختلفة.

عبور التحقق من صحة المؤشرات للبيانات المحددة بشكل مستقل ومتطابق (i.i.d)
افتراض أن بعض البيانات مستقلة ومتطابقة (i.i.d) يعني الافتراض بأن جميع العينات تنبع من نفس العملية التوليدية وأن العملية التوليدية يفترض ألا يكون لها أي سجل للعينات المولدة سابقًا.

يمكن استخدام أدوات التحقق من الصحة المتقاطعة التالية في مثل هذه الحالات.

ملاحظة:

على الرغم من أن البيانات المحددة بشكل مستقل ومتطابق (i.i.d) هي افتراض شائع في نظرية التعلم الآلي، إلا أنه نادرًا ما يحدث في الممارسة العملية. إذا كان المرء يعرف أن العينات تم إنشاؤها باستخدام عملية تعتمد على الوقت، فمن الآمن استخدام مخطط التحقق من الصحة المتقاطع على دراية بالتسلسل الزمني. وبالمثل، إذا كنا نعرف أن للعملية التوليدية هيكل مجموعة (عينات تم جمعها من مواضيع أو تجارب أو أجهزة قياس مختلفة)، فمن الآمن استخدام التحقق من الصحة المتقاطع على مستوى المجموعة.

ك-فولد
تقسم فئة KFold جميع العينات إلى k مجموعات من العينات، تسمى الطيات (إذا كانت k = n، فهذا يعادل استراتيجية "ترك واحد خارجًا")، بأحجام متساوية (إذا أمكن). يتم تعلم دالة التنبؤ باستخدام k - 1 طيات، ويتم استخدام الطية المتروكة للاختبار.

مثال على التحقق من الصحة المتقاطع 2-fold لمجموعة بيانات تحتوي على 4 عينات:

X = ["a"، "b"، "c"، "d"]

kf = KFold(n_splits=2)

for train، test in kf.split(X):

طباعة ("% s٪s"٪ (train، test))

[2 3] [0 1]

[0 1] [2 3]

فيما يلي توضيح لسلوك التحقق من الصحة المتقاطع. لاحظ أن فئة KFold لا تتأثر بالطبقات أو المجموعات.

يتم تشكيل كل طية بواسطة مصفوفتين: الأولى تتعلق بمجموعة "التدريب"، والثانية بمجموعة "الاختبار". وبالتالي، يمكن إنشاء مجموعات التدريب/الاختبار باستخدام الفهرسة النيبية:

X = np.array ([[0.، 0.]، [1.، 1.]، [-1.، -1.]، [2.، 2.]])

y = np.array ([0، 1، 0، 1])

X_train، X_test، y_train، y_test = X [train]، X [test]، y [train]، y [test]

كرر K-فولد

تكرر فئة RepeatedKFold K-Fold n مرات. يمكن استخدامه عندما يحتاج المرء إلى تشغيل KFold n مرات، وإنتاج تقسيمات مختلفة في كل تكرار.

مثال على K-Fold 2-fold مكررة مرتين:

X = np.array ([[1، 2]، [3، 4]، [1، 2]، [3، 4]])

random_state = 12883823

rkf = RepeatedKFold(n_splits=2، n_repeats=2، random_state=random_state)

for train، test in rkf.split(X):

طباعة ("% s٪s"٪ (train، test))

[2 3] [0 1]

[0 1] [2 3]

[0 2] [1 3]

[1 3] [0 2]

وبالمثل، تكرر فئة RepeatedStratifiedKFold استراتيجية K-Fold الموزونة n مرات بتعشيش مختلف في كل تكرار.

اترك واحد خارج (LOO)

LeaveOneOut (أو LOO) هو تحقق بسيط من الصحة المتقاطعة. يتم إنشاء كل مجموعة تعلم عن طريق أخذ جميع العينات باستثناء واحدة، ومجموعة الاختبار هي العينة المتروكة. وبالتالي، بالنسبة إلى n عينة، لدينا n مجموعات تدريب و n مجموعات اختبار مختلفة. لا يهدر هذا الإجراء للتحقق من الصحة المتقاطع الكثير من البيانات حيث تتم إزالة عينة واحدة فقط من مجموعة التدريب:

X = [1، 2، 3، 4]

loo = LeaveOneOut()

for train، test in loo.split(X):

طباعة ("% s٪s"٪ (train، test))

[1 2 3] [0]

[0 2 3] [1]

[0 1 3] [2]

[0 1 2] [3]

يجب أن يزن المستخدمون المحتملون لـ LOO لاختيار النماذج بعض التحذيرات المعروفة. عند مقارنته بالتحقق من الصحة المتقاطع K-fold، يقوم المرء ببناء n نماذج من n عينة بدلاً من k نماذج، حيث n > k. علاوة على ذلك، يتم تدريب كل منها على n - 1 عينة بدلاً من (k-1) n / k. في كلا الاتجاهين، طالما أن k ليست كبيرة جدًا وk <n، فإن LOO أكثر تكلفة من الناحية الحسابية من التحقق من الصحة المتقاطع K-fold.

من حيث الدقة، غالبًا ما يؤدي LOO إلى تباين عالٍ كمقدّر لخطأ الاختبار. بديهياً، نظرًا لأن n - 1 من n عينة يتم استخدامها لبناء كل نموذج، فإن النماذج المشتقة من الطيات متطابقة فعليًا مع بعضها البعض ومع النموذج المشتق من مجموعة التدريب بالكامل.

ومع ذلك، إذا كان منحنى التعلم حادًا لحجم التدريب قيد البحث، فقد يبالغ التحقق من الصحة المتقاطع 5 أو 10 في خطأ التعميم.

كقاعدة عامة، يقترح معظم المؤلفين والأدلة التجريبية أن التحقق من الصحة المتقاطع 5 أو 10 يجب أن يكون مفضلًا على LOO.

مراجع

- <http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html>؛
- تي هاستي، آر تيبشراني، جي فريدمان، عناصر التعلم الإحصائي، سبرينجر 2009
- إل بريمان، بي سبيكتور، اختيار النموذج الفرعي وتقييمه في الانحدار: حالة X-random، المراجعة الإحصائية الدولية 1992؛
- ر. كوهافي، دراسة للتحقق من الصحة المتقاطع والتمهيد لتقدير الدقة واختيار النموذج، مؤتمر IJCAI الدولي
- ر. بهارات راو، جي فونج، آر روزاليس، حول مخاطر التحقق من الصحة المتقاطع. تقييم تجريبي، SIAM 2008؛
- جي جيمس، دي ويتن، تي هاستي، آر تيبشراني، مقدمة في التعلم الإحصائي، سبرينجر 2013.

اترك P خارج (LPO)

تتشابه فئة LeavePOut كثيرًا مع فئة LeaveOneOut حيث تقوم بإنشاء جميع مجموعات التدريب/الاختبار الممكنة عن طريق إزالة p عينات من المجموعة الكاملة. بالنسبة إلى n عينة، ينتج عن هذا {n \ choose p} أزواج التدريب/الاختبار. على عكس LeaveOneOut وKFold، ستتداخل مجموعات الاختبار لـ p > 1.

مثال على Leave-2-Out على مجموعة بيانات تحتوي على 4 عينات:

X = np.ones (4)

lpo = LeavePOut(p=2)

for train، test in lpo.split(X):

طباعة ("% s٪s"٪ (train، test))

[2 3] [0 1]

[1 3] [0 2]

[1 2] [0 3]

[0 3] [1 2]

[0 2] [1 3]

[0 1] [2 3]

التقسيم العشوائي، المعروف أيضًا باسم Shuffle & Split

سينشئ مؤشر ShuffleSplit عددًا محددًا من المستخدمين من تقسيمات مجموعات التدريب/الاختبار المستقلة. يتم خلط العينات أولاً ثم تقسيمها إلى زوج من مجموعات التدريب والاختبار.

من الممكن التحكم في العشوائية لإمكانية إعادة إنتاج النتائج عن طريق البذر الصريح لمولد الأرقام العشوائية "random_state".

فيما يلي مثال على الاستخدام:

X = np.arange (10)

ss = ShuffleSplit(n_splits=5، test_size=0.25، random_state=0)

for train_index، test_index in ss.split(X):

طباعة ("% s٪s"٪ (train_index، test_index))

[9 1 6 7 3 0 5] [2 8 4]

[2 9 8 0 6 7 4] [3 5 1]

[4 5 1 0 6 9 7] [2 3 8]

[2 7 5 8 0 3 4] [6 1 9]

[4 1 0 6 8 9 3] [5 2 7]

فيما يلي توضيح لسلوك التحقق من الصحة المتقاطع. لاحظ أن فئة ShuffleSplit لا تتأثر بالطبقات أو المجموعات.

لذلك، فإن ShuffleSplit بديل جيد للتحقق من الصحة المتقاطع K-fold الذي يسمح بمزيد من التحكم الدقيق في عدد التكرارات ونسبة العينات على جانبي الانقسام التدريبي/الاختباري.

مؤشرات التحقق من الصحة المتقاطع مع الاسترات طبقة على أساس التسميات

يمكن لبعض مشكلات التصنيف أن تظهر اختلالًا كبيرًا في توزيع فئات الهدف: على سبيل المثال، قد يكون هناك عدة مرات أكثر من العينات السلبية مقارنة بالعينات الإيجابية. في مثل هذه الحالات، يوصى باستخدام الاسترات طبقة كما هو مطبق في StratifiedKFold وStratifiedShuffleSplit لضمان الحفاظ على الترددات النسبية لفئة الهدف تقريبًا في كل طية تدريب وتحقق من الصحة.

ك-فولد الموزون

StratifiedKFold هو تنوع من k-fold الذي يعيد الطيات الموزونة: تحتوي كل مجموعة على نفس النسبة المئوية تقريبًا من عينات كل فئة هدف مثل المجموعة الكاملة.

فيما يلي مثال على التحقق من الصحة المتقاطع 3-fold الموزون على مجموعة بيانات تحتوي على 50 عينة من فئتين غير متوازنتين. نحن نعرض عدد العينات في كل فئة ونقارنها مع KFold:

X، y = np.ones ((50، 1))، np.hstack (([0] * 45، [1] * 5))

skf = StratifiedKFold(n_splits=3)

for train، test in skf.split(X، y):

طباعة ('التدريب - {} | اختبار - {} '. تنسيق (

np.bincount (y [train]، np.bincount (y [test])))

التدريب - [30 3] | الاختبار - [15 2]

التدريب - [30 3] | الاختبار - [15 2]

التدريب - [30 4] | الاختبار - [15 1]

kf = KFold(n_splits=3)

for train، test in kf.split(X، y):

طباعة ('التدريب - {} | اختبار - {} '. تنسيق (

np.bincount (y [train]، np.bincount (y [test])))

التدريب - [28 5] | الاختبار - [17]

التدريب - [28 5] | الاختبار - [17]

التدريب - [34] | الاختبار - [11 5]

يمكننا أن نرى أن فئة StratifiedKFold تحافظ على نسب الفئات (تقريبًا 1/10) في كل من مجموعة التدريب ومجموعة الاختبار.

فيما يلي توضيح لسلوك التحقق من الصحة المتقاطع.

يمكن استخدام RepeatedStratifiedKFold لتكرار استراتيجية K-Fold الموزونة n مرات بتعشيش مختلف في كل تكرار.

تقسيم التبديل الموزون

StratifiedShuffleSplit هو تنوع من ShuffleSplit، والذي يعيد تقسيمات الموزون، أي أنه يقوم بإنشاء تقسيمات عن طريق الحفاظ على نفس النسبة المئوية لكل فئة هدف كما هو الحال في المجموعة الكاملة.

فيما يلي توضيح لسلوك التحقق من الصحة المتقاطع.

تقسيمات الطيات المسبقة التعريف/مجموعات التحقق

بالنسبة لبعض مجموعات البيانات، يوجد بالفعل تقسيم مسبق التعريف للبيانات إلى طية تدريب وتحقق أو إلى عدة طيات تحقق من الصحة المتقاطعة. باستخدام فئة PredefinedSplit، من الممكن استخدام هذه الطيات، على سبيل المثال، عند البحث عن أفضل المعلمات.

على سبيل المثال، عند استخدام مجموعة تحقق، قم بتعيين "test_fold" إلى 0 لجميع العينات التي تعد جزءًا من مجموعة التحقق، وإلى -1 لجميع العينات الأخرى.

مؤشرات التحقق من الصحة المتقاطعة للبيانات المجمعة
يتم كسر افتراض i.i.d إذا أدت عملية التوليد الأساسية إلى مجموعات من العينات المعتمدة.

إن تجميع هذه البيانات خاص بالنطاق. على سبيل المثال، عندما تكون هناك بيانات طبية تم جمعها من مرضى متعددين، مع أخذ عينات متعددة من كل مريض. ومن المرجح أن تعتمد هذه البيانات على المجموعة الفردية. في مثالنا، سيكون معرف المريض لكل عينة هو معرف المجموعة الخاص بها.

في هذه الحالة، نريد أن نعرف ما إذا كان النموذج المدرب على مجموعة معينة من المجموعات يتم تعميمه جيدًا على المجموعات غير المرئية. لقياس ذلك، نحتاج إلى التأكد من أن جميع العينات في طية التحقق من الصحة تأتي من مجموعات غير ممثلة على الإطلاق في طية التدريب المقترنة.

يمكن استخدام برامج تقسيم التحقق من صحة التقاطع التالية للقيام بذلك.

يتم تحديد معرف المجموعة للنماذج عبر معلمة "المجموعات".

Group k-fold
^^^^^^^^^^^^

:class:`GroupKFold` هو تباين من k-fold والذي يضمن أن المجموعة نفسها غير ممثلة في كل من مجموعات الاختبار والتدريب. على سبيل المثال، إذا تم الحصول على البيانات من مواضيع مختلفة مع عدة عينات لكل موضوع، وإذا كان النموذج مرنًا بدرجة كافية للتعلم من الميزات المحددة للشخص، فقد يفشل في التعميم على مواضيع جديدة. :class:`GroupKFold` يجعل من الممكن اكتشاف هذا النوع من حالات الإفراط في التكيّف.

تخيل أن لديك ثلاثة مواضيع، لكل منها رقم مرتبط من 1 إلى 3::

  >>> from sklearn.model_selection import GroupKFold

  >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]
  >>> y = ["a", "b", "b", "b", "c", "c", "c", "d", "d", "d"]
  >>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]

  >>> gkf = GroupKFold(n_splits=3)
  >>> for train, test in gkf.split(X, y, groups=groups):
  ...     print("%s %s" % (train, test))
  [0 1 2 3 4 5] [6 7 8 9]
  [0 1 2 6 7 8 9] [3 4 5]
  [3 4 5 6 7 8 9] [0 1 2]

يوجد كل موضوع في طية اختبار مختلفة، ولا يوجد أبدًا نفس الموضوع في الاختبار والتدريب. لاحظ أن الطيات لا تحتوي على نفس الحجم تمامًا بسبب عدم التوازن في البيانات. إذا كان يجب موازنة نسب الفئات عبر الطيات، فإن :class:`StratifiedGroupKFold` هو خيار أفضل.

فيما يلي توضيح لسلوك التحقق من الصحة عبر الدورات.

.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_007.png
   :target: ../auto_examples/model_selection/plot_cv_indices.html
   :align: center
   :scale: 75%

على غرار :class:`KFold`، ستشكل مجموعات الاختبار من :class:`GroupKFold` تقسيمًا كاملاً لجميع البيانات. على عكس :class:`KFold`، فإن :class:`GroupKFold` ليس عشوائيًا على الإطلاق، في حين أن :class:`KFold` يكون عشوائيًا عندما ``shuffle=True``.

StratifiedGroupKFold
^^^^^^^^^^^^^^^^^^^^

:class:`StratifiedGroupKFold` هو مخطط للتحقق من الصحة عبر الدورات يجمع بين :class:`StratifiedKFold` و:class:`GroupKFold`. الفكرة هي محاولة الحفاظ على توزيع الفئات في كل تقسيم مع الحفاظ على كل مجموعة داخل تقسيم واحد. قد يكون ذلك مفيدًا عندما يكون لديك مجموعة بيانات غير متوازنة بحيث قد يؤدي استخدام :class:`GroupKFold` فقط إلى إنتاج تقسيمات منحرفة.

مثال::

  >>> from sklearn.model_selection import StratifiedGroupKFold
  >>> X = list(range(18))
  >>> y = [1] * 6 + [0] * 12
  >>> groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]
  >>> sgkf = StratifiedGroupKFold(n_splits=3)
  >>> for train, test in sgkf.split(X, y, groups=groups):
  ...     print("%s %s" % (train, test))
  [ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]
  [ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]
  [ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]

.. dropdown:: ملاحظات التنفيذ

  - مع التنفيذ الحالي، لا يكون الخلط الكامل ممكنًا في معظم السيناريوهات. عندما يكون shuffle=True، يحدث ما يلي:

    1. يتم خلط جميع المجموعات.
    2. يتم فرز المجموعات حسب الانحراف المعياري للفئات باستخدام الفرز المستقر.
    3. يتم فحص المجموعات المرتبة وتعيينها إلى الطيات.

    وهذا يعني أنه يتم خلط المجموعات التي لها نفس الانحراف المعياري لتوزيع الفئات فقط، والذي قد يكون مفيدًا عندما تحتوي كل مجموعة على فئة واحدة فقط.
  - يقوم الخوارزمية بشكل جشع بتعيين كل مجموعة إلى واحدة من مجموعات الاختبار n_splits، واختيار مجموعة الاختبار التي تقلل من التباين في توزيع الفئات عبر مجموعات الاختبار. تتقدم عملية تعيين المجموعة من المجموعات ذات أعلى تباين إلى أدنى تباين في تردد الفئات، أي أن المجموعات الكبيرة التي تركز على فئة واحدة أو عدد قليل من الفئات يتم تعيينها أولاً.
  - هذا التقسيم دون المستوى الأمثل بمعنى أنه قد ينتج تقسيمات غير متوازنة حتى إذا كان من الممكن تحقيق الاستراتيجية المثالية. إذا كان لديك توزيع قريب نسبيًا للفئات في كل مجموعة، فإن استخدام :class:`GroupKFold` أفضل.

فيما يلي توضيح لسلوك التحقق من الصحة عبر الدورات للمجموعات غير المتساوية:

.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_005.png
   :target: ../auto_examples/model_selection/plot_cv_indices.html
   :align: center
   :scale: 75%

Leave One Group Out
^^^^^^^^^^^^^^^^^^^

:class:`LeaveOneGroupOut` هو مخطط للتحقق من الصحة عبر الدورات حيث يحتفظ كل تقسيم بالعينات التي تنتمي إلى مجموعة محددة واحدة. يتم توفير معلومات المجموعة عبر مصفوفة ترميز مجموعة كل عينة.

وبالتالي، يتم تشكيل كل مجموعة تدريب من جميع العينات باستثناء تلك المتعلقة بمجموعة محددة واحدة. هذا مشابه لـ :class:`LeavePGroupsOut` مع `n_groups=1` ونفس :class:`GroupKFold` مع `n_splits` يساوي عدد التسميات الفريدة التي تم تمريرها إلى معلمة "المجموعات".

على سبيل المثال، في حالات التجارب المتعددة، يمكن استخدام :class:`LeaveOneGroupOut` لإنشاء تحقق من الصحة عبر الدورات يعتمد على التجارب المختلفة: نقوم بإنشاء مجموعة تدريب باستخدام عينات من جميع التجارب باستثناء واحدة::

  >>> from sklearn.model_selection import LeaveOneGroupOut

  >>> X = [1, 5, 10, 50, 60, 70, 80]
  >>> y = [0, 1, 1, 2, 2, 2, 2]
  >>> groups = [1, 1, 2, 2, 3, 3, 3]
  >>> logo = LeaveOneGroupOut()
  >>> for train, test in logo.split(X, y, groups=groups):
  ...     print("%s %s" % (train, test))
  [2 3 4 5 6] [0 1]
  [0 1 4 5 6] [2 3]
  [0 1 2 3] [4 5 6]

هناك تطبيق شائع آخر وهو استخدام معلومات الوقت: على سبيل المثال، يمكن أن تكون المجموعات هي سنة جمع العينات، وبالتالي السماح بالتحقق من الصحة عبر التقسيمات القائمة على الوقت.

Leave P Groups Out
^^^^^^^^^^^^^^^^^^

:class:`LeavePGroupsOut` مشابه لـ :class:`LeaveOneGroupOut`، ولكنه يزيل العينات المتعلقة بـ :math:`P` groups لكل مجموعة تدريب/اختبار. يتم ترك جميع المجموعات الممكنة من :math:`P` groups، مما يعني أن مجموعات الاختبار ستتداخل لـ :math:`P>1`.

مثال على ترك 2 مجموعة::

  >>> from sklearn.model_selection import LeavePGroupsOut

  >>> X = np.arange(6)
  >>> y = [1, 1, 1, 2, 2, 2]
  >>> groups = [1, 1, 2, 2, 3, 3]
  >>> lpgo = LeavePGroupsOut(n_groups=2)
  >>> for train, test in lpgo.split(X, y, groups=groups):
  ...     print("%s %s" % (train, test))
  [4 5] [0 1 2 3]
  [2 3] [0 1 4 5]
  [0 1] [2 3 4 5]

Group Shuffle Split
^^^^^^^^^^^^^^^^^^^

يتصرف مولد :class:`GroupShuffleSplit` كمزيج من :class:`ShuffleSplit` و:class:`LeavePGroupsOut`، وينشئ تسلسلًا من التقسيمات العشوائية التي يتم فيها الاحتفاظ بمجموعة فرعية من المجموعات لكل تقسيم. يتم إجراء كل تقسيم تدريب/اختبار بشكل مستقل، مما يعني أنه لا توجد علاقة مضمونة بين مجموعات الاختبار المتتالية.

فيما يلي مثال على الاستخدام::

  >>> from sklearn.model_selection import GroupShuffleSplit

  >>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]
  >>> y = ["a", "b", "b", "b", "c", "c", "c", "a"]
  >>> groups = [1, 1, 2, 2, 3, 3, 4, 4]
  >>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)
  >>> for train, test in gss.split(X, y, groups=groups):
  ...     print("%s %s" % (train, test))
  ...
  [0 1 2 3] [4 5 6 7]
  [2 3 6 7] [0 1 4 5]
  [2 3 4 5] [0 1 6 7]
  [4 5 6 7] [0 1 2 3]

فيما يلي توضيح لسلوك التحقق من الصحة عبر الدورات.

.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_011.png
   :target: ../auto_examples/model_selection/plot_cv_indices.html
   :align: center
   :scale: 75%

تعد هذه الفئة مفيدة عندما يكون سلوك :class:`LeavePGroupsOut` مرغوبًا، ولكن يكون عدد المجموعات كبيرًا بدرجة كافية بحيث يكون إنشاء جميع التقسيمات الممكنة باستخدام :math:`P` groups مكلفًا للغاية. في مثل هذا السيناريو، يوفر :class:`GroupShuffleSplit` عينة عشوائية (مع الاستبدال) من تقسيمات التدريب/الاختبار التي تم إنشاؤها بواسطة :class:`LeavePGroupsOut`.

استخدام برامج التحقق من الصحة عبر الدورات لتقسيم التدريب والاختبار
--------------------------------------------------------

قد تكون وظائف التحقق من الصحة عبر الدورات للمجموعات المذكورة أعلاه مفيدة أيضًا لتقسيم مجموعة من البيانات إلى مجموعات فرعية للتدريب والاختبار. لاحظ أن وظيفة الملاءمة :func:`train_test_split` عبارة عن غلاف حول :func:`ShuffleSplit` وبالتالي فهي تسمح فقط بالتقسيم الاستراتيجي (باستخدام تسميات الفئات) ولا يمكنها التعامل مع المجموعات.

لإجراء تقسيم التدريب والاختبار، استخدم المؤشرات الخاصة بمجموعات التدريب والاختبار التي تم إنتاجها بواسطة مخرجات المولد من طريقة `split()` لمقسم التحقق من الصحة عبر الدورات. على سبيل المثال::

  >>> import numpy as np
  >>> from sklearn.model_selection import GroupShuffleSplit

  >>> X = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001])
  >>> y = np.array(["a", "b", "b", "b", "c", "c", "c", "a"])
  >>> groups = np.array([1, 1, 2, 2, 3, 3, 4, 4])
  >>> train_indx, test_indx = next(
  ...     GroupShuffleSplit(random_state=7).split(X, y, groups)
  ... )
  >>> X_train, X_test, y_train, y_test = \
  ...     X[train_indx], X[test_indx], y[train_indx], y[test_indx]
  >>> X_train.shape, X_test.shape
  ((6,), (2,))
  >>> np.unique(groups[train_indx]), np.unique(groups[test_indx])
  (array([1, 2, 4]), array([3]))

Cross validation of time series data
------------------------------------

تتميز بيانات السلاسل الزمنية بالارتباط بين الملاحظات القريبة في الوقت (*autocorrelation*). ومع ذلك، تفترض تقنيات التحقق من الصحة عبر الدورات الكلاسيكية مثل :class:`KFold` و:class:`ShuffleSplit` أن العينات مستقلة ومتطابقة في التوزيع، وقد يؤدي ذلك إلى ارتباط غير معقول بين مثيلات التدريب والاختبار (مما يؤدي إلى تقديرات سيئة لخطأ التعميم) في بيانات السلاسل الزمنية. لذلك، من المهم جدًا تقييم نموذجنا لبيانات السلاسل الزمنية على الملاحظات "المستقبلية" الأقل تشابهاً مع تلك المستخدمة لتدريب النموذج. ولتحقيق ذلك، يقدم :class:`TimeSeriesSplit` حلاً واحدًا.

Time Series Split
^^^^^^^^^^^^^^^^^

:class:`TimeSeriesSplit` هو تباين من *k-fold* والذي يعيد أول :math:`k` folds كمجموعة تدريب والطي :math:`(k+1)` th كمجموعة اختبار. لاحظ أنه على عكس طرق التحقق من الصحة عبر الدورات القياسية، فإن مجموعات التدريب المتعاقبة هي مجموعات فرعية من تلك التي تأتي قبلها.

يضيف هذا الفصل أيضًا جميع البيانات الفائضة إلى أول قسم تدريب، والذي يتم استخدامه دائمًا لتدريب النموذج.

يمكن استخدام هذه الفئة للتحقق من صحة بيانات السلاسل الزمنية التي تتم ملاحظتها بفترات زمنية ثابتة.

مثال على التحقق من الصحة عبر الدورات للسلاسل الزمنية المكونة من 3 تقسيمات على مجموعة بيانات بها 6 عينات::

  >>> from sklearn.model_selection import TimeSeriesSplit

  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
  >>> y = np.array([1, 2, 3, 4, 5, 6])
  >>> tscv = TimeSeriesSplit(n_splits=3)
  >>> print(tscv)
  TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)
  >>> for train, test in tscv.split(X):
  ...     print("%s %s" % (train, test))
  [0 1 2] [3]
  [0 1 2 3] [4]
  [0 1 2 3 4] [5]

فيما يلي توضيح لسلوك التحقق من الصحة عبر الدورات.

.. figure:: ../auto_examples/model_selection/images/sphx_glr_plot_cv_indices_013.png
   :target: ../auto_examples/model_selection/plot_cv_indices.html
   :align: center
   :scale: 7
إذا لم يكن ترتيب البيانات عشوائيًا (على سبيل المثال، إذا كانت العينات ذات التسمية الطبقية نفسها متجاورة)، فقد يكون من الضروري خلطها أولاً للحصول على نتيجة صحيحة للتحقق من الصحة. ومع ذلك، قد يكون العكس صحيحًا إذا لم تكن العينات موزعة بشكل مستقل ومتطابق. على سبيل المثال، إذا كانت العينات المقابلة لمقالات الأخبار، ومرتبة حسب وقت النشر، فإن خلط البيانات سيؤدي على الأرجح إلى نموذج مفرط في التكيف ودرجة تحقق من الصحة منتفخة: فسيتم اختباره على عينات مشابهة بشكل مصطنع (قريبة في الوقت) لعينات التدريب.

تتوفر بعض برامج تشغيل التحقق من الصحة المتقاطع، مثل :class:`KFold`، على خيار مدمج لخلط مؤشرات البيانات قبل تقسيمها. لاحظ ما يلي:

* تستهلك هذه الطريقة ذاكرة أقل من خلط البيانات مباشرة.
* بشكل افتراضي، لا يحدث أي خلط، بما في ذلك للطي (الاستراتيفي) K للتحقق من الصحة المتقاطع الذي يتم إجراؤه عن طريق تحديد "cv=some_integer" إلى :func:`cross_val_score`، والبحث الشبكي، وما إلى ذلك. ضع في اعتبارك أن :func:`train_test_split` لا يزال يعيد تقسيمًا عشوائيًا.
* يُعيّن معيار "random_state" افتراضيًا إلى "None"، مما يعني أن الخلط سيكون مختلفًا في كل مرة يتم فيها تشغيل "KFold(..., shuffle=True)". ومع ذلك، سيستخدم "GridSearchCV" نفس الخلط لكل مجموعة من المعلمات التي يتم التحقق من صحتها عن طريق استدعاء واحد لأسلوب "fit".
* للحصول على نتائج متطابقة لكل تقسيم، قم بتعيين "random_state" إلى رقم صحيح.

للاطلاع على مزيد من التفاصيل حول كيفية التحكم في العشوائية لبرامج التقسيم المتقاطع وتجنب المشكلات الشائعة، راجع :ref:`randomness`.

التحقق من صحة المتقاطع واختيار النموذج
====================================

يمكن أيضًا استخدام برامج تشغيل التحقق من الصحة المتقاطع لأداء اختيار النموذج مباشرةً باستخدام البحث الشبكي لأفضل المعلمات المفرطة للنموذج. هذا هو موضوع القسم التالي: :ref:`grid_search`.

.. _permutation_test_score:

نتيجة اختبار التبديل
======================

:func:`~sklearn.model_selection.permutation_test_score` يقدم طريقة أخرى
لتقييم أداء المصنفات. فهو يوفر قيمة p قائمة على التبديل، والتي تمثل مدى احتمال الحصول على أداء المُصنِّف المُلاحظ عن طريق الصدفة. الفرضية الصفرية في هذا الاختبار هي أن المُصنِّف لا يستفيد من أي تبعية إحصائية بين الميزات والعلامات للتنبؤ الصحيح بالبيانات المستبعدة. :func:`~sklearn.model_selection.permutation_test_score` ينشئ توزيعًا صفريًا عن طريق حساب `n_permutations` تبديلات مختلفة للبيانات. في كل تبديل، يتم خلط العلامات بشكل عشوائي، مما يؤدي إلى إزالة أي تبعية بين الميزات والعلامات. نتيجة p-value
هي نسبة التبديلات التي يكون فيها متوسط درجات التحقق من الصحة المتقاطع التي يحصل عليها النموذج أفضل من نتيجة التحقق من الصحة المتقاطع التي يحصل عليها النموذج باستخدام البيانات الأصلية. للحصول على نتائج موثوقة، يجب أن يكون "n_permutations" عادةً أكبر من 100 و"cv" بين 3-10 طيات.

تقدم قيمة p منخفضة دليلاً على أن مجموعة البيانات تحتوي على تبعية حقيقية
بين الميزات والعلامات وأن المُصنِّف كان قادرًا على الاستفادة من ذلك
للحصول على نتائج جيدة. قد تكون قيمة p المرتفعة بسبب عدم وجود تبعية
بين الميزات والعلامات (لا يوجد اختلاف في قيم الميزات بين الفئات) أو
لأن المُصنِّف لم يتمكن من استخدام التبعية في البيانات. في الحالة
الأخيرة، فإن استخدام مُصنِّف أكثر ملاءمة قادر على الاستفادة من
الهيكل الموجود في البيانات، سيؤدي إلى قيمة p أقل.

يوفر التحقق من الصحة المتقاطع معلومات حول مدى تعميم المُصنِّف جيدًا،
وبالتحديد نطاق أخطاء المُصنِّف المتوقعة. ومع ذلك، فإن المُصنِّف الذي
تم تدريبه على مجموعة بيانات عالية الأبعاد بدون هيكل قد يؤدي
مع ذلك إلى أداء أفضل من المتوقع في التحقق من الصحة المتقاطع، فقط عن
طريق الصدفة. يمكن أن يحدث هذا عادةً مع مجموعات البيانات الصغيرة التي
تحتوي على أقل من بضع مئات من العينات. :func:`~sklearn.model_selection.permutation_test_score`
يوفر معلومات حول ما إذا كان المُصنِّف قد وجد هيكل فئة حقيقيًا ويمكن أن
يساعد في تقييم أداء المُصنِّف.

من المهم ملاحظة أن هذا الاختبار قد أثبت أنه ينتج قيم p منخفضة حتى إذا
كان هناك هيكل ضعيف فقط في البيانات لأن مجموعات البيانات المقابلة
التي تم تبديلها لا تحتوي على أي هيكل على الإطلاق. لذلك، لا يمكن
لهذا الاختبار سوى إظهار عندما يتفوق النموذج بشكل موثوق على التخمين
العشوائي.

أخيرًا، يتم حساب :func:`~sklearn.model_selection.permutation_test_score`
باستخدام القوة الغاشمة، ويتم تثبيت "n_permutations" داخليًا
"*(n_permutations + 1) * n_cv" من النماذج. لذلك، فهو ممكن فقط مع
مجموعات البيانات الصغيرة التي يكون فيها تثبيت نموذج فردي سريعًا
جداً.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_model_selection_plot_permutation_tests_for_classification.py`

.. dropdown:: المراجع

  * أويالا وغارريجا. `اختبارات التبديل لدراسة أداء المُصنِّف
    <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_.
    J. Mach. Learn. Res. 2010.