.. _naive_bayes:

===========
خوارزمية ناييف بايز
===========

.. currentmodule:: sklearn.naive_bayes


خوارزمية ناييف بايز هي مجموعة من خوارزميات التعلم الخاضع للإشراف
التي تستند إلى تطبيق نظرية بايز مع افتراض "ساذج" للاستقلال الشرطي
بين كل زوج من الميزات بالنظر إلى
قيمة المتغير التابع. وتنص نظرية بايز على العلاقة التالية، بالنظر إلى المتغير التابع :math:`y`
ومجموعة الميزات التابعة :math:`x_1` حتى :math:`x_n`:

.. math::

   P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots, x_n \mid y)}
                                    {P(x_1, \dots, x_n)}

باستخدام افتراض الاستقلال الشرطي الساذج بأن

.. math::

   P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y),

بالنسبة لكل :math:`i`، يتم تبسيط هذه العلاقة إلى

.. math::

   P(y \mid x_1, \dots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)}
                                    {P(x_1, \dots, x_n)}

نظرًا لأن :math:`P(x_1, \dots, x_n)` ثابت بالنظر إلى المدخلات،
يمكننا استخدام قاعدة التصنيف التالية:

.. math::

   P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)

   \Downarrow

   \hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y),

ويمكننا استخدام تقدير الحد الأقصى الاحتمالي a posteriori (MAP) لتقدير
:math:`P(y)` و :math:`P(x_i \mid y)`؛
الأول هو التردد النسبي للصنف :math:`y`
في مجموعة التدريب.

تختلف مصنفات ناييف بايز المختلفة بشكل رئيسي في الافتراضات التي تقدمها فيما يتعلق بتوزيع :math:`P(x_i \mid y)`.

على الرغم من افتراضاتها المبسطة بشكل مفرط، إلا أن مصنفات ناييف بايز
أثبتت فعاليتها في العديد من المواقف الواقعية، مثل تصنيف الوثائق وتصفية الرسائل غير المرغوب فيها. وهي تتطلب كمية صغيرة
من بيانات التدريب لتقدير المعلمات اللازمة. (للأسباب النظرية التي تجعل ناييف بايز يعمل بشكل جيد، وأنواع البيانات التي يعمل عليها، راجع
المراجع أدناه.)

يمكن أن تكون خوارزميات التعلم والمصنف ناييف بايز سريعة للغاية مقارنة بالخوارزميات الأكثر
تعقيدًا.
وفصل توزيعات الميزات الشرطية للصنف يعني أنه يمكن تقدير كل توزيع بشكل مستقل كتوزيع أحادي البعد.
وهذا بدوره يساعد في التخفيف من المشكلات الناجمة عن لعنة الأبعاد.

على الجانب الآخر، على الرغم من أن ناييف بايز معروف بأنه مصنف جيد،
إلا أنه معروف أيضًا بأنه مقدر سيئ، لذا لا ينبغي أخذ احتمالات الإخراج من
``predict_proba`` على محمل الجد.

.. dropdown:: المراجع

   * H. Zhang (2004). `The optimality of Naive Bayes.
     <https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf>`_
     Proc. FLAIRS.

.. _gaussian_naive_bayes:

ناييف بايز الغاوسي
--------------------

:class:`GaussianNB` ينفذ خوارزمية ناييف بايز الغاوسي للتصنيف. يفترض أن احتمال الميزات يتبع التوزيع الغاوسي:

.. math::

   P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)

ويتم تقدير المعلمات :math:`\sigma_y` و :math:`\mu_y`
باستخدام الاحتمال الأقصى.

   >>> from sklearn.datasets import load_iris
   >>> from sklearn.model_selection import train_test_split
   >>> from sklearn.naive_bayes import GaussianNB
   >>> X, y = load_iris(return_X_y=True)
   >>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)
   >>> gnb = GaussianNB()
   >>> y_pred = gnb.fit(X_train, y_train).predict(X_test)
   >>> print("عدد النقاط المصنفة بشكل خاطئ من أصل %d نقطة : %d"
   ...       % (X_test.shape[0], (y_test != y_pred).sum()))
   عدد النقاط المصنفة بشكل خاطئ من أصل 75 نقطة : 4

.. _multinomial_naive_bayes:

ناييف بايز متعدد الحدود
-----------------------

:class:`MultinomialNB` ينفذ خوارزمية ناييف بايز للتوزيع متعدد الحدود، وهو أحد المتغيرين الكلاسيكيين لناييف بايز المستخدمين في
تصنيف النصوص (حيث يتم تمثيل البيانات عادةً كميات ناقلات الكلمات، على الرغم من أن متجهات tf-idf معروفة أيضًا بأنها تعمل بشكل جيد في الممارسة العملية).
يتم معلمجة التوزيع بواسطة المتجهات
:math:`\theta_y = (\theta_{y1},\ldots,\theta_{yn})`
لكل صنف :math:`y`، حيث :math:`n` هو عدد الميزات
(في تصنيف النصوص، حجم المفردات)
و :math:`\theta_{yi}` هو احتمال :math:`P(x_i \mid y)`
لظهور الميزة :math:`i` في عينة تنتمي إلى الصنف :math:`y`.

ويتم تقدير المعلمة :math:`\theta_y` باستخدام نسخة ممهدة
من الاحتمال الأقصى، أي حساب التردد النسبي:

.. math::

    \hat{\theta}_{yi} = \frac{ N_{yi} + \alpha}{N_y + \alpha n}

حيث :math:`N_{yi} = \sum_{x \in T} x_i` هو
عدد مرات ظهور الميزة :math:`i` في عينة من الصنف :math:`y`
في مجموعة التدريب :math:`T`،
و :math:`N_{y} = \sum_{i=1}^{n} N_{yi}` هو العدد الإجمالي
لجميع الميزات للصنف :math:`y`.

وتحسب معلمات التمهيد :math:`\alpha \ge 0`
الميزات غير الموجودة في عينات التعلم وتمنع الاحتمالات الصفرية
في الحسابات اللاحقة.
ويعرف تعيين :math:`\alpha = 1` باسم التمهيد Laplace،
بينما يعرف :math:`\alpha < 1` باسم تمهيد Lidstone.

.. _complement_naive_bayes:

ناييف بايز التكميلي
----------------------

:class:`ComplementNB` ينفذ خوارزمية ناييف بايز التكميلي (CNB).
CNB هو تكييف لخوارزمية ناييف بايز متعددة الحدود القياسية
المناسبة بشكل خاص لمجموعات البيانات غير المتوازنة. وعلى وجه التحديد، يستخدم CNB
الإحصاءات من متممة كل صنف لحساب أوزان النموذج. ويبين مخترعو CNB تجريبياً أن تقديرات المعلمات لـ CNB
أكثر استقرارًا من تلك الخاصة بـ MNB. علاوة على ذلك، يفوق CNB بانتظام MNB (غالبًا بهامش كبير)
في مهام تصنيف النصوص.

.. dropdown:: حساب الأوزان

   تتمثل إجراءات حساب الأوزان فيما يلي:

   .. math::

      \hat{\theta}_{ci} = \frac{\alpha_i + \sum_{j:y_j \neq c} d_{ij}}
                              {\alpha + \sum_{j:y_j \neq c} \sum_{k} d_{kj}}

      w_{ci} = \log \hat{\theta}_{ci}

      w_{ci} = \frac{w_{ci}}{\sum_{j} |w_{cj}|}

   حيث يتم إجراء الجمع لجميع الوثائق :math:`j` غير الموجودة في الصنف :math:`c`،
   :math:`d_{ij}` هو إما عدد مرات ظهور المصطلح :math:`i` في الوثيقة
   :math:`j`، أو قيمة tf-idf، :math:`\alpha_i` هو معلمة التمهيد مثل تلك الموجودة في
   MNB، و :math:`\alpha = \sum_{i} \alpha_i`. ويعالج التطبيع الثاني
   ميل الوثائق الأطول إلى الهيمنة على تقديرات المعلمات في MNB. وقاعدة التصنيف هي:

   .. math::

      \hat{c} = \arg\min_c \sum_{i} t_i w_{ci}

   أي، يتم تعيين وثيقة إلى الصنف الذي يمثل أسوأ تطابق متمم.

.. dropdown:: المراجع

   * Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).
     `Tackling the poor assumptions of naive bayes text classifiers.
     <https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf>`_
     In ICML (Vol. 3, pp. 616-623).


.. _bernoulli_naive_bayes:

ناييف بايز برنولي
---------------------

:class:`BernoulliNB` ينفذ خوارزميات التدريب والتصنيف ناييف بايز
للبيانات الموزعة وفقًا لتوزيعات برنولي متعددة المتغيرات؛ أي، قد يكون هناك ميزات متعددة ولكن يفترض
أن تكون كل منها متغيرًا ذا قيمة ثنائية (برنولي، بولياني).
لذلك، تتطلب هذه الفئة تمثيل العينات كمصفوفات ذات قيم ثنائية؛ إذا تم تمرير أي نوع آخر من البيانات،
فقد تقوم مثيلة :class:`BernoulliNB` بتمثيل بياناتها ثنائياً (اعتمادًا على معلمة ``binarize``).

وتستند قاعدة القرار لناييف بايز برنولي إلى

.. math::

    P(x_i \mid y) = P(x_i = 1 \mid y) x_i + (1 - P(x_i = 1 \mid y)) (1 - x_i)

والتي تختلف عن قاعدة ناييف بايز متعدد الحدود
في أنها تعاقب بشكل صريح على عدم حدوث ميزة :math:`i`
التي تعد مؤشراً للصنف :math:`y`،
حيث تتجاهل المتغيرات متعددة الحدود ببساطة الميزة غير الحادثة.

في حالة تصنيف النصوص، قد يتم استخدام متجهات حدوث الكلمات (بدلاً من متجهات عدد الكلمات) لتدريب واستخدام هذا المصنف. :class:`BernoulliNB`
قد يؤدي أداءً أفضل على بعض مجموعات البيانات، خاصة تلك التي تحتوي على وثائق أقصر.
ومن المستحسن تقييم كلا النموذجين، إذا سمح الوقت بذلك.

.. dropdown:: المراجع

   * C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to
     Information Retrieval. Cambridge University Press, pp. 234-265.

   * A. McCallum and K. Nigam (1998).
     `A comparison of event models for Naive Bayes text classification.
     <https://citeseerx.ist.psu.edu/doc_view/pid/04ce064505b1635583fa0d9cc07cac7e9ea993cc>`_
     Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.

   * V. Metsis, I. Androutsopoulos and G. Paliouras (2006).
     `Spam filtering with Naive Bayes -- Which Naive Bayes?
     <https://citeseerx.ist.psu.edu/doc_view/pid/8bd0934b366b539ec95e683ae39f8abb29ccc757>`_
     3rd Conf. on Email and Anti-Spam (CEAS).


.. _categorical_naive_bayes:

ناييف بايز الفئوي
-----------------------

:class:`CategoricalNB` ينفذ خوارزمية ناييف بايز الفئوية
للبيانات الموزعة فئوياً. يفترض أن لكل ميزة،
التي يتم وصفها بالمؤشر :math:`i`، توزيعها الفئوي الخاص بها.

بالنسبة لكل ميزة :math:`i` في مجموعة التدريب :math:`X`،
يقدر :class:`CategoricalNB` توزيعًا فئوياً لكل ميزة :math:`i`
من X مشروطًا بالصنف y. ومجموعة فهارس العينات معرفة على النحو
:math:`J = \{ 1, \dots, m \}`, حيث :math:`m` هو عدد العينات.

.. dropdown:: حساب الاحتمال

   يتم تقدير احتمال الفئة :math:`t` في الميزة :math:`i` بالنظر إلى الصنف
   :math:`c` على النحو التالي:

   .. math::

      P(x_i = t \mid y = c \: ;\, \alpha) = \frac{ N_{tic} + \alpha}{N_{c} +
                                             \alpha n_i},

   حيث :math:`N_{tic} = |\{j \in J \mid x_{ij} = t, y_j = c\}|` هو عدد
   مرات ظهور الفئة :math:`t` في العينات :math:`x_{i}`، التي تنتمي
   إلى الصنف :math:`c`، :math:`N_{c} = |\{ j \in J\mid y_j = c\}|` هو عدد
   العينات ذات الصنف c، :math:`\alpha` هو معلمة التمهيد و
   :math:`n_i` هو عدد الفئات المتاحة للميزة :math:`i`.

يفترض :class:`CategoricalNB` أن مصفوفة العينات :math:`X` مشفرة (بمساعدة
:class:`~sklearn.preprocessing.OrdinalEncoder` على سبيل المثال) بحيث
يتم تمثيل جميع الفئات لكل ميزة :math:`i` بأرقام
:math:`0, ..., n_i - 1` حيث :math:`n_i` هو عدد الفئات المتاحة
للميزة :math:`i`.

التدريب خارج الذاكرة لنماذج ناييف بايز
-------------------------------------

يمكن استخدام نماذج ناييف بايز لمعالجة مشكلات التصنيف واسعة النطاق
التي قد لا تناسب مجموعة التدريب بالكامل في الذاكرة. لمعالجة هذه الحالة،
يكشف :class:`MultinomialNB` و :class:`BernoulliNB` و :class:`GaussianNB`
عن طريقة ``partial_fit`` التي يمكن استخدامها
بشكل متزايد كما هو الحال مع المصنفات الأخرى كما هو موضح في
:ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`. وتدعم جميع مصنفات ناييف بايز وزن العينات.

على عكس طريقة ``fit``، فإن أول استدعاء لـ ``partial_fit`` يجب أن يتم
تمرير قائمة بجميع ملصقات الفئات المتوقعة.

للاطلاع على نظرة عامة على الاستراتيجيات المتاحة في sklearn، راجع أيضًا
توثيق :ref:`التعلم خارج الذاكرة <scaling_strategies>`.

.. note::

   تتضمن طريقة استدعاء ``partial_fit`` لنماذج ناييف بايز بعض
   النفقات العامة الحسابية. يُنصح باستخدام أحجام قطع البيانات التي تكون كبيرة قدر الإمكان، أي كما تسمح بها الذاكرة RAM المتاحة.