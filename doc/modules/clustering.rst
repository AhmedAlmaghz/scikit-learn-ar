  =======
    التجميع
    =======
    
    يمكن إجراء `التجميع`_ للبيانات غير المسمى باستخدام وحدة :mod:`sklearn.cluster`.
    
    تأتي كل خوارزمية تجميع في نوعين: صنف، ينفذ طريقة ``fit`` لتعلم التجمعات على بيانات التدريب، ودالة، تعطى بيانات التدريب، ترجع مصفوفة من التسميات الصحيحة المقابلة للتجمعات المختلفة. بالنسبة للصنف، يمكن العثور على التسميات على بيانات التدريب في سمة ``labels_``.
    
    .. currentmodule:: sklearn.cluster
    
    .. topic:: بيانات الإدخال
    
        من المهم ملاحظة أن الخوارزميات المنفذة في هذه الوحدة يمكن أن تأخذ أنواعًا مختلفة من المصفوفات كمدخلات. تقبل جميع الطرق مصفوفات بيانات قياسية من الشكل ``(n_samples, n_features)``.
        يمكن الحصول على هذه المصفوفات من الأصناف في وحدة :mod:`sklearn.feature_extraction`. بالنسبة لـ :class:`AffinityPropagation`، و :class:`SpectralClustering`
        و:class:`DBSCAN` يمكن أيضًا إدخال مصفوفات تشابه من الشكل
        ``(n_samples, n_samples)``. يمكن الحصول على هذه المصفوفات من الدوال في وحدة :mod:`sklearn.metrics.pairwise`.
    
    نظرة عامة على أساليب التجميع
    ============================
    
    .. figure:: ../auto_examples/cluster/images/sphx_glr_plot_cluster_comparison_001.png
       :target: ../auto_examples/cluster/plot_cluster_comparison.html
       :align: center
       :scale: 50
    
       مقارنة خوارزميات التجميع في scikit-learn
    
    
    .. list-table::
       :header-rows: 1
       :widths: 14 15 19 25 20
    
       * - اسم الطريقة
         - المعلمات
         - قابلية التوسع
         - حالة الاستخدام
         - الهندسة (المقياس المستخدم)
    
       * - :ref:`K-Means <k_means>`
         - عدد التجمعات
         - ``n_samples`` كبير جدًا، ``n_clusters`` متوسط مع :ref:`كود MiniBatch <mini_batch_kmeans>`
         - الغرض العام، حجم التجمع المتساوي، الهندسة المسطحة، ليس الكثير من التجمعات، استقرائي
         - المسافات بين النقاط
    
       * - :ref:`انتشار التقارب <affinity_propagation>`
         - التخميد، تفضيل العينة
         - غير قابل للتطوير مع ``n_samples``
         - العديد من التجمعات، حجم التجمع غير المتساوي، الهندسة غير المسطحة، استقرائي
         - مسافة الرسم البياني (على سبيل المثال الرسم البياني لأقرب الجيران)
    
       * - :ref:`تحريك الوسط <mean_shift>`
         - عرض النطاق
         - غير قابل للتطوير مع ``n_samples``
         - العديد من التجمعات، حجم التجمع غير المتساوي، الهندسة غير المسطحة، استقرائي
         - المسافات بين النقاط
    
       * - :ref:`التجميع الطيفي <spectral_clustering>`
         - عدد التجمعات
         - ``n_samples`` متوسط، ``n_clusters`` صغير
         - عدد قليل من التجمعات، حجم التجمع المتساوي، الهندسة غير المسطحة، تحويلي
         - مسافة الرسم البياني (على سبيل المثال الرسم البياني لأقرب الجيران)
    
       * - :ref:`تجميع Ward الهرمي <hierarchical_clustering>`
         - عدد التجمعات أو عتبة المسافة
         - ``n_samples`` كبير و ``n_clusters`` كبير
         - العديد من التجمعات، ربما قيود الاتصال، تحويلي
         - المسافات بين النقاط
    
       * - :ref:`تجميع تكتلي <hierarchical_clustering>`
         - عدد التجمعات أو عتبة المسافة، نوع الربط، المسافة
         - ``n_samples`` كبير و ``n_clusters`` كبير
         - العديد من التجمعات، ربما قيود الاتصال، مسافات غير إقليدية، تحويلي
         - أي مسافة زوجية
    
       * - :ref:`DBSCAN <dbscan>`
         - حجم الحي
         - ``n_samples`` كبير جدًا، ``n_clusters`` متوسط
         - الهندسة غير المسطحة، أحجام التجمع غير المتساوية، إزالة الشذوذ، تحويلي
         - المسافات بين أقرب النقاط
    
       * - :ref:`HDBSCAN <hdbscan>`
         - الحد الأدنى لعضوية التجمع، الحد الأدنى لجيران النقطة
         - ``n_samples`` كبير، ``n_clusters`` متوسط
         - هندسة غير مسطحة، أحجام تجمعات غير متساوية، إزالة الشذوذ، تحويلي، هرمي، كثافة تجمعات متغيرة
         - المسافات بين أقرب النقاط
    
       * - :ref:`OPTICS <optics>`
         - الحد الأدنى لعضوية التجمع
         - ``n_samples`` كبير جدًا، ``n_clusters`` كبير
         - هندسة غير مسطحة، أحجام تجمعات غير متساوية، كثافة تجمعات متغيرة، إزالة الشذوذ، تحويلي
         - المسافات بين النقاط
    
       * - :ref:`خليط غاوسي <mixture>`
         - العديد
         - غير قابل للتطوير
         - الهندسة المسطحة، جيدة لتقدير الكثافة، استقرائي
         - مسافات Mahalanobis إلى المراكز
    
       * - :ref:`BIRCH <birch>`
         - عامل التفرع، العتبة، المجمع العالمي الاختياري.
         - ``n_clusters`` كبير و ``n_samples`` كبير
         - مجموعة بيانات كبيرة، إزالة الشذوذ، تقليل البيانات، استقرائي
         - المسافة الإقليدية بين النقاط
    
       * - :ref:`التقسيم الثنائي K-Means <bisect_k_means>`
         - عدد التجمعات
         - ``n_samples`` كبير جدًا، ``n_clusters`` متوسط
         - الغرض العام، حجم التجمع المتساوي، الهندسة المسطحة، لا تجمعات فارغة، استقرائي، هرمي
         - المسافات بين النقاط
    
    يكون تجميع الهندسة غير المسطحة مفيدًا عندما يكون للتجمعات شكل معين، أي متنوعة غير مسطحة، والمسافة الإقليدية القياسية ليست المقياس الصحيح. تنشأ هذه الحالة في الصفين العلويين من الشكل أعلاه.
    
    تم وصف نماذج الخليط الغاوسي، المفيدة للتجميع، في :ref:`فصل آخر من التوثيق <mixture>` مخصص لنماذج الخليط. يمكن اعتبار KMeans حالة خاصة من نموذج الخليط الغاوسي مع تساوي التباين لكل مكون.
    
    طرق التجميع :term:`التحويلي <transductive>` (على النقيض من طرق التجميع :term:`الاستقرائي <inductive>`) غير مصممة للتطبيق على بيانات جديدة وغير مرئية.
    
    .. _k_means:
    
    K-means
    
    

.. _التجميع: https://en.wikipedia.org/wiki/Cluster_analysis
.. _كود MiniBatch: #
.. _انتشار التقارب: #
.. _تحريك الوسط: #
.. _التجميع الطيفي: #
.. _تجميع Ward الهرمي: #
.. _تجميع تكتلي: #
.. _DBSCAN: #
.. _HDBSCAN: #
.. _OPTICS: #
.. _خليط غاوسي: #
.. _BIRCH: #
.. _التقسيم الثنائي K-Means: #
.. _الاستقرائي: #
.. _التحويلي: #























































































































































  خوارزمية :class:`KMeans` تعمل على تجميع البيانات عن طريق محاولة فصل العينات في n مجموعة من التباين المتساوي، مع تقليل معيار يعرف باسم "القصور الذاتي" أو مجموع مربعات داخل المجموعة (انظر أدناه). تتطلب هذه الخوارزمية تحديد عدد المجموعات. إنها قابلة للتطوير بشكل جيد لأعداد كبيرة من العينات وتم استخدامها عبر مجموعة كبيرة من مجالات التطبيق في العديد من الحقول المختلفة.

    تقوم خوارزمية k-means بتقسيم مجموعة من :math:`N` عينات :math:`X` إلى :math:`K` مجموعات منفصلة :math:`C`، كل منها موصوف بالمتوسط :math:`\mu_j` للعينات في المجموعة. تسمى المتوسطات عادة "وسطاء المجموعة"؛ لاحظ أنها ليست، بشكل عام، نقاط من :math:`X`، على الرغم من أنها تعيش في نفس الفضاء.

    تهدف خوارزمية K-means إلى اختيار وسطاء المجموعة التي تقلل من "القصور الذاتي"، أو معيار مجموع مربعات داخل المجموعة:

    .. math:: \sum_{i=0}^{n}\min_{\mu_j \in C}(||x_i - \mu_j||^2)

    يمكن التعرف على القصور الذاتي كمقياس لمدى تماسك المجموعات داخليًا. يعاني من العديد من السلبيات:

    - يفترض القصور الذاتي أن المجموعات تكون محدبة ومتساوية الخواص، وهو ما ليس صحيحًا دائمًا. إنه يستجيب بشكل سيئ للمجموعات الممدودة، أو الحيزات ذات الأشكال غير المنتظمة.

    - القصور الذاتي ليس مقياسًا موحدًا: نحن نعلم أن القيم المنخفضة أفضل وأن الصفر مثالي. ولكن في المساحات عالية الأبعاد للغاية، تميل المسافات الإقليدية إلى التضخم (هذه حالة من ما يسمى "لعنة الأبعاد"). يمكن أن يخفف تشغيل خوارزمية تقليل الأبعاد مثل :ref:`PCA` قبل تجميع k-means من هذه المشكلة ويسرع العمليات الحسابية.

    .. image:: ../auto_examples/cluster/images/sphx_glr_plot_kmeans_assumptions_002.png
       :target: ../auto_examples/cluster/plot_kmeans_assumptions.html
       :align: center
       :scale: 50

    للحصول على وصف أكثر تفصيلاً للقضايا الموضحة أعلاه وكيفية معالجتها، راجع الأمثلة :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py` و :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py`.

    غالبًا ما يشار إلى K-means باسم خوارزمية لويد. ببساطة، للخوارزمية ثلاثة خطوات. الخطوة الأولى تختار الوسطاء الأولية، مع الطريقة الأكثر أساسية هي اختيار :math:`k` عينات من مجموعة البيانات :math:`X`. بعد التهيئة، يتكون K-means من حلقتين بين الخطوتين الأخريين. تقوم الخطوة الأولى بتعيين كل عينة إلى أقرب وسيط لها. تنشئ الخطوة الثانية وسطاء جددًا عن طريق أخذ القيمة الوسطى لجميع العينات المخصصة لكل وسيط سابق. يتم حساب الفرق بين الوسطاء القدامى والجدد وتكرر الخوارزمية هاتين الخطوتين الأخيرتين حتى تكون هذه القيمة أقل من عتبة معينة. بمعنى آخر، تكرر حتى لا تتحرك الوسطاء بشكل كبير.

    .. image:: ../auto_examples/cluster/images/sphx_glr_plot_kmeans_digits_001.png
       :target: ../auto_examples/cluster/plot_kmeans_digits.html
       :align: right
       :scale: 35

    K-means يعادل خوارزمية التوقع والتعظيم مع مصفوفة التباين التربيعي الصغيرة والمتساوية.

    يمكن أيضًا فهم الخوارزمية من خلال مفهوم `الرسومات المبيانية لفورونوي <https://en.wikipedia.org/wiki/Voronoi_diagram>`_. أولاً، يتم حساب مخطط فورونوي للنقاط باستخدام الوسطاء الحالية. يصبح كل جزء في مخطط فورونوي مجموعة منفصلة. ثانيًا، يتم تحديث الوسطاء إلى متوسط كل جزء. ثم تكرر الخوارزمية هذا حتى يتم استيفاء معيار الإيقاف. عادةً، تتوقف الخوارزمية عندما يكون الانخفاض النسبي في دالة الهدف بين التكرارات أقل من قيمة التحمل المحددة. هذا ليس هو الحال في هذا التنفيذ: يتوقف التكرار عندما تتحرك الوسطاء أقل من التسامح.

    بالنظر إلى ما يكفي من الوقت، سوف تتقارب K-means دائمًا، ولكن هذا قد يكون إلى الحد الأدنى المحلي. هذا يعتمد بشكل كبير على تهيئة الوسطاء. نتيجة لذلك، غالبًا ما يتم إجراء الحساب عدة مرات، مع تهيئة مختلفة للوسطاء. إحدى الطرق للمساعدة في معالجة هذه المشكلة هي مخطط التهيئة k-means ++، والذي تم تنفيذه في scikit-learn (استخدم المعلمة ``init='k-means++'``). هذا يهيئ الوسطاء ليكونوا (بشكل عام) بعيدين عن بعضهم البعض، مما يؤدي إلى نتائج أفضل على الأرجح من التهيئة العشوائية، كما هو موضح في المرجع. للحصول على مثال تفصيلي لمقارنة مخططات التهيئة المختلفة، راجع :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`.

    يمكن أيضًا استدعاء K-means ++ بشكل مستقل لتحديد البذور لخوارزميات التجميع الأخرى، انظر :func:`sklearn.cluster.kmeans_plusplus` للحصول على التفاصيل ومثال الاستخدام.

    تدعم الخوارزمية أوزان العينات، والتي يمكن إعطاؤها بواسطة معلمة ``sample_weight``. هذا يسمح بتعيين وزن أكبر لبعض العينات عند حساب وسطاء المجموعة وقيم القصور الذاتي. على سبيل المثال، إن تعيين وزن 2 لعينة ما يعادل إضافة نسخة مكررة من تلك العينة إلى مجموعة البيانات :math:`X`.

    يمكن استخدام K-means للتحويل القياسي للمتجهات. يتم تحقيق ذلك باستخدام طريقة ``transform`` لنموذج مدرب من :class:`KMeans`. للحصول على مثال عن إجراء التحويل القياسي للمتجهات على صورة، راجع :ref:`sphx_glr_auto_examples_cluster_plot_color_quantization.py`.

    .. rubric:: أمثلة

    * :ref:`sphx_glr_auto_examples_cluster_plot_cluster_iris.py`: مثال لاستخدام :class:`KMeans` باستخدام مجموعة بيانات القزحية

    * :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`: تجميع المستندات باستخدام :class:`KMeans` و :class:`MiniBatchKMeans` بناءً على البيانات النادرة

    التوازي منخفض المستوى
    
    (ملاحظة: لم يتم ترجمة الرموز الخاصة والرموز والمعادلات الرياضية والروابط والتاجات والشفرة البرمجية كما هو مطلوب في السؤال)

:class:`KMeans` يستفيد من التوازي القائم على OpenMP من خلال Cython. تتم معالجة أجزاء صغيرة من البيانات (256 عينة) بالتوازي، مما يؤدي أيضًا إلى تقليل استخدام الذاكرة. لمزيد من التفاصيل حول كيفية التحكم في عدد الخيوط، يرجى الرجوع إلى ملاحظاتنا حول :ref:`parallelism`.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`: توضيح الحالات التي يعمل فيها k-means بشكل بديهي وحالات عدم عمله
* :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`: تجميع الأرقام المكتوبة بخط اليد

.. dropdown:: المراجع

  * `"k-means++: The advantages of careful seeding"
    <http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf>`_
    آرثر، ديفيد، وسيرجي فاسيليفيسكي،
    *Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete
    algorithms*, Society for Industrial and Applied Mathematics (2007)


.. _mini_batch_kmeans:

Mini Batch K-Means
------------------

إن :class:`MiniBatchKMeans` هو نوع من خوارزمية :class:`KMeans` الذي يستخدم mini-batches لتقليل وقت الحساب، بينما لا يزال يحاول تحسين دالة الهدف نفسها. Mini-batches هي مجموعات فرعية من بيانات الإدخال، يتم اختيارها عشوائيًا في كل تكرار للتدريب. هذه المجموعات المصغرة تقلل بشدة من كمية الحسابات المطلوبة للتقارب إلى حل محلي. على النقيض من الخوارزميات الأخرى التي تقلل من وقت تقارب k-means، فإن mini-batch k-means تنتج نتائج تكون عمومًا أسوأ قليلاً فقط من الخوارزمية القياسية.

تتكرر الخوارزمية بين خطوتين رئيسيتين، على غرار k-means التقليدي. في الخطوة الأولى، يتم سحب :math:`b` عينات عشوائيًا من مجموعة البيانات، لتشكيل mini-batch. ثم يتم تعيين هذه العينات إلى المركز الأقرب. في الخطوة الثانية، يتم تحديث المراكز. على النقيض من k-means، يتم ذلك على أساس كل عينة على حدة. لكل عينة في mini-batch، يتم تحديث المركز المعين من خلال أخذ المتوسط المتدفق للعينة وجميع العينات السابقة المعينة لهذا المركز. هذا له تأثير تقليل معدل التغيير للمركز بمرور الوقت. يتم تنفيذ هذه الخطوات حتى التقارب أو الوصول إلى عدد محدد مسبقًا من التكرارات.

:class:`MiniBatchKMeans` يتقارب بشكل أسرع من :class:`KMeans`، ولكن يتم تقليل جودة النتائج. في الواقع، يمكن أن يكون هذا الفرق في الجودة صغيرًا جدًا، كما هو موضح في المثال والمراجع المذكورة.

.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_mini_batch_kmeans_001.png
   :target: ../auto_examples/cluster/plot_mini_batch_kmeans.html
   :align: center
   :scale: 100


.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`: مقارنة بين
  :class:`KMeans` و :class:`MiniBatchKMeans`

* :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`: تجميع المستندات
  باستخدام :class:`KMeans` و :class:`MiniBatchKMeans` بناءً على بيانات متفرقة

* :ref:`sphx_glr_auto_examples_cluster_plot_dict_face_patches.py`

.. dropdown:: المراجع

  * `"Web Scale K-Means clustering"
    <https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf>`_
    D. Sculley، *Proceedings of the 19th international conference on World
    wide web* (2010)

.. _affinity_propagation:

انتشار التقارب

    

:class:`AffinityPropagation` تنشئ مجموعات عن طريق إرسال رسائل بين أزواج من العينات حتى التقارب. ثم يتم وصف مجموعة البيانات باستخدام عدد صغير من النماذج، والتي يتم تحديدها على أنها الأكثر تمثيلاً للعينات الأخرى. تمثل الرسائل المرسلة بين الأزواج مدى ملاءمة عينة واحدة لتكون نموذجًا للآخر، والتي يتم تحديثها استجابةً للقيم من أزواج أخرى. يحدث هذا التحديث بشكل متكرر حتى التقارب، وعند هذه النقطة يتم اختيار النماذج النهائية، وبالتالي يتم إعطاء التجميع النهائي.

.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_affinity_propagation_001.png
   :target: ../auto_examples/cluster/plot_affinity_propagation.html
   :align: center
   :scale: 50


يمكن أن تكون Affinity Propagation مثيرة للاهتمام لأنها تختار عدد المجموعات بناءً على البيانات المقدمة. لهذا الغرض، هناك معلمان مهمان هما *التفضيل*، الذي يتحكم في عدد النماذج المستخدمة، و *عامل التخميد* الذي يثبط رسائل المسؤولية والتوفر لتجنب التذبذبات العددية عند تحديث هذه الرسائل.

العيب الرئيسي لـ Affinity Propagation هو تعقيدها. يحتوي الخوارزمية على تعقيد زمني من الترتيب :math:`O(N^2 T)`, حيث :math:`N` هو عدد العينات و :math:`T` هو عدد التكرارات حتى التقارب. علاوة على ذلك، فإن تعقيد الذاكرة هو من الترتيب :math:`O(N^2)` إذا تم استخدام مصفوفة تشابه كثيفة، ولكن يمكن تقليلها إذا تم استخدام مصفوفة تشابه متفرقة. هذا يجعل Affinity Propagation الأنسب لمجموعات البيانات الصغيرة إلى المتوسطة الحجم.

.. dropdown:: وصف الخوارزمية

  تنتمي الرسائل المرسلة بين النقاط إلى إحدى فئتين. الأولى هي المسؤولية :math:`r(i, k)`, وهي الدليل المتراكم على أن العينة :math:`k` يجب أن تكون النموذج للعينة :math:`i`. والثاني هو التوفر :math:`a(i, k)` والذي يمثل الدليل المتراكم على أن العينة :math:`i` يجب أن تختار العينة :math:`k` لتكون نموذجها، وتأخذ في الاعتبار القيم لجميع العينات الأخرى التي يجب أن تكون :math:`k` نموذجًا لها. بهذه الطريقة، يتم اختيار النماذج بواسطة العينات إذا كانت (1) متشابهة بدرجة كافية للعديد من العينات و (2) اختارتها العديد من العينات لتمثيل نفسها.

  بشكل أكثر رسمية، يتم إعطاء مسؤولية العينة :math:`k` لتكون نموذجًا للعينة :math:`i` بواسطة:

  .. math::

      r(i, k) \leftarrow s(i, k) - max [ a(i, k') + s(i, k') \forall k' \neq k ]

  حيث :math:`s(i, k)` هو التشابه بين العينات :math:`i` و :math:`k`. يتم إعطاء مدى توفر العينة :math:`k` لتكون النموذج للعينة :math:`i` بواسطة:

  .. math::

      a(i, k) \leftarrow min [0, r(k, k) + \sum_{i'~s.t.~i' \notin \{i, k\}}{r(i', k)}]

  في البداية، يتم تعيين جميع القيم لـ :math:`r` و :math:`a` على الصفر، ويتم حساب كل منها حتى التقارب. كما تمت مناقشته أعلاه، من أجل تجنب التذبذبات العددية عند تحديث الرسائل، يتم تقديم عامل التخميد :math:`\lambda` إلى عملية التكرار:

  .. math:: r_{t+1}(i, k) = \lambda\cdot r_{t}(i, k) + (1-\lambda)\cdot r_{t+1}(i, k)
  .. math:: a_{t+1}(i, k) = \lambda\cdot a_{t}(i, k) + (1-\lambda)\cdot a_{t+1}(i, k)

  حيث :math:`t` يشير إلى أوقات التكرار.


.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`: Affinity Propagation على مجموعات بيانات تركيبية ثنائية الأبعاد مع 3 فئات
* :ref:`sphx_glr_auto_examples_applications_plot_stock_market.py` Affinity Propagation على السلاسل الزمنية المالية لإيجاد مجموعات الشركات


.. _mean_shift:

Shift المتوسط
    

(ملاحظة: لم تتم ترجمة أسماء الدوال والصفوف والرموز الرياضية والعناوين الفرعية والأمثلة لتجنب أي لبس في الفهم والتطبيق)

تقوم تجميع MeanShift باكتشاف "blobs" في كثافة سلسة من العينات. وهو عبارة عن خوارزمية تستند إلى المركز، والتي تعمل عن طريق تحديث المرشحين للمراكز بحيث تكون متوسط النقاط داخل منطقة معينة. ثم يتم تصفية هؤلاء المرشحين في مرحلة ما بعد المعالجة لإزالة شبه المكررات وتشكيل المجموعة النهائية من المراكز.

.. dropdown:: التفاصيل الرياضية

  يتم تعديل موقع مرشحي المركز بشكل تكراري باستخدام تقنية تسمى التسلق الجبلي، والتي تجد القيم القصوى المحلية لكثافة الاحتمال المقدرة. بالنظر إلى مرشح المركز :math:`x` للتكرار :math:`t`، يتم تحديث المرشح وفقًا للمعادلة التالية:

  .. math::

      x^{t+1} = x^t + m(x^t)

  حيث :math:`m` هو متجه *mean shift* الذي يتم حسابه لكل مركز يشير نحو منطقة الزيادة القصوى في كثافة النقاط. لحساب :math:`m` ، نقوم بتعريف :math:`N(x)` على أنه الجوار للعينة داخل مسافة معينة حول :math:`x`. وبعد ذلك، يتم حساب :math:`m` باستخدام المعادلة التالية، مما يؤدي إلى تحديث مركز ليكون متوسط العينات داخل جواره:

  .. math::

      m(x) = \frac{1}{|N(x)|} \sum_{x_j \in N(x)}x_j - x

  بشكل عام، تعتمد معادلة :math:`m` على نواة تستخدم لتقدير الكثافة. الصيغة العامة هي:

  .. math::

      m(x) = \frac{\sum_{x_j \in N(x)}K(x_j - x)x_j}{\sum_{x_j \in N(x)}K(x_j -
      x)} - x

  في التنفيذ الخاص بنا، :math:`K(x)` تساوي 1 إذا كان :math:`x` صغيرًا بما فيه الكفاية وتساوي 0 بخلاف ذلك. بشكل فعال :math:`K(y - x)` يشير إلى ما إذا كان :math:`y` في جوار :math:`x`.


تضبط الخوارزمية تلقائيًا عدد العناقيد، بدلاً من الاعتماد على معامل ``bandwidth`` الذي يملي حجم المنطقة التي يجب البحث فيها. يمكن ضبط هذه المعلمة يدويًا، ولكن يمكن تقديرها باستخدام وظيفة ``estimate_bandwidth`` المقدمة، والتي يتم استدعاؤها إذا لم يتم ضبط عرض النطاق الترددي.

لا تتمتع الخوارزمية بقابلية عالية للتطوير، حيث تتطلب عمليات بحث متعددة عن الجيران الأقرب أثناء تنفيذ الخوارزمية. يضمن التقارب الخوارزمي، ومع ذلك ستتوقف الخوارزمية عن التكرار عندما يكون التغيير في المراكز صغيرًا.

يتم وضع علامة على عينة جديدة من خلال إيجاد أقرب مركز لعينة معينة.


.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_mean_shift_001.png
   :target: ../auto_examples/cluster/plot_mean_shift.html
   :align: center
   :scale: 5


.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`: Mean Shift clustering
  على مجموعة بيانات اصطناعية ثنائية الأبعاد مع 3 فئات.

.. dropdown:: المراجع

  * :doi:`"Mean shift: A robust approach toward feature space analysis"
    <10.1109/34.1000236>` D. Comaniciu and P. Meer, *IEEE Transactions on Pattern
    Analysis and Machine Intelligence* (2002)


.. _spectral_clustering:

تجميع الطيفي

    

(ملاحظة: لم أقم بترجمة القسم الخاص بتجميع الطيفي لأنه يقع خارج نطاق طلب الترجمة، ولكني سأقوم بترجمته إذا أردت.)

:class:`SpectralClustering` تقوم بإجراء تضمين منخفض الأبعاد لمصفوفة التقارب بين العينات، يليه التجميع، على سبيل المثال، بواسطة KMeans، لعناصر متجهاتeigenvectors في الفضاء منخفض الأبعاد. إنه فعال بشكل خاص إذا كانت مصفوفة التقارب متفرقة ويتم استخدام حل `amg` لمشكلة القيمة الذاتية (لاحظ أن حل `amg` يتطلب تثبيت وحدة `pyamg <https://github.com/pyamg/pyamg>`_).

الإصدار الحالي من SpectralClustering يتطلب تحديد عدد العناقيد مسبقًا. إنه يعمل بشكل جيد لعدد صغير من العناقيد، ولكن لا ينصح به للعديد من العناقيد.

بالنسبة لعنقودين، يحل SpectralClustering مشكلة استرخاء محدبة من مشكلة `normalized cuts <https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf>`_ على الرسم البياني للتشابه: قطع الرسم البياني إلى قسمين بحيث يكون وزن الحواف المقطوعة صغيرًا مقارنة بأوزان الحواف داخل كل عنقود. هذه المعايير هي محل اهتمام خاص عند العمل على الصور، حيث تكون رؤوس الرسم البياني هي البكسلات، ويتم حساب أوزان حواف رسم بياني التشابه باستخدام دالة من تدرج الصورة.

.. |noisy_img| image:: ../auto_examples/cluster/images/sphx_glr_plot_segmentation_toy_001.png
    :target: ../auto_examples/cluster/plot_segmentation_toy.html
    :scale: 50

.. |segmented_img| image:: ../auto_examples/cluster/images/sphx_glr_plot_segmentation_toy_002.png
    :target: ../auto_examples/cluster/plot_segmentation_toy.html
    :scale: 50

.. centered:: |noisy_img| |segmented_img|

.. warning:: تحويل المسافة إلى تشابهات حسنة السلوك

    لاحظ أنه إذا كانت قيم مصفوفة التشابه الخاصة بك ليست موزعة بشكل جيد، على سبيل المثال، مع قيم سالبة أو مع مصفوفة مسافة بدلاً من مصفوفة تشابه، فإن المشكلة الطيفية ستكون مفردة والمشكلة غير قابلة للحل. في هذه الحالة، يوصى بتطبيق تحويل على إدخالات المصفوفة. على سبيل المثال، في حالة مصفوفة مسافة موقعة، من الشائع تطبيق نواة حرارية::

        similarity = np.exp(-beta * distance / distance.std())

    راجع الأمثلة لمثل هذا التطبيق.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`: تجميع الكائنات من خلفية صاخبة باستخدام التجميع الطيفي.
* :ref:`sphx_glr_auto_examples_cluster_plot_coin_segmentation.py`: التجميع الطيفي لتقسيم صورة العملات المعدنية إلى مناطق.


.. |coin_kmeans| image:: ../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_001.png
  :target: ../auto_examples/cluster/plot_coin_segmentation.html
  :scale: 35

.. |coin_discretize| image:: ../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_002.png
  :target: ../auto_examples/cluster/plot_coin_segmentation.html
  :scale: 35

.. |coin_cluster_qr| image:: ../auto_examples/cluster/images/sphx_glr_plot_coin_segmentation_003.png
  :target: ../auto_examples/cluster/plot_coin_segmentation.html
  :scale: 35


استراتيجيات تعيين تسميات مختلفة
-------------------------------------

يمكن استخدام استراتيجيات تعيين تسميات مختلفة، مما يتوافق مع معلمة ``assign_labels`` لـ :class:`SpectralClustering`. يمكن لاستراتيجية ``"kmeans"`` مطابقة التفاصيل الدقيقة، ولكنها قد تكون غير مستقرة. على وجه الخصوص، ما لم تتحكم في ``random_state``، فقد لا تكون قابلة للتكرار من مسار إلى آخر، لأنها تعتمد على التهيئة العشوائية. البديل استراتيجية ``"discretize"`` هي 100٪ قابلة للتكرار، ولكنها تميل إلى إنشاء طرود ذات شكل متساوي ومنظم. الخيار الذي تمت إضافته مؤخرًا ``"cluster_qr"`` هو بديل حتمي يميل إلى إنشاء أفضل تقسيم بصريًا على تطبيق المثال أدناه.

================================  ================================  ================================
 ``assign_labels="kmeans"``        ``assign_labels="discretize"``    ``assign_labels="cluster_qr"``
================================  ================================  ================================
|coin_kmeans|                          |coin_discretize|                  |coin_cluster_qr|
================================  ================================  ================================

.. dropdown:: المراجع

  * `"Multiclass spectral clustering"
    <https://people.eecs.berkeley.edu/~jordan/courses/281B-spring04/readings/yu-shi.pdf>`_
    Stella X. Yu, Jianbo Shi, 2003

  * :doi:`"Simple, direct, and efficient multi-way spectral clustering"<10.1093/imaiai/iay008>`
    Anil Damle, Victor Minden, Lexing Ying, 2019


.. _spectral_clustering_graph:

التجميع الطيفي للرسوم البيانية
--------------------------

يمكن أيضًا استخدام التجميع الطيفي لتقسيم الرسوم البيانية عبر تضميناتها الطيفية. في هذه الحالة، تكون مصفوفة التقارب هي مصفوفة مجاورة للرسم البياني، ويتم تهيئة SpectralClustering باستخدام `affinity='precomputed'`::

    >>> from sklearn.cluster import SpectralClustering
    >>> sc = SpectralClustering(3, affinity='precomputed', n_init=100,
    ...                         assign_labels='discretize')
    >>> sc.fit_predict(adjacency_matrix)  # doctest: +SKIP

.. dropdown:: المراجع

  * :doi:`"A Tutorial on Spectral Clustering" <10.1007/s11222-007-9033-z>` Ulrike
    von Luxburg, 2007

  * :doi:`"Normalized cuts and image segmentation" <10.1109/34.868688>` Jianbo
    Shi, Jitendra Malik, 2000

  * `"A Random Walks View of Spectral Segmentation"
    <https://citeseerx.ist.psu.edu/doc_view/pid/84a86a69315e994cfd1e0c7debb86d62d7bd1f44>`_
    Marina Meila, Jianbo Shi, 2001

  * `"On Spectral Clustering: Analysis and an algorithm"
    <https://citeseerx.ist.psu.edu/doc_view/pid/796c5d6336fc52aa84db575fb821c78918b65f58>`_
    Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001

  * :arxiv:`"Preconditioned Spectral Clustering for Stochastic Block Partition
    Streaming Graph Challenge" <1708.07481>` David Zhuzhunashvili, Andrew Knyazev


.. _hierarchical_clustering:

التجميع الهرمي

    

التجميع الهرمي هو عائلة عامة من خوارزميات التجميع التي تبني تجمعات متداخلة عن طريق دمجها أو تقسيمها بشكل متتابع. يتم تمثيل هرمية التجمعات هذه كشجرة (أو دندوغرام). جذر الشجرة هو التجمع الفريد الذي يجمع كل العينات، والأوراق هي التجمعات التي تحتوي على عينة واحدة فقط. راجع صفحة ويكيبيديا للحصول على مزيد من التفاصيل.

يقوم كائن AgglomerativeClustering بتنفيذ تجميع هرمي باستخدام نهج تصاعدي: تبدأ كل ملاحظة في مجموعتها الخاصة، ويتم دمج التجمعات بشكل متتالي معًا. تحدد معايير الربط المقياس المستخدم لاستراتيجية الدمج:

- **Ward** يقلل من مجموع الفروق المربعة داخل جميع التجمعات. إنه نهج لتقليل التباين وبهذا المعنى يشبه دالة الهدف k-means ولكن يتم معالجتها بنهج هرمي تجميعي.
- **الحد الأقصى** أو **الربط الكامل** يقلل المسافة القصوى بين ملاحظات أزواج التجمعات.
- **متوسط الربط** يقلل متوسط المسافات بين جميع الملاحظات لأزواج التجمعات.
- **الربط الفردي** يقلل المسافة بين أقرب الملاحظات لأزواج التجمعات.

يمكن لـ AgglomerativeClustering أيضًا التوسع إلى عدد كبير من العينات عند استخدامه مع مصفوفة التوصيلية، ولكنه مكلف حسابيًا عندما لا يتم إضافة قيود التوصيلية بين العينات: فهو يعتبر في كل خطوة جميع عمليات الدمج المحتملة.

**FeatureAgglomeration**

يستخدم FeatureAgglomeration التجميع التراكمي لتجميع الميزات المتشابهة معًا، وبالتالي تقليل عدد الميزات. إنها أداة لتقليل الأبعاد، انظر تقليل البيانات.

أنواع ربط مختلفة: Ward، كامل، متوسط، وربط فردي
-------------------------------------------------------------------

يدعم AgglomerativeClustering استراتيجيات ربط Ward، والفردي، والمتوسط، والكامل.

.. image:: ../auto_examples/cluster/images/sphx_glr_plot_linkage_comparison_001.png
    :target: ../auto_examples/cluster/plot_linkage_comparison.html
    :scale: 43

يحتوي التجميع التراكمي على سلوك "الثراء يزداد ثراءً" الذي يؤدي إلى أحجام متفاوتة للتجمع. في هذا الصدد، فإن الربط الفردي هو أسوأ استراتيجية، ويعطي Ward الأحجام الأكثر انتظامًا. ومع ذلك، لا يمكن تغيير القرابة (أو المسافة المستخدمة في التجميع) باستخدام Ward، وبالتالي بالنسبة للقياسات غير الإقليدية، يكون الربط المتوسط بديلاً جيدًا. يمكن حساب الربط الفردي بكفاءة عالية جدًا، وبالتالي يمكن أن يكون مفيدًا لتوفير تجميع هرمي لمجموعات البيانات الأكبر حجمًا. يمكن أيضًا أن يعمل الربط الفردي بشكل جيد على البيانات غير الكروية.

أمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_digits_linkage.py`: استكشاف استراتيجيات الربط المختلفة في مجموعة بيانات حقيقية.

  * :ref:`sphx_glr_auto_examples_cluster_plot_linkage_comparison.py`: استكشاف استراتيجيات الربط المختلفة في مجموعات البيانات التجريبية.


تصور هرمية التجميع
----------------------------------

من الممكن تصور الشجرة التي تمثل الهرمية الدمج للتجمعات كدندوغرام. يمكن أن يكون الفحص البصري مفيدًا في كثير من الأحيان لفهم بنية البيانات، وإن كان ذلك أكثر في حالة أحجام العينات الصغيرة.

.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_dendrogram_001.png
    :target: ../auto_examples/cluster/plot_agglomerative_dendrogram.html
    :scale: 42

أمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_dendrogram.py`


إضافة قيود التوصيلية

    

جانب مثير للاهتمام في :class:`AgglomerativeClustering` هو أنه يمكن إضافة قيود التوصيلية إلى هذا الخوارزمية (فقط المجموعات المجاورة يمكن دمجها معا)، من خلال مصفوفة التوصيلية التي تحدد لكل عينة العينات المجاورة التالية لهيكل معين من البيانات. على سبيل المثال، في مثال السويس رول أدناه، تمنع قيود التوصيلية دمج النقاط غير المجاورة على السويس رول، وبالتالي تجنب تشكيل مجموعات تمتد عبر الطيات المتداخلة للرول.

.. |unstructured| image:: ../auto_examples/cluster/images/sphx_glr_plot_ward_structured_vs_unstructured_001.png
        :target: ../auto_examples/cluster/plot_ward_structured_vs_unstructured.html
        :scale: 49

.. |structured| image:: ../auto_examples/cluster/images/sphx_glr_plot_ward_structured_vs_unstructured_002.png
        :target: ../auto_examples/cluster/plot_ward_structured_vs_unstructured.html
        :scale: 49

.. centered:: |unstructured| |structured|

هذه القيود مفيدة لفرض بنية محلية معينة، لكنها أيضًا تجعل الخوارزمية أسرع، خاصة عندما يكون عدد العينات مرتفعًا.

يتم فرض قيود التوصيلية عبر مصفوفة التوصيلية: مصفوفة scipy sparse التي لها عناصر فقط عند تقاطع صف وعمود بمؤشرات مجموعة البيانات التي يجب توصيلها. يمكن بناء هذه المصفوفة من معلومات مسبقة: على سبيل المثال، قد ترغب في تجميع صفحات الويب عن طريق دمج الصفحات فقط مع ارتباط يشير من أحدها إلى الآخر. يمكن أيضًا تعلمه من البيانات، على سبيل المثال باستخدام :func:`sklearn.neighbors.kneighbors_graph` لتقييد الدمج إلى أقرب الجيران كما في :ref:`هذا المثال <sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py>`, أو باستخدام :func:`sklearn.feature_extraction.image.grid_to_graph` لتمكين دمج وحدات البكسل المجاورة فقط على الصورة، كما في مثال :ref:`coin <sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py>`.

.. warning:: **قيود التوصيلية مع الربط الأحادي والمتوسط والكامل**

    يمكن أن تعزز قيود التوصيلية والربط الأحادي أو الكامل أو المتوسط جانب "الثراء يزداد ثراءً" للتجميع الهرمي، خاصة إذا تم بناؤها باستخدام :func:`sklearn.neighbors.kneighbors_graph`. في حدود عدد صغير من المجموعات، فإنها تميل إلى إعطاء عدد قليل من المجموعات المحتلة بشكل كبير وتلك شبه الفارغة. (انظر المناقشة في :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`). الربط الأحادي هو خيار الربط الأكثر هشاشة فيما يتعلق بهذه المشكلة.

.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_001.png
    :target: ../auto_examples/cluster/plot_agglomerative_clustering.html
    :scale: 38

.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_002.png
    :target: ../auto_examples/cluster/plot_agglomerative_clustering.html
    :scale: 38

.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_003.png
    :target: ../auto_examples/cluster/plot_agglomerative_clustering.html
    :scale: 38

.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_004.png
    :target: ../auto_examples/cluster/plot_agglomerative_clustering.html
    :scale: 38

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`: تجميع وارد لتقسيم صورة العملات المعدنية إلى مناطق.

* :ref:`sphx_glr_auto_examples_cluster_plot_ward_structured_vs_unstructured.py`: مثال لخوارزمية وارد على السويس رول، مقارنة النهج المهيكلة بالنهج غير المهيكلة.

* :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`: مثال
  على تقليل الأبعاد باستخدام تجميع الميزات بناءً على تجميع وارد الهرمي.

* :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`


تغيير المقياس
-------------------

يمكن استخدام الربط الأحادي والمتوسط والكامل مع مجموعة متنوعة من المسافات (أو التقاربات)، لا سيما مسافة أوكليد (*l2*)، ومسافة مانهاتن (أو Cityblock، أو *l1*)، والمسافة الكوزين، أو أي مصفوفة تقارب مسبقة الحساب.

* مسافة *l1* غالبًا ما تكون جيدة للميزات النادرة، أو الضوضاء النادرة: أي
  العديد من الميزات صفر، كما في تعدين النص باستخدام تكرارات الكلمات النادرة.

* مسافة *cosine* مثيرة للاهتمام لأنها غير قابلة للتغيير في المقاييس العالمية
  لإشارة.

المبادئ التوجيهية لاختيار مقياس هي استخدام مقياس يزيد من المسافة بين العينات في فئات مختلفة، ويقلل ذلك ضمن كل فئة.

.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_metrics_005.png
    :target: ../auto_examples/cluster/plot_agglomerative_clustering_metrics.html
    :scale: 32

.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_metrics_006.png
    :target: ../auto_examples/cluster/plot_agglomerative_clustering_metrics.html
    :scale: 32

.. image:: ../auto_examples/cluster/images/sphx_glr_plot_agglomerative_clustering_metrics_007.png
    :target: ../auto_examples/cluster/plot_agglomerative_clustering_metrics.html
    :scale: 32

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering_metrics.py`


تقسيم K-Means

    

النص الذي تريد ترجمته هو عبارة عن وصف خوارزمية "Bisecting K-Means" المستخدمة في التجمعات الهرمية التقسيمية، وهي نسخة تكرارية من خوارزمية "K-Means". يتم إنشاء CENTROIDS بشكل تدريجي بناءً على التجمع السابق، حيث يتم تقسيم تجمع إلى تجمعين جديدين بشكل متكرر حتى يتم الوصول إلى عدد التجمعات المستهدف.

تعد خوارزمية "Bisecting K-Means" أكثر كفاءة من خوارزمية "K-Means" عندما يكون عدد التجمعات كبيرًا، حيث تعمل فقط على مجموعة فرعية من البيانات في كل قسم، بينما تعمل خوارزمية "K-Means" دائمًا على مجموعة البيانات الكاملة.

على الرغم من أن خوارزمية "Bisecting K-Means" لا يمكنها الاستفادة من مزايا التهيئة "k-means++"، إلا أنها ستظل تنتج نتائج قابلة للمقارنة مع "KMeans(init="k-means++")" من حيث القصور الذاتي بتكلفة حسابية أقل، ومن المحتمل أن تنتج نتائج أفضل من "KMeans" مع تهيئة عشوائية.

تعتبر هذه النسخة أكثر كفاءة من التجمعات الترابطية إذا كان عدد التجمعات صغيرًا مقارنة بعدد نقاط البيانات.

لا تنتج هذه النسخة تجمعات فارغة.

توجد استراتيجيتان لاختيار التجمع الذي سيتم تقسيمه:

- ``bisecting_strategy="largest_cluster"`` تختار التجمع الذي يحتوي على معظم النقاط
- ``bisecting_strategy="biggest_inertia"`` تختار التجمع بأكبر قصور ذاتي (تجمع بأكبر مجموع أخطاء مربعة داخلية)

عادةً ما ينتج الاختيار عن طريق أكبر عدد من نقاط البيانات نتائج دقيقة مثل الاختيار عن طريق القصور الذاتي، وهو أسرع (خاصةً بالنسبة لعدد أكبر من نقاط البيانات، حيث قد يكون حساب الخطأ مكلفًا).

من المحتمل أيضًا أن ينتج الاختيار عن طريق أكبر عدد من نقاط البيانات تجمعات ذات أحجام متشابهة، بينما من المعروف أن "KMeans" تنتج تجمعات بأحجام مختلفة.

يمكن رؤية الفرق بين "Bisecting K-Means" و"K-Means" العادية في المثال :ref:`sphx_glr_auto_examples_cluster_plot_bisect_kmeans.py`. بينما يميل خوارزمية "K-Means" العادية إلى إنشاء تجمعات غير مترابطة، فإن التجمعات من "Bisecting K-Means" منظمة جيدًا وتخلق هرمية واضحة.

يمكن الاطلاع على المراجع التالية للمزيد من المعلومات:

- `"A Comparison of Document Clustering Techniques" <http://www.philippe-fournier-viger.com/spmf/bisectingkmeans.pdf>`_ Michael Steinbach، George Karypis وVipin Kumar، قسم علوم الكمبيوتر والهندسة، جامعة مينيسوتا (يونيو 2000)
- `"Performance Analysis of K-Means and Bisecting K-Means Algorithms in Weblog Data" <https://ijeter.everscience.org/Manuscripts/Volume-4/Issue-8/Vol-4-issue-8-M-23.pdf>`_ K.Abirami وDr.P.Mayilvahanan، المجلة الدولية للتقنيات الناشئة في أبحاث الهندسة (IJETER) المجلد 4، العدد 8، (أغسطس 2016)
- `"Bisecting K-means Algorithm Based on K-valued Self-determining and Clustering Center Optimization" <http://www.jcomputers.us/vol13/jcp1306-01.pdf>`_ Jian Di، Xinyue Gou School of Control and Computer Engineering، جامعة شمال الصين الكهربائية، باودينغ، خبي، الصين (أغسطس 2017)

DBSCAN

(لم يتم توفير معلومات عن DBSCAN في النص المقدم.)

  الخوارزمية :class:`DBSCAN` تنظر إلى العناقيد كمناطق ذات كثافة عالية مفصولة بمناطق ذات كثافة منخفضة. وبسبب هذا المنظور العام، يمكن أن تكون العناقيد التي تم العثور عليها بواسطة DBSCAN بأي شكل، على عكس خوارزمية k-means التي تفترض أن العناقيد ذات شكل محدب. المكون الرئيسي لخوارزمية DBSCAN هو مفهوم "عينات النواة"، وهي عينات تقع في مناطق ذات كثافة عالية. وبالتالي، فإن العنقود عبارة عن مجموعة من عينات النواة، كل منها قريب من الآخر (يتم قياسه ببعض مقاييس المسافة) ومجموعة من العينات غير النواة التي تكون قريبة من عينة نواة (ولكنها ليست نفسها عينات نواة). هناك معلمان للخوارزمية، `min_samples` و `eps`، اللذان يعرفان بشكل رسمي ما نعنيه عندما نقول "كثيف". يشير `min_samples` الأعلى أو `eps` الأدنى إلى كثافة أعلى ضرورية لتشكيل عنقود.

    بشكل أكثر رسمية، نعرف عينة النواة على أنها عينة في مجموعة البيانات الموجودة مثل هذا، هناك `min_samples` عينات أخرى داخل مسافة `eps`، والتي يتم تعريفها على أنها "جيران" لعينة النواة. هذا يخبرنا أن عينة النواة تقع في منطقة كثيفة من الفضاء المتجه. العنقود عبارة عن مجموعة من عينات النواة التي يمكن بناؤها عن طريق أخذ عينة نواة بشكل متكرر، وإيجاد جميع جيرانها الذين هم عينات نواة، وإيجاد جميع جيرانهم الذين هم عينات نواة، وهكذا. يحتوي العنقود أيضًا على مجموعة من العينات غير النواة، وهي عينات تكون جيرانها لعينة نواة في العنقود ولكنها ليست عينات نواة نفسها. بشكل حدسي، هذه العينات على هامش العنقود.

    أي عينة نواة هي جزء من عنقود، بحكم التعريف. أي عينة ليست عينة نواة، وتبعد مسافة `eps` على الأقل عن أي عينة نواة، تعتبر شاذة من قبل الخوارزمية.

    في حين أن معلمة `min_samples` تتحكم أساسًا في مدى تسامح الخوارزمية مع الضوضاء (في مجموعات البيانات الكبيرة والضوضاء قد يكون من المرغوب فيه زيادة هذه المعلمة)، فإن المعلمة `eps` هي *حاسمة لاختيارها بشكل مناسب* لمجموعة البيانات ووظيفة المسافة ولا يمكن عادةً تركها عند القيمة الافتراضية. يتحكم في الحي المحلي للنقاط. عندما يتم اختياره صغيرًا جدًا، لن يتم تجميع معظم البيانات على الإطلاق (ويتم وضع علامة عليها باسم `-1` لـ "الضوضاء"). عندما يتم اختياره كبيرًا جدًا، فإنه يتسبب في دمج العناقيد القريبة في مجموعة واحدة، وفي النهاية يتم إرجاع مجموعة البيانات بالكامل كعنقود واحد. تمت مناقشة بعض الطرق الإرشادية لاختيار هذه المعلمة في الأدبيات، على سبيل المثال بناءً على الركبة في رسم مسافات الجيران الأقرب (كما نوقش في المراجع أدناه).

    في الشكل أدناه، يشير اللون إلى عضوية العنقود، مع الدوائر الكبيرة التي تشير إلى عينات النواة التي وجدتها الخوارزمية. الدوائر الأصغر هي عينات غير نواة لا تزال جزءًا من العنقود. علاوة على ذلك، يتم الإشارة إلى القيم المتطرفة بالنقاط السوداء أدناه.

    .. |dbscan_results| image:: ../auto_examples/cluster/images/sphx_glr_plot_dbscan_002.png
        :target: ../auto_examples/cluster/plot_dbscan.html
        :scale: 5

    .. centered:: |dbscan_results|

    .. rubric:: أمثلة

    * :ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`

    .. dropdown:: التنفيذ

      خوارزمية DBSCAN محددة، وتنتج دائمًا نفس العناقيد عند إعطاء نفس البيانات بنفس الترتيب. ومع ذلك، يمكن أن تختلف النتائج عند تقديم البيانات بترتيب مختلف. أولاً، على الرغم من أن عينات النواة سيتم تعيينها دائمًا إلى نفس العناقيد، فإن تسميات تلك العناقيد ستعتمد على الترتيب الذي يتم فيه مواجهة تلك العينات في البيانات. ثانيًا والأهم من ذلك، يمكن أن تختلف العناقيد التي يتم تعيين عينات غير النواة لها اعتمادًا على ترتيب البيانات. سيحدث هذا عندما تكون عينة غير نواة لها مسافة أقل من `eps` إلى عينتي نواة في مجموعات مختلفة. وفقًا لعدم المساواة الثلاثي، يجب أن تكون تلك العينتان الأساسيتان أكثر بعدًا من `eps` عن بعضهما البعض، وإلا فسيكونان في نفس العنقود. يتم تعيين العينة غير النواة إلى أي مجموعة يتم إنشاؤها أولاً في اجتياز البيانات، وبالتالي ستعتمد النتائج على ترتيب البيانات.

      يستخدم التنفيذ الحالي أشجار الكرة والأشجار kd لتحديد جوار النقاط، مما يتجنب حساب مصفوفة المسافة الكاملة (كما كان الحال في إصدارات scikit-learn قبل 0.14). يتم الاحتفاظ بإمكانية استخدام مقاييس مخصصة؛ للتفاصيل، انظر :class:`NearestNeighbors`.

    .. dropdown:: استهلاك الذاكرة لأحجام العينات الكبيرة

      هذا التنفيذ ليس افتراضيًا ذا كفاءة في استخدام الذاكرة لأنه يبني مصفوفة تشابه زوجية كاملة في الحالة التي لا يمكن فيها استخدام أشجار kd أو أشجار الكرة (على سبيل المثال، باستخدام المصفوفات المتفرقة). ستستهلك هذه المصفوفة :math:`n^2` أعدادًا كسرية. بعض الآليات للتغلب على هذا هي:

      - استخدم :ref:`OPTICS <optics>` clustering بالاقتران مع طريقة `extract_dbscan`. كما تحسب مجموعات OPTICS أيضًا المصفوفة الزوجية الكاملة، ولكنها تحتفظ بسطر واحد فقط في الذاكرة في كل مرة (تعقيد الذاكرة n).

      - يمكن حساب مخطط حي نصف قطر متفرق مسبقًا (حيث يفترض أن الإدخالات المفقودة خارج `eps`) ويمكن تشغيل dbscan فوق هذا مع ``metric='precomputed'``. انظر :meth:`sklearn.neighbors.NearestNeighbors.radius_neighbors_graph`.

      - يمكن ضغط مجموعة البيانات، إما عن طريق إزالة التكرارات الدقيقة إذا حدثت في بياناتك، أو باستخدام BIRCH. ثم يكون لديك فقط عدد صغير نسبيًا من الممثلين لعدد كبير من النقاط. يمكنك بعد ذلك توفير ``sample_weight`` عند تركيب DBSCAN.

    .. dropdown:: المراجع

    * `خوارزمية تعتمد على الكثافة لاكتشاف العناقيد في قواعد البيانات المكانية الكبيرة مع الضوضاء <https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf>`_ إستر، م.، هـ. ب. كريجل، ي. ساندر، و X. Xu، في وقائع المؤتمر الدولي الثاني لاكتشاف المعرفة واستخراج البيانات، بورتلاند، أوريغون، مطبعة AAAI، ص. 226-231. 1996

    * :doi:`DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. <10.1145/3068335>` شوبيرت، إي، ساندر، ج، إستر، م، كريجل، هـ. ب.، و XU، X. (2017). في معاملات ACM على أنظمة قواعد البيانات (TODS)، 42 (3)، 19.


    .. _hdbscan:

    HDBSCAN

    

(ملحوظة: لم تتم ترجمة الرموز الخاصة ولا المعادلات الرياضية ولا الروابط والتاجات ولا الشفرة البرمجية كما هو مطلوب في السؤال)

  الخوارزمية :class:`HDBSCAN` يمكن اعتبارها امتدادًا للخوارزميتين :class:`DBSCAN` و :class:`OPTICS`. على وجه التحديد، تفترض :class:`DBSCAN` أن معيار التجميع (مثل متطلبات الكثافة) متجانس *عالميًا*. بمعنى آخر، قد تكافح :class:`DBSCAN` لالتقاط مجموعات ذات كثافات مختلفة بنجاح.
    تخفف :class:`HDBSCAN` هذا الافتراض وتستكشف جميع مقاييس الكثافة الممكنة من خلال بناء تمثيل بديل لمشكلة التجميع.

    .. note::

      هذا التنفيذ هو نسخة معدلة من التنفيذ الأصلي لـ HDBSCAN، `scikit-learn-contrib/hdbscan <https://github.com/scikit-learn-contrib/hdbscan>`_ بناءً على [LJ2017]_.

    .. rubric:: أمثلة

    * :ref:`sphx_glr_auto_examples_cluster_plot_hdbscan.py`

    رسم بياني للوصول المتبادل
    -------------------------

    تقوم HDBSCAN أولاً بتعريف :math:`d_c(x_p)`, *المسافة الأساسية* لعينة :math:`x_p`, على أنها المسافة إلى جارها الأقرب `min_samples`، بما في ذلك نفسها. على سبيل المثال،
    إذا كان `min_samples=5` و :math:`x_*` هو الجار الخامس الأقرب لـ :math:`x_p`
    فإن المسافة الأساسية هي:

    .. math:: d_c(x_p)=d(x_p, x_*).

    بعد ذلك ، يحدد :math:`d_m(x_p, x_q)`, *مسافة الوصول المتبادل* لنقطتين
    :math:`x_p, x_q`, كالتالي:

    .. math:: d_m(x_p, x_q) = \max\{d_c(x_p), d_c(x_q), d(x_p, x_q)\}

    هذه الفكرتان تتيحان لنا بناء *رسم بياني للوصول المتبادل*
    :math:`G_{ms}` المحدد لاختيار ثابت لـ `min_samples` عن طريق ربط كل
    عينة :math:`x_p` برأس من رؤوس الرسم البياني، وبالتالي تكون الحواف بين النقطتين
    :math:`x_p, x_q` هي مسافة الوصول المتبادل :math:`d_m(x_p, x_q)`
    بينهما. يمكننا إنشاء مجموعات فرعية من هذا الرسم البياني، يشار إليها باسم
    :math:`G_{ms,\varepsilon}`, عن طريق إزالة أي حواف ذات قيمة أكبر من :math:`\varepsilon`:
    من الرسم البياني الأصلي. في هذه المرحلة، يتم تمييز أي نقاط ذات مسافة أساسية أقل من :math:`\varepsilon`: على أنها ضجيج. ثم يتم تجميع النقاط المتبقية عن طريق
    العثور على المكونات المرتبطة بهذا الرسم البياني المفصول.

    .. note::

      إن أخذ المكونات المرتبطة لرسم بياني مقطوع :math:`G_{ms,\varepsilon}` هو
      ما يعادل تشغيل DBSCAN* مع `min_samples` و :math:`\varepsilon`. DBSCAN* هي نسخة
      معدلة قليلاً من DBSCAN مذكورة في [CM2013]_.

    التجميع الهرمي
    -----------------------
    يمكن اعتبار HDBSCAN خوارزمية تقوم بتجميع DBSCAN* عبر جميع
    قيم :math:`\varepsilon`. كما ذكرنا سابقًا، هذا يعادل إيجاد المكونات المتصلة
    لرسوم بيانية الوصول المتبادل لجميع قيم :math:`\varepsilon`. للقيام بذلك
    بكفاءة، تستخرج HDBSCAN أولاً شجرة ممتدة دنيا (MST) من الرسم البياني الوصول المتبادل الكامل، ثم تقطع بشكل طماع الحواف ذات الوزن الأعلى. فيما يلي مخطط لخوارزمية HDBSCAN:

    1. استخراج MST من :math:`G_{ms}`.
    2. قم بتوسيع MST عن طريق إضافة "حافة ذاتية" لكل رأس، مع وزن يساوي
       المسافة الأساسية للعينة الأساسية.
    3. قم بتهيئة مجموعة واحدة وعلامة واحدة لمخطط MST.
    4. قم بإزالة الحافة ذات الوزن الأكبر من MST (يتم إزالة العلاقات في وقت واحد).
    5. قم بتعيين تسميات المجموعة للمكونات المرتبطة التي تحتوي على
       نقاط النهاية للحافة التي تمت إزالتها الآن. إذا كان المكون لا يحتوي على حافة واحدة على الأقل، يتم تعيين "علامة" فارغة تشير إلى أنه ضجيج.
    6. كرر الخطوات 4-5 حتى لا توجد مكونات متصلة أخرى.

    وبالتالي، فإن HDBSCAN قادرة على الحصول على جميع الأقسام الممكنة التي يمكن تحقيقها بواسطة
    DBSCAN* لاختيار ثابت لـ `min_samples` بطريقة هرمية.
    في الواقع، هذا يسمح لـ HDBSCAN بأداء التجميع عبر كثافات متعددة
    وبالتالي لم تعد بحاجة إلى :math:`\varepsilon` ليتم إعطاؤها كمعلمة. بدلاً من ذلك
    تعتمد فقط على اختيار `min_samples`، والتي تميل إلى أن تكون معلمة أكثر قوة.

    .. |hdbscan_ground_truth| image:: ../auto_examples/cluster/images/sphx_glr_plot_hdbscan_005.png
        :target: ../auto_examples/cluster/plot_hdbscan.html
        :scale: 75
    .. |hdbscan_results| image:: ../auto_examples/cluster/images/sphx_glr_plot_hdbscan_007.png
        :target: ../auto_examples/cluster/plot_hdbscan.html
        :scale: 75

    .. centered:: |hdbscan_ground_truth|
    .. centered:: |hdbscan_results|

    يمكن تنعيم HDBSCAN باستخدام معلمة إضافية `min_cluster_size`
    والتي تحدد أنه أثناء التجميع الهرمي، تعتبر المكونات التي تحتوي على عينات أقل من
    `minimum_cluster_size` ضجيجًا. في الممارسة العملية، يمكن للمرء تعيين `minimum_cluster_size = min_samples` لدمج المعلمات وتبسيط مساحة المعلمة.

    .. rubric:: المراجع

    .. [CM2013] Campello، R.J.G.B.، Moulavi، D.، Sander، J. (2013). التجميع القائم على الكثافة
      استنادًا إلى تقديرات الكثافة الهرمية. في: Pei، J.، Tseng، V.S.،
      Cao، L.، Motoda، H.، Xu، G. (محرران) التطورات في اكتشاف المعرفة واستخراج البيانات. PAKDD 2013. محاضرات في علوم الحاسب الآلي()، المجلد 7819. سبرنجر،
      برلين، هايدلبرغ. :doi:`التجميع القائم على الكثافة
      استنادًا إلى تقديرات الكثافة الهرمية <10.1007/978-3-642-37456-2_14>`

    .. [LJ2017] L. McInnes و J. Healy، (2017). تسريع التسلسل الهرمي القائم على الكثافة
      التجميع. في: IEEE المؤتمر الدولي لورش عمل استخراج البيانات
      (ICDMW)، 2017، ص. 33-42. :doi:`تسريع التسلسل الهرمي القائم على الكثافة
      التجميع <10.1109/ICDMW.2017.12>`

    .. _optics:

    OPTICS
    
    (لا تترجم هذا الجزء الأخير "OPTICS" لأنه ليس نصًا مترجمًا بل هو عنوان للقسم التالي.)

  الخوارزمية :class:`OPTICS` تتشارك العديد من أوجه التشابه مع خوارزمية :class:`DBSCAN`، ويمكن اعتبارها تعميماً لـ DBSCAN التي تُرخي شرط ``eps`` من قيمة واحدة إلى نطاق قيمة. يكمن الاختلاف الرئيسي بين DBSCAN و OPTICS في أن خوارزمية OPTICS تبني رسم بياني *reachability*، والذي يعين لكل عينة كل من مسافة ``reachability_`` ومكان داخل سمة ``ordering_`` للمجموعة؛ يتم تعيين هاتين السمتين عند تركيب النموذج، ويتم استخدامهما لتحديد عضوية المجموعة. إذا تم تشغيل OPTICS مع القيمة الافتراضية *inf* المحددة لـ ``max_eps``، فيمكن إجراء استخراج مجموعة على غرار DBSCAN بشكل متكرر في وقت خطي لأي قيمة ``eps`` معينة باستخدام طريقة ``cluster_optics_dbscan``. سيؤدي تعيين ``max_eps`` إلى قيمة أقل إلى تقليل أوقات التشغيل، ويمكن اعتباره نصف قطر الجوار الأقصى من كل نقطة للعثور على نقاط قابلة للوصول المحتملة الأخرى.

    .. |optics_results| image:: ../auto_examples/cluster/images/sphx_glr_plot_optics_001.png
            :target: ../auto_examples/cluster/plot_optics.html
            :scale: 50

    .. centered:: |optics_results|

    تسمح مسافات *reachability* التي تولدها OPTICS باستخراج الكثافة المتغيرة للمجموعات داخل مجموعة بيانات واحدة. كما هو موضح في الرسم البياني أعلاه، فإن الجمع بين مسافات *reachability* و ``ordering_`` لمجموعة البيانات ينتج *reachability plot*، حيث يتم تمثيل كثافة النقطة على المحور ص، ويتم ترتيب النقاط بحيث تكون النقاط القريبة متجاورة. ينتج عن "قطع" مخطط قابلية الوصول عند قيمة واحدة نتائج تشبه DBSCAN؛ يتم تصنيف جميع النقاط فوق "قطع" على أنها ضوضاء، وكل مرة يحدث فيها كسر عند القراءة من اليسار إلى اليمين تعني مجموعة جديدة. يبحث استخراج المجموعة الافتراضية باستخدام OPTICS في المنحدرات الشديدة داخل الرسم البياني لإيجاد مجموعات، ويمكن للمستخدم تعريف ما يعتبر منحدرًا شديدًا باستخدام المعلمة ``xi``. هناك أيضًا إمكانيات أخرى للتحليل على الرسم البياني نفسه، مثل توليد تمثيلات هرمية للبيانات من خلال مخططات الوصول الهرمية، ويمكن الوصول إلى تسلسل هرمية المجموعات التي اكتشفتها الخوارزمية من خلال معلمة ``cluster_hierarchy_``. تم تلوين الرسم البياني أعلاه بحيث تتطابق ألوان المجموعة في الفضاء المستوي مع مجموعات القطاعات الخطية لمخطط قابلية الوصول. لاحظ أن المجموعات الزرقاء والحمراء متجاورة في مخطط قابلية الوصول، ويمكن تمثيلها هرميًا كأطفال لمجموعة أب أكبر.

    .. rubric:: أمثلة

    * :ref:`sphx_glr_auto_examples_cluster_plot_optics.py`


    .. dropdown:: مقارنة مع DBSCAN

      النتائج من طريقة OPTICS ``cluster_optics_dbscan`` و DBSCAN متشابهة للغاية، ولكنها ليست متطابقة دائمًا؛ على وجه التحديد، تسمية نقاط المحيط والضوضاء. ويرجع ذلك جزئيًا إلى أن العينات الأولى من كل منطقة كثيفة تتم معالجتها بواسطة OPTICS لها قيمة reachability كبيرة بينما تكون قريبة من نقاط أخرى في منطقتها، وبالتالي سيتم تمييزها أحيانًا على أنها ضوضاء بدلاً من المحيط. هذا يؤثر على النقاط المجاورة عندما يتم اعتبارها كمرشحين لتمييزها إما بالمحيط أو الضوضاء.

      لاحظ أنه لأي قيمة واحدة من ``eps``، فإن DBSCAN ستميل إلى وقت تشغيل أقصر من OPTICS؛ ومع ذلك، بالنسبة للتشغيل المتكرر عند قيم ``eps`` المختلفة، قد يتطلب التشغيل الفردي لـ OPTICS وقت تشغيل تراكمي أقل من DBSCAN. من المهم أيضًا ملاحظة أن إخراج OPTICS قريب من DBSCAN فقط إذا كانت ``eps`` و ``max_eps`` قريبة.

    .. dropdown:: التعقيد الحسابي

      يتم استخدام أشجار الفهرسة المكانية لتجنب حساب مصفوفة المسافة الكاملة، وتسمح باستخدام الذاكرة بكفاءة على مجموعات كبيرة من العينات. يمكن توفير مقاييس مسافة مختلفة عبر كلمة ``metric`` الرئيسية.

      بالنسبة لمجموعات البيانات الكبيرة، يمكن الحصول على نتائج مشابهة (ولكن ليست متطابقة) عبر :class:`HDBSCAN`. تطبيق HDBSCAN متعدد الخيوط، وله تعقيد وقت تشغيل خوارزمي أفضل من OPTICS، على حساب التدرج الأسوأ للذاكرة. بالنسبة لمجموعات البيانات الكبيرة للغاية التي تستنفد ذاكرة النظام باستخدام HDBSCAN، ستحافظ OPTICS على :math:`n` (بدلاً من :math:`n^ 2`) تحجيم الذاكرة؛ ومع ذلك، من المحتمل أن يلزم استخدام ضبط معلمة ``max_eps`` لإعطاء حل في وقت جداري معقول.


    .. dropdown:: المراجع

      * "OPTICS: ترتيب النقاط لتحديد بنية التجميع." Ankerst، Mihael، Markus M. Breunig، Hans-Peter Kriegel، و Jörg Sander. في ACM Sigmod Record، المجلد 28، العدد 2، الصفحات 49-60. ACM، 1999.


    .. _birch:

    BIRCH

    

    (ملاحظة:Birch لم تتم ترجمته لأنه كان عنوانا فقط دون محتوى)

يُنشئ Birch شجرة تسمى شجرة ميزات التجميع (CFT) للبيانات المحددة. يتم ضغط البيانات بشكل أساسي بدون فقدان إلى مجموعة من عقد ميزات التجميع (عقد CF). تحتوي عقد CF على عدد من العناقيد الفرعية المسماة بتجميع الميزات الفرعية (CF Subclusters) وهذه العناقيد الفرعية CF الموجودة في عقد CF غير الطرفية يمكن أن يكون لها عقد CF كأطفال.

تحتوي العناقيد الفرعية CF على المعلومات اللازمة للتجميع والتي تمنع الحاجة إلى الاحتفاظ بكامل بيانات الإدخال في الذاكرة. تتضمن هذه المعلومات:

- عدد العينات في العنقود الفرعي.
- المجموع الخطي - متجه ذو أبعاد n يحمل مجموع جميع العينات
- مجموع مربعات القاعدة L2 لجميع العينات.
- CENTROIDS - لتجنب إعادة الحساب المجموع الخطي / n_samples.
- المعيار المربع لل CENTROIDS.

لدى خوارزمية BIRCH اثنين من المعالم، الحد الأدنى وعامل التفرع. يحدد عامل التفرع عدد العناقيد الفرعية في عقدة ويحدد الحد الأدنى المسافة بين العينة الداخلة والعناقيد الفرعية الموجودة.

يمكن اعتبار هذه الخوارزمية كطريقة تقليل بيانات أو عينات، لأنها تقلل بيانات الإدخال إلى مجموعة من العناقيد الفرعية التي يتم الحصول عليها مباشرة من أوراق CFT. يمكن معالجة هذه البيانات المخفضة بشكل أكبر عن طريق تغذيتها إلى مجمع عالمي. يمكن تعيين هذا المجمع العالمي بواسطة ``n_clusters``. إذا تم تعيين ``n_clusters`` إلى None، فإن العناقيد الفرعية من الأوراق تتم قراءتها مباشرة، وإلا فإن خطوة التجميع العالمية تسجل هذه العناقيد الفرعية في الأعناقيد العالمية (العلامات) ويتم تعيين العينات إلى الملصق العالمي لأقرب عنقود فرعي.

.. dropdown:: وصف الخوارزمية

  - يتم إدخال عينة جديدة في جذر شجرة CF والتي تكون عقدة CF. ثم يتم دمجها مع العنقود الفرعي للجذر، الذي له أصغر نصف قطر بعد الدمج، مقيدًا بشروط الحد الأدنى وعامل التفرع. إذا كان للعنقود الفرعي أي عقدة طفل، فإن هذا يتكرر حتى يصل إلى ورقة. بعد العثور على أقرب عنقود فرعي في الورقة، يتم تحديث خصائص هذا العنقود الفرعي والعناقيد الفرعية الأصل بشكل متكرر.

  - إذا كان نصف قطر العنقود الفرعي الذي تم الحصول عليه عن طريق دمج العينة الجديدة وأقرب عنقود فرعي أكبر من مربع الحد الأدنى وإذا كان عدد العناقيد الفرعية أكبر من عامل التفرع، يتم تخصيص مساحة مؤقتة لهذه العينة الجديدة. يتم أخذ أبعد عقدتين فرعيتين ويتم تقسيم العناقيد الفرعية إلى مجموعتين على أساس المسافة بين هذه العناقيد الفرعية.

  - إذا كانت عقدة الانقسام هذه لها عنقود فرعي الأصل وهناك مجال لعنقود فرعي جديد، فإن الأصل ينقسم إلى اثنين. إذا لم يكن هناك مجال، فإن هذه العقدة تنقسم مرة أخرى إلى اثنين وتستمر العملية بشكل متكرر، حتى تصل إلى الجذر.

.. dropdown:: BIRCH أو MiniBatchKMeans؟

  - لا يتدرج BIRCH جيدًا للبيانات عالية الأبعاد. كقاعدة عامة إذا كان ``n_features`` أكبر من عشرين، فمن الأفضل بشكل عام استخدام MiniBatchKMeans.
  - إذا كان عدد حالات البيانات يحتاج إلى تخفيض، أو إذا أراد المرء عددًا كبيرًا من العناقيد الفرعية إما كخطوة أولية أو غير ذلك، فإن BIRCH أكثر فائدة من MiniBatchKMeans.

  .. image:: ../auto_examples/cluster/images/sphx_glr_plot_birch_vs_minibatchkmeans_001.png
    :target: ../auto_examples/cluster/plot_birch_vs_minibatchkmeans.html

.. dropdown:: كيفية استخدام partial_fit؟

  لتجنب حساب التجميع العالمي، ينصح المستخدم لكل مكالمة من ``partial_fit``:

  1. تعيين ``n_clusters=None`` مبدئيًا.
  2. تدريب جميع البيانات عن طريق مكالمات متعددة لـ partial_fit.
  3. تعيين ``n_clusters`` إلى قيمة مطلوبة باستخدام
     ``brc.set_params(n_clusters=n_clusters)``.
  4. استدعاء ``partial_fit`` أخيرًا بدون أي وسيطات، أي ``brc.partial_fit()``
     والذي يؤدي التجميع العالمي.

.. dropdown:: المراجع

  * تيان تشانغ، راغو راماكريشنان، مارون ليفني BIRCH: طريقة تجميع بيانات فعالة لقواعد البيانات الكبيرة.
    https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf

  * روبرتو بيرديسي JBirch - تنفيذ جافا لخوارزمية التجميع BIRCH
    https://code.google.com/archive/p/jbirch



.. _clustering_evaluation:

تقييم أداء التجميع

    

(ملاحظة: لم تتم ترجمة الروابط والصور والرموز الخاصة والرموز والمعادلات الرياضية والشفرة البرمجية وفقًا للتعليمات)

تقييم أداء خوارزمية التجميع ليس بتافه مثل حساب عدد الأخطاء أو دقة واسترجاع خوارزمية التصنيف الخاضعة للإشراف. على وجه الخصوص، يجب ألا يأخذ أي مقياس تقييم القيم المطلقة لتسميات العنقود في الاعتبار، ولكن إذا كان هذا التجميع يحدد فواصل للبيانات مشابهة لمجموعة من الفئات الحقيقية أو تلبية افتراض معين بحيث يكون الأعضاء المنتمون إلى نفس الفئة أكثر تشابهًا من أعضاء الفئات المختلفة وفقًا لبعض مقاييس التشابه.

.. currentmodule:: sklearn.metrics

.. _rand_score:
.. _adjusted_rand_score:

مؤشر راند

(ملاحظة: لا توجد ترجمة للرموز الخاصة والرموز والمعادلات الرياضية والروابط والتاجات والشفرة البرمجية، وتم الحفاظ على تنسيق RST الأصلي في الترجمة.)

بالنظر إلى معرفة تعيينات الفئة الحقيقية الأرضية `labels_true` وتعيينات الخوارزمية العنقودية لنفس العينات `labels_pred`، فإن **مؤشر راند (المعدل أو غير المعدل)** هو دالة تقيس **تشابه** التعيينين، متجاهلة التباديل::

  >>> من sklearn استيراد metrics
  >>> labels_true = [0, 0, 0, 1, 1, 1]
  >>> labels_pred = [0, 0, 1, 1, 2, 2]
  >>> metrics.rand_score(labels_true, labels_pred)
  0.66...

لا يضمن مؤشر راند الحصول على قيمة قريبة من 0.0 لتسمية عشوائية. مؤشر راند المعدل **يصحح من أجل الصدفة** وسيعطي مثل هذا الأساس.

  >>> metrics.adjusted_rand_score(labels_true, labels_pred)
  0.24...

كما هو الحال مع جميع مقاييس التجمعات، يمكن للمرء أن يبدل 0 و 1 في التسمية المتوقعة، وإعادة تسمية 2 إلى 3، والحصول على نفس النتيجة::

  >>> labels_pred = [1, 1, 0, 0, 3, 3]
  >>> metrics.rand_score(labels_true, labels_pred)
  0.66...
  >>> metrics.adjusted_rand_score(labels_true, labels_pred)
  0.24...

علاوة على ذلك، كل من :func:`rand_score` :func:`adjusted_rand_score` **متناظر**ان: تبديل الحجة لا يغير النقاط. وبالتالي يمكن استخدامها كمقاييس **توافق**::

  >>> metrics.rand_score(labels_pred, labels_true)
  0.66...
  >>> metrics.adjusted_rand_score(labels_pred, labels_true)
  0.24...

يتم تسجيل التسمية المثالية 1.0::

  >>> labels_pred = labels_true[:]
  >>> metrics.rand_score(labels_true, labels_pred)
  1.0
  >>> metrics.adjusted_rand_score(labels_true, labels_pred)
  1.0

تتلقى التسميات متدنية التوافق (مثل التسمية المستقلة) درجات أقل، وبالنسبة لمؤشر راند المعدل ستكون النتيجة سلبية أو قريبة من الصفر. ومع ذلك، بالنسبة لمؤشر راند غير المعدل، ستكون النتيجة، على الرغم من كونها أقل، ليست بالضرورة قريبة من الصفر.::

  >>> labels_true = [0, 0, 0, 0, 0, 0, 1, 1]
  >>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]
  >>> metrics.rand_score(labels_true, labels_pred)
  0.39...
  >>> metrics.adjusted_rand_score(labels_true, labels_pred)
  -0.07...


.. topic:: المميزات:

  - **القابلية للتفسير**: مؤشر راند غير المعدل متناسب مع عدد أزواج العينات التي تكون تسمياتها متماثلة في كل من `labels_pred` و `labels_true`، أو تكون مختلفة في كليهما.

  - **تعيينات التسمية العشوائية (الموحدة) لها درجة مؤشر راند معدلة قريبة من 0.0** لأي قيمة من ``n_clusters`` و ``n_samples`` (وهو ليس هو الحال بالنسبة لمؤشر راند غير المعدل أو مقياس V-measure على سبيل المثال).

  - **النطاق المحدود**: تشير القيم المنخفضة إلى تسميات مختلفة، ولدى التجمعات المماثلة مؤشر راند مرتفع (معدل أو غير معدل)، 1.0 هي نتيجة المطابقة المثالية. نطاق النتيجة هو [0، 1] لمؤشر راند غير المعدل و [-0.5، 1] لمؤشر راند المعدل.

  - **لا يتم افتراض أي افتراض عن بنية المجموعة**: يمكن استخدام مؤشر راند (المعدل أو غير المعدل) لمقارنة جميع أنواع خوارزميات التجمعات، ويمكن استخدامها لمقارنة خوارزميات التجمعات مثل k-means التي تفترض أشكالًا متساوية الأطوار مع نتائج خوارزميات التجمعات الطيفية التي يمكن أن تجد تجمعًا بـ "أشكال مطوية".

.. topic:: العيوب:

  - على عكس القصور الذاتي، **يتطلب مؤشر راند (المعدل أو غير المعدل) معرفة فئات الحقيقة الأرضية** والتي لا تكون متاحة تقريبًا في الممارسة العملية أو تتطلب تعيينات يدوية من قبل المعلقين البشريين (كما هو الحال في إعداد التعلم الخاضع للإشراف).

    ومع ذلك، يمكن أن يكون (مؤشر راند المعدل أو غير المعدل) مفيدًا أيضًا في إعداد غير خاضع للإشراف ككتلة بناء لمؤشر توافق يمكن استخدامه لاختيار نموذج التجميع (TODO).

  - غالبًا ما يكون **مؤشر راند غير المعدل قريبًا من 1.0** حتى لو كانت التجمعات نفسها مختلفة اختلافًا كبيرًا. يمكن فهم ذلك عند تفسير مؤشر راند على أنه دقة تسمية أزواج العناصر الناتجة عن التجمعات: في الممارسة العملية يكون هناك غالبًا أغلبية من أزواج العناصر التي يتم تعيينها على تسمية ``مختلفة`` في كل من التوقع والحقيقة الأرضية، ما يؤدي إلى نسبة عالية من تسميات الأزواج التي تتفق، والتي تؤدي بعد ذلك إلى نتيجة عالية.

.. rubric:: الأمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`:
  تحليل تأثير حجم مجموعة البيانات على قيمة
  قياسات التجمعات للتعيينات العشوائية.

.. dropdown:: الصياغة الرياضية

  إذا كان C تعيين فئة الحقيقة الأرضية و K التجميع، فلنحدد
  :math:`a` و :math:`b` كالتالي:

  - :math:`a`، عدد أزواج العناصر الموجودة في نفس المجموعة في C وفي نفس المجموعة في K

  - :math:`b`، عدد أزواج العناصر الموجودة في مجموعات مختلفة في C وفي مجموعات مختلفة في K

  يتم إعطاء مؤشر راند غير المعدل بعد ذلك بواسطة:

  .. math:: \text{RI} = \frac{a + b}{C_2^{n_{samples}}}

  حيث :math:`C_2^{n_{samples}}` هو إجمالي عدد الأزواج المحتملة في مجموعة البيانات. لا يهم ما إذا كان الحساب يتم على أزواج مرتبة أو أزواج غير مرتبة طالما أن الحساب يتم بطريقة متسقة.

  ومع ذلك، لا يضمن مؤشر راند أن تعيينات التسمية العشوائية ستحصل على قيمة قريبة من الصفر (خاصةً إذا كان عدد التجمعات في نفس ترتيب حجم عدد العينات).

  لمكافحة هذا التأثير، يمكننا خصم مؤشر راند المتوقع :math:`E[\text{RI}]` للتسميات العشوائية من خلال تحديد مؤشر راند المعدل على النحو التالي:

  .. math:: \text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}

.. dropdown:: المراجع

  * `Comparing Partitions
    <https://link.springer.com/article/10.1007%2FBF01908075>`_ L. Hubert and P.
    Arabie, Journal of Classification 1985

  * `Properties of the Hubert-Arabie adjusted Rand index
    <https://psycnet.apa.org/record/2004-17801-007>`_ D. Steinley, Psychological
    Methods 2004

  * `Wikipedia entry for the Rand index
    <https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index>`_

  * :doi:`Minimum adjusted Rand index for two clusterings of a given size, 2022, J. E. Chacón and A. I. Rastrojo <10.1007/s11634-022-00491-w>`


.. _mutual_info_score:

مقاييس تعتمد على المعلومات المتبادلة

    

مقاييس تعتمد على المعلومات المتبادلة

بالنظر إلى معرفة تعيينات الفئة الحقيقية الأرضية `labels_true` وتعيينات الخوارزمية العنقودية لنفس العينات `labels_pred`، فإن مقياس **الت信息化vocab
  نص بتنسيق RST الذي تريد ترجمته هو دليل استخدام لوظائف مكتبة sklearn الخاصة بقياس توافق التجمعات clustering agreements. سأقوم بترجمة النص إلى اللغة العربية مع الحفاظ على جميع الرموز الخاصة والمعادلات الرياضية والروابط والتاجات والشيفرة البرمجية دون تغيير:

    بالنظر إلى معرفتنا بتعيينات الفئة الحقيقية ``labels_true`` وتعيينات خوارزمية التجميع لدينا لنفس العينات ``labels_pred``، فإن **المعلومات المشتركة Mutual Information** هي دالة تقيس **توافق** التعيينين، مع تجاهل التباديل. تتوفر نسختان مختلفتان من هذا المقياس، **المعلومات المشتركة العادية Normalized Mutual Information (NMI)** و **المعلومات المشتركة المعدلة Adjusted Mutual Information (AMI)** . غالبًا ما تستخدم NMI في الأدبيات، بينما تم اقتراح AMI مؤخرًا ويتم **تطبيعها ضد الصدفة**::

      >>> from sklearn import metrics
      >>> labels_true = [0, 0, 0, 1, 1, 1]
      >>> labels_pred = [0, 0, 1, 1, 2, 2]

      >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
      0.22504...

    يمكن للمرء أن يعكس 0 و 1 في التسميات المتوقعة، ويعيد تسمية 2 إلى 3 والحصول على نفس النتيجة::

      >>> labels_pred = [1, 1, 0, 0, 3, 3]
      >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
      0.22504...

    كل من :func:`mutual_info_score`، :func:`adjusted_mutual_info_score` و :func:`normalized_mutual_info_score` هي مقاييس متناظرة: تبديل الوسيطة لا يغير النتيجة. وبالتالي يمكن استخدامها كمقياس **إجماع**::

      >>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)  # doctest: +SKIP
      0.22504...

    يتم تسجيل التصنيف المثالي بقيمة 1.0::

      >>> labels_pred = labels_true[:]
      >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
      1.0

      >>> metrics.normalized_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
      1.0

    هذا ليس صحيحًا بالنسبة لـ ``mutual_info_score``، والذي يصعب الحكم عليه::

      >>> metrics.mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
      0.69...

    يتم تسجيل النتيجة السيئة (مثل تعيينات التسمية المستقلة) بنتائج غير موجبة::

      >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
      >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
      >>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)  # doctest: +SKIP
      -0.10526...

    .. topic:: المزايا:

      - **تعيينات التسمية العشوائية (موحدة) لها درجة AMI قريبة من 0.0** لأي قيمة من ``n_clusters`` و ``n_samples`` (وهو ليس هو الحال بالنسبة للمعلومات المشتركة الأولية أو مقياس V-measure على سبيل المثال).

      - **الحد الأعلى 1**: تشير القيم القريبة من الصفر إلى تعييني تسمية مستقلين إلى حد كبير، بينما تشير القيم القريبة من واحد إلى توافق كبير. علاوة على ذلك، تشير AMI التي تبلغ بالضبط 1 إلى أن تعييني التسمية متساويان (مع أو بدون تبديل).

    .. topic:: العيوب:

      - على عكس القصور الذاتي، **تتطلب مقاييس MI المعرفة المسبقة لفئات الحقيقة الأرضية** في حين أنها غير متوفرة تقريبًا في الممارسة العملية أو تتطلب تعيينًا يدويًا من قبل معلقين بشريين (كما هو الحال في إعداد التعلم الخاضع للإشراف).

        ومع ذلك، يمكن أن تكون مقاييس MI مفيدة أيضًا في إعدادات غير خاضعة للإشراف كعنصر لبناء مؤشر الإجماع الذي يمكن استخدامه لاختيار نموذج التجميع.

      - لا يتم ضبط NMI و MI ضد الصدفة.

    .. rubric:: الأمثلة

    * :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: تحليل
      تأثير حجم مجموعة البيانات على قيمة مقاييس التجميع لتعيينات عشوائية. يتضمن هذا المثال أيضًا Adjusted Rand Index.

    .. dropdown:: الصياغة الرياضية

      افترض وجود تعييني تسمية (لنفس كائنات N)، :math:`U` و :math:`V`.
      الانتروبيا الخاصة بها هي مقدار عدم اليقين لمجموعة التقسيم، المحددة بواسطة:

      .. math:: H(U) = - \sum_{i=1}^{|U|}P(i)\log(P(i))

      حيث :math:`P(i) = |U_i| / N` هو احتمال أن يسقط كائن تم اختياره عشوائيًا من :math:`U` في الفئة :math:`U_i`. وبالمثل لـ :math:`V`:

      .. math:: H(V) = - \sum_{j=1}^{|V|}P'(j)\log(P'(j))

      مع :math:`P'(j) = |V_j| / N`. يتم حساب المعلومات المشتركة (MI) بين :math:`U`
      و :math:`V` بواسطة:

      .. math:: \text{MI}(U, V) = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}P(i, j)\log\left(\frac{P(i,j)}{P(i)P'(j)}\right)

      حيث :math:`P(i, j) = |U_i \cap V_j| / N` هو احتمال أن يسقط كائن تم اختياره عشوائيًا في كل من الفئتين :math:`U_i` و :math:`V_j`.

      يمكن التعبير عنها أيضًا في صياغة عدد العناصر في المجموعة:

      .. math:: \text{MI}(U, V) = \sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \frac{|U_i \cap V_j|}{N}\log\left(\frac{N|U_i \cap V_j|}{|U_i||V_j|}\right)

      يتم تعريف المعلومات المشتركة العادية على أنها

      .. math:: \text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\text{mean}(H(U), H(V))}

      هذه القيمة للمعلومات المشتركة وكذلك المتغير العادي غير معدلة للصدفة وستميل إلى الزيادة مع زيادة عدد التسميات المختلفة (التجمعات)، بغض النظر عن مقدار "المعلومات المشتركة" الفعلية بين تعييني التسمية.

      يمكن حساب القيمة المتوقعة للمعلومات المشتركة باستخدام المعادلة التالية [VEB2009]_. في هذه المعادلة، :math:`a_i = |U_i|` (عدد العناصر في :math:`U_i`) و :math:`b_j = |V_j|` (عدد العناصر في :math:`V_j`).

      .. math:: E[\text{MI}(U,V)]=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \sum_{n_{ij}=(a_i+b_j-N)^+
        }^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
        \frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
        (N-a_i-b_j+n_{ij})!}

      باستخدام القيمة المتوقعة، يمكن بعد ذلك حساب المعلومات المشتركة المعدلة باستخدام نموذج مشابه لمؤ
Vinh وآخرون (2010) قاموا بتسمية متغيرات NMI و AMI according to their averaging method [VEB2010]_. Their 'sqrt' and 'sum' averages are the geometric and arithmetic means; نحن نستخدم هذه الأسماء الأكثر شيوعًا.

.. rubric:: المراجع

* Strehl, Alexander, and Joydeep Ghosh (2002). "Cluster ensembles - a knowledge reuse framework for combining multiple partitions". Journal of Machine Learning Research 3: 583-617. `doi:10.1162/153244303321897735 <http://strehl.com/download/strehl-jmlr02.pdf>`_.

* `Wikipedia entry for the (normalized) Mutual Information <https://en.wikipedia.org/wiki/Mutual_Information>`_

* `Wikipedia entry for the Adjusted Mutual Information <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_

.. [VEB2009] Vinh, Epps, and Bailey, (2009). "Information theoretic measures for clusterings comparison". Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09. `doi:10.1145/1553374.1553511 <https://dl.acm.org/citation.cfm?doid=1553374.1553511>`_. ISBN 9781605585161.

.. [VEB2010] Vinh, Epps, and Bailey, (2010). "Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance". JMLR <https://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>

.. [YAT2016] Yang, Algesheimer, and Tessone, (2016). "A comparative analysis of community detection algorithms on artificial networks". Scientific Reports 6: 30750. `doi:10.1038/srep30750 <https://www.nature.com/articles/srep30750>`_.


.. _homogeneity_completeness:

Homogeneity, completeness and V-measure


(ملاحظة: لم يتم ترجمة النصوص داخل الروابط والمراجع والرموز الخاصة والمعادلات الرياضية والتاجات والشفرة البرمجية كما هو مطلوب في السؤال.)

بالنظر إلى معرفة تعيينات الفئة الحقيقية للعينة ، من الممكن تحديد بعض المقاييس البديهية باستخدام تحليل الانتروبيا الشرطي.

على وجه الخصوص ، يحدد Rosenberg و Hirschberg (2007) الهدفين التاليين المرغوب فيهما لأي مهمة تجميع:

- **التجانس**: يحتوي كل مجموعة على أعضاء من فئة واحدة فقط.

- **الاكتمال**: يتم تعيين جميع أعضاء فئة معينة إلى نفس المجموعة.

يمكن تحويل هذه المفاهيم إلى درجات: `homogeneity_score` و `completeness_score`. كلاهما محصوران من أسفل بقيمة 0.0 ومن أعلى بقيمة 1.0 (كلما ارتفعت القيمة كانت أفضل)::

  >>> from sklearn import metrics
  >>> labels_true = [0, 0, 0, 1, 1, 1]
  >>> labels_pred = [0, 0, 1, 1, 2, 2]

  >>> metrics.homogeneity_score(labels_true, labels_pred)
  0.66...

  >>> metrics.completeness_score(labels_true, labels_pred)
  0.42...

يتم حساب المتوسط التوافقي المسمى **V-measure** بواسطة `v_measure_score`::

  >>> metrics.v_measure_score(labels_true, labels_pred)
  0.51...

صيغة هذه الدالة هي كالتالي:

.. math:: v = \frac{(1 + \beta) \times \text{homogeneity} \times \text{completeness}}{(\beta \times \text{homogeneity} + \text{completeness})}

تأخذ `beta` بشكل افتراضي قيمة 1.0 ، ولكن لاستخدام قيمة أقل من 1 لـ beta::

  >>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6)
  0.54...

سيتم إعطاء وزن أكبر للتجانس ، واستخدام قيمة أكبر من 1::

  >>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)
  0.48...

سيتم إعطاء وزن أكبر للاكتمال.

في الواقع ، فإن مقياس V-measure يعادل المعلومات المتبادلة (NMI) المذكورة أعلاه ، مع كون دالة التجميع هي الوسط الحسابي [B2011]_.

يمكن حساب التجانس والاكتمال و V-measure في وقت واحد باستخدام `homogeneity_completeness_v_measure` على النحو التالي::

  >>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
  (0.66..., 0.42..., 0.51...)

تعيين التجميع التالي أفضل قليلاً ، لأنه متجانس ولكن غير كامل::

  >>> labels_pred = [0, 0, 0, 1, 2, 2]
  >>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
  (1.0, 0.68..., 0.81...)

.. note::

  دالة `v_measure_score` هي **متناظرة**: يمكن استخدامها لتقييم **الاتفاق** بين تعيينين مستقلين على نفس مجموعة البيانات.

  هذا ليس هو الحال بالنسبة لـ `completeness_score` و `homogeneity_score`: كلاهما مقيدان بالعلاقة::

    homogeneity_score(a, b) == completeness_score(b, a)


.. topic:: المزايا:

  - **درجات محصورة**: 0.0 هي أسوأ قيمة ممكنة ، و 1.0 هي درجة مثالية.

  - التفسير البديهي: يمكن تحليل التجميع ذي درجة V-measure سيئة **نوعيًا من حيث التجانس والاكتمال** لإحساس أفضل بنوع الأخطاء التي يتم ارتكابها في التعيين.

  - **لا يتم افتراض أي افتراض حول بنية التجميع**: يمكن استخدامه لمقارنة خوارزميات التجميع مثل k-means التي تفترض أشكالًا متساوية blob مع نتائج خوارزميات التجميع الطيفي التي يمكنها العثور على تجمعات بأشكال "مطوية".

.. topic:: السلبيات:

  - المقاييس التي تم تقديمها سابقًا **غير موحدة فيما يتعلق بالتسمية العشوائية**: هذا يعني أنه اعتمادًا على عدد العينات ، والتجمعات والفئات الحقيقية ، فإن التسمية العشوائية تمامًا لن تسفر دائمًا عن نفس القيم للتجانس والاكتمال وبالتالي مقياس v. على وجه الخصوص ، **لن تسفر التسمية العشوائية عن درجات صفرية خاصة عندما يكون عدد التجمعات كبيرًا**.

    يمكن تجاهل هذه المشكلة بأمان عندما يكون عدد العينات أكثر من ألف وعدد التجمعات أقل من 10. **بالنسبة لأحجام العينات الأصغر أو عدد أكبر من التجمعات ، من الأسلم استخدام فهرس معدل مثل Adjusted Rand Index (ARI)**.

  .. figure:: ../auto_examples/cluster/images/sphx_glr_plot_adjusted_for_chance_measures_001.png
    :target: ../auto_examples/cluster/plot_adjusted_for_chance_measures.html
    :align: center
    :scale: 100

  - تتطلب هذه المقاييس **معرفة الفئات الحقيقية** بينما نادرًا ما تكون متاحة في الممارسة أو تتطلب تعيينًا يدويًا من قبل المحللين البشريين (كما هو الحال في إعداد التعلم الخاضع للإشراف).

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: تحليل
  تأثير حجم مجموعة البيانات على قيمة مقاييس التجميع للتعيينات العشوائية.

.. dropdown:: الصيغة الرياضية

  يتم إعطاء درجات التجانس والاكتمال رسميًا بواسطة:

  .. math:: h = 1 - \frac{H(C|K)}{H(C)}

  .. math:: c = 1 - \frac{H(K|C)}{H(K)}

 حيث :math:`H(C|K)` هو **الانتروبيا الشرطي للفئات نظرًا لتعيينات التجميع** ويعطى بواسطة:

  .. math:: H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{n_{c,k}}{n}
            \cdot \log\left(\frac{n_{c,k}}{n_k}\right)

  و :math:`H(C)` هو **انتروبيا الفئات** ويعطى بواسطة:

  .. math:: H(C) = - \sum_{c=1}^{|C|} \frac{n_c}{n} \cdot \log\left(\frac{n_c}{n}\right)

  مع :math:`n` إجمالي عدد العينات ، :math:`n_c` و :math:`n_k` عدد العينات التي تنتمي على التوالي إلى الفئة :math:`c` والتجمع :math:`k`، وأخيراً :math:`n_{c،k}` عدد العينات من الفئة :math:`c` المعينة للتجمع :math:`k`.

  يتم تعريف **الانتروبيا الشرطي للتجمعات نظرًا للفئة** :math:`H(K|C)` و **انتروبيا التجمعات** :math:`H(K)` بطريقة متناظرة.

  يعرّف Rosenberg و Hirschberg المزيد من **مقياس V** على أنه **متوسط تنافسي للتجانس والاكتمال**:

  .. math:: v = 2 \cdot \frac{h \cdot c}{h + c}

.. rubric:: المراجع

* `V-Measure: A conditional entropy-based external cluster evaluation measure
  <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_ Andrew Rosenberg and Julia
  Hirschberg, 2007

.. [B2011] `Identification and Characterization of Events in Social Media
  <http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf>`_, Hila
  Becker, PhD Thesis.


.. _fowlkes_mallows_scores:

درجات Fowlkes-Mallows

    

درجات Fowlkes-Mallows

    

يمكن استخدام مؤشر Fowlkes-Mallows (:func:`sklearn.metrics.fowlkes_mallows_score`) عندما تكون تعيينات الفئة الحقيقية للعينات معروفة. يتم تعريف مؤشر Fowlkes-Mallows FMI كمتوسط هندسي للدقة والتذكر الزوجي:

.. math:: \text{FMI} = \frac{\text{TP}}{\sqrt{(\text{TP} + \text{FP}) (\text{TP} + \text{FN})}}

حيث ``TP`` هو عدد **الإيجابيات الحقيقية** (أي عدد أزواج النقاط التي تنتمي إلى نفس العناقيد في كل من التسميات الحقيقية والتسميات المتوقعة)، و``FP`` هو عدد **الإيجابيات الكاذبة** (أي عدد أزواج النقاط التي تنتمي إلى نفس العناقيد في التسميات الحقيقية وليس في التسميات المتوقعة) و``FN`` هو عدد **السلبيات الكاذبة** (أي عدد أزواج النقاط التي تنتمي إلى نفس العناقيد في التسميات المتوقعة وليس في التسميات الحقيقية).

يتراوح الدرجات من 0 إلى 1. تشير القيمة العالية إلى التشابه الجيد بين عنقودين.

  >>> from sklearn import metrics
  >>> labels_true = [0, 0, 0, 1, 1, 1]
  >>> labels_pred = [0, 0, 1, 1, 2, 2]

  >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
  0.47140...

يمكن للمرء أن يقوم بتبديل 0 و 1 في التسميات المتوقعة، إعادة تسمية 2 إلى 3 والحصول

...

على نفس الدرجة::

  >>> labels_pred = [1, 1, 0, 0, 3, 3]

  >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
  0.47140...

يتم تسجيل التسمية المثالية 1.0::

  >>> labels_pred = labels_true[:]
  >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
  1.0

لدى السيئة (على سبيل المثال تسمية مستقلة) درجات الصفر::

  >>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
  >>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
  >>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
  0.0

.. topic:: المزايا:

  - **تعيينات الملصقات العشوائية (الموحدة) لها درجة FMI قريبة من 0.0** لأي قيمة من ``n_clusters`` و ``n_samples`` (وهو ليس الحال بالنسبة لمعلومات الترابط الخام أو مقياس V على سبيل المثال).

  - **محددة في الأعلى عند 1**: تشير القيم القريبة من الصفر إلى تعيينين للعلامات مستقلين إلى حد كبير، بينما تشير القيم القريبة من واحد إلى اتفاق كبير. علاوة على ذلك، تشير قيم الصفر تمامًا إلى تعيينات **خالصة** للعلامات المستقلة و FMI بالضبط من 1 يشير إلى أن تعييني الملصقين متساويان (مع أو بدون تبديل).

  - **لا يتم إجراء أي افتراض بشأن بنية العنقود**: يمكن استخدامه لمقارنة خوارزميات التجميع مثل k-means التي تفترض أشكالًا متساوية الكثافة مع نتائج خوارزميات التجميع الطيفية التي يمكنها العثور على مجموعة مع أشكال "مطوية".

.. topic:: العيوب:

  - على عكس القصور الذاتي، **تتطلب التدابير المستندة إلى FMI معرفة فئات الحقيقة الأساسية** في حين أنها غير متوفرة عمليًا أو تتطلب تعيينًا يدويًا من خلال وضع علامة من قبل البشر (كما هو الحال في إعداد التعلم الخاضع للإشراف).

.. dropdown:: المراجع

  * E. B. Fowkles و C. L. Mallows، 1983. "طريقة لمقارنة مجموعتين هرميتين". مجلة الجمعية الأمريكية للإحصاء. https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008

  * `Wikipedia entry for the Fowlkes-Mallows Index <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_


.. _silhouette_coefficient:

معامل الظل
----------

إذا لم تكن تسميات الحقيقة الأساسية معروفة، فيجب إجراء التقييم باستخدام النموذج نفسه. معامل الظل (:func:`sklearn.metrics.silhouette_score`) هو مثال على هذا التقييم، حيث ترتبط درجة معامل الظل الأعلى بنموذج به تجمعات محددة بشكل أفضل. يتم تحديد معامل الظل لكل عينة ويتكون من درجتين:

- **a**: متوسط المسافة بين العينة وجميع النقاط الأخرى في نفس الفئة.

- **b**: متوسط المسافة بين العينة وجميع النقاط الأخرى في المجموعة *التالية الأقرب*.

يتم إعطاء معامل الظل *s* لعينة واحدة على النحو التالي:

.. math:: s = \frac{b - a}{max(a, b)}

يتم إعطاء معامل الظل لمجموعة من العينات كمتوسط معامل الظل لكل عينة.


  >>> from sklearn import metrics
  >>> from sklearn.metrics import pairwise_distances
  >>> from sklearn import datasets
  >>> X, y = datasets.load_iris(return_X_y=True)

في الاستخدام العادي، يتم تطبيق معامل الظل على نتائج تحليل التجميع.

  >>> import numpy as np
  >>> from sklearn.cluster import KMeans
  >>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
  >>> labels = kmeans_model.labels_
  >>> metrics.silhouette_score(X, labels, metric='euclidean')
  0.55...

.. topic:: المزايا:

  - الدرجة محدودة بين -1 للتجميع غير الصحيح و +1 للتجميع عالي الكثافة. تشير الدرجات حول الصفر إلى تجمعات متداخلة.

  - تكون الدرجة أعلى عندما تكون التجمعات كثيفة ومفصولة جيدًا، مما يتعلق بمفهوم قياسي للمجموعة.

.. topic:: العيوب:

  - معامل الظل أعلى بشكل عام للتجمعات المحدبة من المفاهيم الأخرى للتجمعات، مثل التجمعات القائمة على الكثافة مثل تلك التي تم الحصول عليها من خلال DBSCAN.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py` : في هذا المثال، يتم استخدام تحليل الظل لاختيار قيمة مثالية لـ n_clusters.

.. dropdown:: المراجع

  * Peter J. Rousseeuw (1987). :doi:`"Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis"<10.1016/0377-0427(87)90125-7>`. Computational and Applied Mathematics 20: 53-65.


.. _calinski_harabasz_index:

معامل كالينسكي هاراباس

    

...

(ينتهي الترجمة هنا لأن النص الأصلي الذي قدمته لم يتضمن القسم الأخير بشأن معامل كالينسكي هاراباس، والذي يبدأ بهذه العلامة).

إذا لم تكن تسميات الحقيقة الأرضية معروفة، فيمكن استخدام مؤشر Calinski-Harabasz (:func:`sklearn.metrics.calinski_harabasz_score`) - المعروف أيضًا باسم معيار نسبة التباين - لتقييم النموذج، حيث يرتبط ارتفاع قيمة Calinski-Harabasz بنموذج يحتوي على مجموعات محددة بشكل أفضل.

 المؤشر هو نسبة مجموع التشتت بين المجموعات وتشتت داخل المجموعة لجميع المجموعات (حيث يتم تعريف التشتت على أنه مجموع المسافات المربعة):

  >>> from sklearn import metrics
  >>> from sklearn.metrics import pairwise_distances
  >>> from sklearn import datasets
  >>> X, y = datasets.load_iris(return_X_y=True)

 في الاستخدام العادي، يتم تطبيق مؤشر Calinski-Harabasz على نتائج تحليل المجموعة:

  >>> import numpy as np
  >>> from sklearn.cluster import KMeans
  >>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
  >>> labels = kmeans_model.labels_
  >>> metrics.calinski_harabasz_score(X, labels)
  561.59...


 .. topic:: المزايا:

  - تكون النتيجة أعلى عندما تكون المجموعات كثيفة ومتفصلة بشكل جيد، مما يتعلق بمفهوم قياسي للمجموعة.

  - الحساب سريع.

 .. topic:: العيوب:

  - يكون مؤشر Calinski-Harabasz بشكل عام أعلى للمجموعات المحدبة من المفاهيم الأخرى للمجموعات، مثل المجموعات القائمة على الكثافة مثل تلك التي يتم الحصول عليها من خلال DBSCAN.

 .. dropdown:: الصياغة الرياضية

  لمجموعة بيانات :math:`E` بحجم :math:`n_E` التي تم تجميعها في :math:`k` مجموعات، يتم تعريف درجة Calinski-Harabasz :math:`s` على أنها نسبة متوسط التشتت بين المجموعات ومتوسط التشتت داخل المجموعة:

  .. math::
    s = \frac{\mathrm{tr}(B_k)}{\mathrm{tr}(W_k)} \times \frac{n_E - k}{k - 1}

  حيث :math:`\mathrm{tr}(B_k)` هي أثر مصفوفة تشتت المجموعة بين المجموعة و:math:`\mathrm{tr}(W_k)` هي أثر مصفوفة تشتت المجموعة داخل المجموعة المحددة بواسطة:

  .. math:: W_k = \sum_{q=1}^k \sum_{x \in C_q} (x - c_q) (x - c_q)^T

  .. math:: B_k = \sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T

  مع :math:`C_q` مجموعة من النقاط في المجموعة :math:`q`، :math:`c_q` مركز المجموعة :math:`q`، :math:`c_E` مركز :math:`E`، و:math:`n_q` عدد النقاط في المجموعة :math:`q`.

 .. dropdown:: المراجع

  * Caliński، T.، & Harabasz، J. (1974). `"طريقة Dendrite لتحليل المجموعة" <https://www.researchgate.net/publication/233096619_A_Dendrite_Method_for_Cluster_Analysis>`_. :doi:`Communications in Statistics-theory and Methods 3: 1-27 <10.1080/03610927408827101>`.


 .. _davies-bouldin_index:

مؤشر Davies-Bouldin
--------------------

إذا لم تكن تسميات الحقيقة الأرضية معروفة، فيمكن استخدام مؤشر Davies-Bouldin (:func:`sklearn.metrics.davies_bouldin_score`) لتقييم النموذج، حيث يرتبط انخفاض مؤشر Davies-Bouldin بنموذج به انفصال أفضل بين المجموعات.

يشير هذا المؤشر إلى متوسط "التشابه" بين المجموعات، حيث التشابه هو مقياس يقارن المسافة بين المجموعات بحجم المجموعات نفسها.

النتيجة الصفرية هي أدنى نتيجة ممكنة. تشير القيم الأقرب إلى الصفر إلى قسم أفضل.

في الاستخدام العادي، يتم تطبيق مؤشر Davies-Bouldin على نتائج تحليل المجموعة على النحو التالي:

  >>> from sklearn import datasets
  >>> iris = datasets.load_iris()
  >>> X = iris.data
  >>> from sklearn.cluster import KMeans
  >>> from sklearn.metrics import davies_bouldin_score
  >>> kmeans = KMeans(n_clusters=3, random_state=1).fit(X)
  >>> labels = kmeans.labels_
  >>> davies_bouldin_score(X, labels)
  0.666...


 .. topic:: المزايا:

  - حساب Davies-Bouldin أبسط من حساب درجات Silhouette.
  - يعتمد المؤشر فقط على الكميات والميزات المتأصلة في مجموعة البيانات حيث يستخدم حسابه مسافات نقطية فقط.

 .. topic:: العيوب:

  - يكون مؤشر Davies-Bouldin بشكل عام أعلى للمجموعات المحدبة من المفاهيم الأخرى للمجموعات، مثل المجموعات القائمة على الكثافة مثل تلك التي يتم الحصول عليها من DBSCAN.
  - يحد استخدام مسافة المركز من مقياس المسافة إلى فضاء إقليدي.

 .. dropdown:: الصياغة الرياضية

  يتم تعريف المؤشر على أنه متوسط التشابه بين كل مجموعة :math:`C_i` لـ :math:`i=1، ..., k` والأكثر تشابهًا :math:`C_j`. في سياق هذا المؤشر، يتم تعريف التشابه على أنه مقياس :math:`R_{ij}` الذي يوازن بين:

  - :math:`s_i`، متوسط المسافة بين كل نقطة من المجموعة :math:`i` ومركز تلك المجموعة - المعروف أيضًا باسم قطر المجموعة.
  - :math:`d_{ij}`، المسافة بين مراكز المجموعة :math:`i` و:math:`j`.

  اختيار بسيط لبناء :math:`R_{ij}` بحيث يكون غير سلبي ومتناسق هو:

  .. math::
    R_{ij} = \frac{s_i + s_j}{d_{ij}}

  ثم يتم تعريف مؤشر Davies-Bouldin على أنه:

  .. math::
    DB = \frac{1}{k} \sum_{i=1}^k \max_{i \neq j} R_{ij}

 .. dropdown:: المراجع

  * Davies، David L.؛ Bouldin، Donald W. (1979). :doi:`"مقياس فصل المجموعة" <10.1109/TPAMI.1৯৭९.৪৭৬৬৯০৯>` IEEE Transactions on Pattern Analysis and Machine Intelligence. PAMI-1 (2): 224-227.

  * Halkidi، Maria؛ Batistakis، Yannis؛ Vazirgiannis، Michalis (2001). :doi:`"على تقنيات التحقق من صحة التجميع" <10.1023/A:1012801612483>` Journal of Intelligent Information Systems، 17(2-3)، 107-145.

  * `إدخال ويكيبيديا لمؤشر Davies-Bouldin <https://en.wikipedia.org/wiki/Davies-Bouldin_index>`_.


 .. _contingency_matrix:

مصفوفة الطوارئ
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
  هذه نص بترميز RST أريد ترجمته إلى اللغة العربية، مع الاحتفاظ بنفس التنسيق:

------------------

مصفوفة الاحتمال (:func:`sklearn.metrics.cluster.contingency_matrix`)
تعرض عدد حالات التقاطع لكل زوج من التجمعات الحقيقية والمتوقعة.
توفر مصفوفة الاحتمال إحصاءات كافية لجميع مقاييس التجمعات حيث تكون العينات مستقلة ومتطابقة التوزيع،
ولا يحتاج المرء إلى حساب بعض الحالات التي لم يتم تجميعها.

فيما يلي مثال::

   >>> from sklearn.metrics.cluster import contingency_matrix
   >>> x = ["a", "a", "a", "b", "b", "b"]
   >>> y = [0, 0, 1, 1, 2, 2]
   >>> contingency_matrix(x, y)
   array([[2, 1, 0],
          [0, 1, 2]])

يشير الصف الأول من مصفوفة الإخراج إلى أن هناك ثلاث عينات يكون تجمعها الحقيقي هو "a".
منهم، اثنتان في التجمع المتوقع 0، وواحدة في 1، ولا شيء في 2.
والصف الثاني يشير إلى أن هناك ثلاث عينات يكون تجمعها الحقيقي هو "b".
منهم، لا شيء في التجمع المتوقع 0، وواحدة في 1 واثنتان في 2.

أ :ref:`مصفوفة الارتباك <confusion_matrix>` للتصنيف هي مصفوفة احتمالية مربعة
حيث يتوافق ترتيب الصفوف والأعمدة مع قائمة الفئات.

.. topic:: الميزات:

  - تسمح بفحص الانتشار لكل تجمع حقيقي عبر التجمعات المتوقعة والعكس صحيح.

  - عادةً ما يتم استخدام جدول الاحتمالات المحسوب في حساب إحصائية التشابه (مثل الأخرى المدرجة في هذه الوثيقة) بين التجمعات.

.. topic:: العيوب:

  - من السهل تفسير مصفوفة الاحتمال لعدد صغير من التجمعات، ولكن يصبح من الصعب جدًا تفسيرها لعدد كبير من التجمعات.

  - لا تعطي مقياسًا واحدًا لاستخدامه كهدف للتجميع الأمثل.

.. dropdown:: المراجع

  * `قائمة ويكيبيديا لمصفوفة الاحتمال <https://en.wikipedia.org/wiki/Contingency_table>`_


.. _pair_confusion_matrix:

مصفوفة الارتباك للزوج
---------------------

مصفوفة الارتباك للزوج (:func:`sklearn.metrics.cluster.pair_confusion_matrix`)
هي مصفوفة تشابه 2x2

.. math::
   C = \left[\begin{matrix}
   C_{00} & C_{01} \\
   C_{10} & C_{11}
   \end{matrix}\right]

بين تجميعين يتم حسابهما عن طريق النظر في جميع الأزواج من العينات
وعد الأزواج التي يتم تعيينها في نفس التجمعات أو في تجمعات مختلفة
تحت تجميعات صحيحة ومتوقعة.

يحتوي على الإدخالات التالية:

:math:`C_{00}` : عدد الأزواج التي يكون فيها كلا التجمعان لهما العينات غير مجمعة معًا

:math:`C_{10}` : عدد الأزواج التي يكون فيها تجميع التسمية الحقيقية له العينات مجمعة معًا ولكن التجميع الآخر لا يحتوي على العينات مجمعة معًا

:math:`C_{01}` : عدد الأزواج التي يكون فيها تجميع التسمية الحقيقية لا يحتوي على العينات مجمعة معًا ولكن التجميع الآخر يحتوي على العينات مجمعة معًا

:math:`C_{11}` : عدد الأزواج التي يكون فيها كلا التجمعين لهما العينات مجمعة معًا

مع الأخذ في الاعتبار زوج العينات الذي يتم تجميعه معًا كزوج إيجابي،
ثم كما هو الحال في التصنيف الثنائي، يكون عدد السلبيات الحقيقية هو
:math:`C_{00}`, السلبيات الخاطئة هي :math:`C_{10}`, الإيجابيات الحقيقية هي
:math:`C_{11}` والإيجابيات الخاطئة هي :math:`C_{01}`.

تطابق التسميات المثالية له جميع الإدخالات غير الصفرية على
القطر بغض النظر عن قيم التسمية الفعلية::

   >>> from sklearn.metrics.cluster import pair_confusion_matrix
   >>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])
   array([[8, 0],
          [0, 4]])

::

   >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])
   array([[8, 0],
          [0, 4]])

التسميات التي تعين جميع أعضاء الفئات إلى نفس التجمعات
تكون كاملة ولكن قد لا تكون دائمًا نقية، وبالتالي يتم معاقبتها،
ولها بعض الإدخالات غير الصفرية خارج القطر::

   >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])
   array([[8, 2],
          [0, 2]])

المصفوفة غير متماثلة::

   >>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])
   array([[8, 0],
          [2, 2]])

إذا تم فصل أعضاء الفئات تمامًا عبر تجمعات مختلفة، فإن
التعيين غير مكتمل تمامًا، وبالتالي فإن المصفوفة تحتوي على جميع الصفر
إدخالات قطرية::

   >>> pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])
   array([[ 0,  0],
          [12,  0]])

.. dropdown:: المراجع

  * :doi:`"Comparing Partitions" <10.1007/BF01908075>` L. Hubert and P. Arabie,
    Journal of Classification 1985
    
    
  [Done]
    
    
  

















































































































































































































































































































































































































































































































































































