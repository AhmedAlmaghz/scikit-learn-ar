
.. _ensemble:

===========================================================================
الطرق التركيبية: التدرج المعزز، الغابات العشوائية، الطرح، التصويت، التكديس
===========================================================================

.. currentmodule:: sklearn.ensemble

**الطرق التركيبية** تجمع توقعات العديد من مقدرات الأساس المبنية باستخدام خوارزمية تعلم معينة لتحسين التعميم/المتانة مقارنة بمقدر واحد.

اثنان من الأمثلة الشهيرة للطرق التركيبية هما :ref:`التدرج المعزز للأشجار <gradient_boosting>` و :ref:`الغابات العشوائية <forest>`.

بشكل عام، يمكن تطبيق النماذج التركيبية على أي متعلم أساس يتجاوز الأشجار، في طرق المتوسط مثل :ref:`طرق الطرح <bagging>`, :ref:`تكديس النموذج <stacking>`, أو :ref:`التصويت <voting_classifier>`, أو في التعزيز، مثل :ref:`AdaBoost <adaboost>`.

.. _gradient_boosting:

التدرج المعزز للأشجار
======================

`Gradient Tree Boosting <https://en.wikipedia.org/wiki/Gradient_boosting>`_
أو Gradient Boosted Decision Trees (GBDT) هو تعميم
لتقنية التعزيز (boosting) لتشمل دالات خسارة قابلة للاختلاف، انظر العمل الرائد لـ
[Friedman2001]_. يعتبر GBDT نموذجًا ممتازًا لكل من الانحدار والتصنيف، وخاصة للبيانات الجدولية.

.. topic:: :class:`GradientBoostingClassifier` مقابل :class:`HistGradientBoostingClassifier`

  يوفر Scikit-learn تنفيذين لشجرة التدرج المعزز:
  :class:`HistGradientBoostingClassifier` مقابل
  :class:`GradientBoostingClassifier` للتصنيف، والفئات المقابلة للانحدار. يمكن أن يكون الأول **أسرع بعدة مرات** من الأخير عندما يكون عدد العينات أكبر من عشرات الآلاف من العينات.

  يتم دعم القيم المفقودة والبيانات التصنيفية بشكل أصلي بواسطة إصدار Hist ...، مما يلغي الحاجة إلى معالجة مسبقة إضافية مثل الإسناد.

  قد يفضل استخدام :class:`GradientBoostingClassifier` و
  :class:`GradientBoostingRegressor` لأحجام العينات الصغيرة نظرًا لأن التجميع في صناديق قد يؤدي إلى نقاط تقسيم تقريبية للغاية في هذا الإعداد.

.. _histogram_based_gradient_boosting:

تدرج التدرج المستند إلى التدرج

   
    
    (ملحوظة: ترجمت " Histogram-Based Gradient Boosting" إلى "تدرج التدرج المستند إلى التدرج" بشكل حرفي، ولكن يمكن أيضًا ترجمتها إلى "التعزيز المتدرج المستند إلى التدرج" أو "التعزيز المتدرج المستند إلى التدرج اللوني" حسب السياق).

---

قدّمت Scikit-learn 0.21 تنفيذين جديدين لشجرة التدرج المعزّزة، وهما: :class:`HistGradientBoostingClassifier` و:class:`HistGradientBoostingRegressor`، والمستوحاة من LightGBM (انظر [LightGBM]_).

يمكن أن يكون هذان المقدّران المستندان إلى الرسم البياني **أسرع بمرتبةٍ من الحجم** من :class:`GradientBoostingClassifier` و:class:`GradientBoostingRegressor` عندما يكون عدد العينات أكبر من عشرات الآلاف من العينات.

كما أنها تدعم القيم المفقودة بشكلٍ مدمج، مما يتجنّب الحاجة إلى استخدام أداة استبدال القيم المفقودة.

تعمل هذه المقدّرات السريعة أولاً على تجميع عينات الدخل "X" في خانات ذات قيم صحيحة (تكون عادةً 256 خانة) مما يقلّل بشكلٍ هائل عدد نقاط الفصل الواجب اعتبارها، ويسمح للخوارزمية بالاعتماد على البُنى المعتمدة على الأعداد الصحيحة (الرسومات البيانية) بدلاً من الاعتماد على القيم المستمرة المصنّفة عند بناء الأشجار. يختلف نمط البرمجة الخاص بهذه المقدّرات قليلاً، وبعض الميزات من :class:`GradientBoostingClassifier` و:class:`GradientBoostingRegressor` غير مدعومة بعد، على سبيل المثال بعض دالات الخسارة.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_inspection_plot_partial_dependence.py`
* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`

الاستخدام
^^^^^

معظم البارامترات لا تزال دون تغيير من :class:`GradientBoostingClassifier` و:class:`GradientBoostingRegressor`. أحد الاستثناءات هو بارامتر ``max_iter`` الذي يحل محل ``n_estimators``، ويتحكم بعدد التكرارات لعملية التعزيز::

  >>> من sklearn.ensemble استورد HistGradientBoostingClassifier
  >>> من sklearn.datasets استورد make_hastie_10_2

  >>> X, y = make_hastie_10_2(random_state=0)
  >>> X_train, X_test = X[:2000], X[2000:]
  >>> y_train, y_test = y[:2000], y[2000:]

  >>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)
  >>> clf.score(X_test, y_test)
  0.8965

دالات الخسارة المتاحة ل**الانحدار** هي:

- 'squared_error'، وهي دالة الخسارة الافتراضية؛
- 'absolute_error'، وهي أقل حساسية للقيم المتطرفة من الخطأ التربيعي؛
- 'gamma'، وهي مناسبة للنمذجة النتائج الإيجابية بشكلٍ حاسم؛
- 'poisson'، وهي مناسبة لنمذجة العدد والتواترات؛
- 'quantile'، والتي تسمح بتقدير الشروط الشرطية التي يمكن استخدامها لاحقًا للحصول على فترات التنبؤ.

بالنسبة ل**التصنيف**، 'log_loss' هو الخيار الوحيد. بالنسبة للتصنيف الثنائي، فإنه يستخدم خسارة السجل الثنائي، والمعروف أيضًا باسم الانحراف الثنائي أو الانتروبيا المتقاطعة الثنائية. لـ `n_classes >= 3`، فإنه يستخدم دالة خسارة السجل متعدد الفئات، مع الانحراف متعدد الحدود والانتروبيا المتقاطعة الفئوية كأسماء بديلة. يتم اختيار نسخة دالة الخسارة المناسبة اعتمادًا على :term:`y` الذي تم تمريره إلى :term:`fit`.

يمكن التحكم بحجم الأشجار من خلال بارامترات ``max_leaf_nodes``، و``max_depth``، و``min_samples_leaf``.

يتم التحكم بعدد الخانات المستخدمة لتجميع البيانات بواسطة بارامتر ``max_bins``. استخدام عدد أقل من الخانات يعمل كشكل من أشكال التنظير. من الموصى به بشكلٍ عام استخدام أكبر عدد ممكن من الخانات (255)، وهو الافتراضي.

يعمل بارامتر ``l2_regularization`` كمنظّم لدالة الخسارة، والذي يقابل :math:`\lambda` في التعبير التالي (انظر المعادلة (2) في [XGBoost]_):

.. math::

    \mathcal{L}(\phi) =  \sum_i l(\hat{y}_i, y_i) + \frac12 \sum_k \lambda ||w_k||^2

.. dropdown:: تفاصيل حول التنظيم l2

  من المهم ملاحظة أن مصطلح الخسارة :math:`l(\hat{y}_i, y_i)` يصف فقط نصف دالة الخسارة الفعلية باستثناء خسارة الكرة والدبابيس والخطأ المطلق.

  يشير الفهرس :math:`k` إلى الشجرة k-th في مجموعة الأشجار. في حالة الانحدار والتصنيف الثنائي، نماذج التعزيز المتدرج تنمو شجرة واحدة لكل تكرار، ثم :math:`k` يمتد حتى `max_iter`. في حالة مشاكل التصنيف متعدد الفئات، القيمة القصوى للمؤشر :math:`k` هي `n_classes` :math:`\times` `max_iter`.

  إذا :math:`T_k` يدل على عدد الأوراق في الشجرة k-th، ثم :math:`w_k` عبارة عن متجه طوله :math:`T_k`، والذي يحتوي على قيم الأوراق من النموذج `w = -sum_gradient / (sum_hessian + l2_regularization)` (انظر المعادلة (5) في [XGBoost]_).

  قيم أوراق الشجر :math:`w_k` مشتقة من قسمة مجموع تدرجات دالة الخسارة على مجموع الهيسيات المجمعة. إضافة التنظيم إلى المقام يعاقب الأوراق ذات الهيسيات الصغيرة (المناطق المسطحة)، مما يؤدي إلى تحديثات أصغر. تساهم قيم :math:`w_k` تلك في تنبؤ النموذج لمدخلات معينة تنتهي في الورقة المقابلة. التنبؤ النهائي هو مجموع التنبؤ الأساسي والمساهمات من كل شجرة. يتم تحويل نتيجة هذا المجموع بواسطة دالة الارتباط العكسي اعتمادًا على اختيار دالة الخسارة (انظر :ref:`gradient_boosting_formulation`).

  لاحظ أن الورقة الأصلية [XGBoost]_ تقدم مصطلح :math:`\gamma\sum_k T_k` الذي يعاقب على عدد الأوراق (مما يجعله إصدارًا سلسًا من `max_leaf_nodes`) غير مقدم هنا لأنه غير مطبق في scikit-learn؛ في حين أن :math:`\lambda` يعاقب حجم تنبؤات الشجرة الفردية قبل إعادة تحجيمها بمعدل التعلم، انظر :ref:`gradient_boosting_shrinkage`.


لاحظ أن **الإيقاف المبكر ممكن افتراضيًا إذا كان عدد العينات أكبر من 10,000**. يتم التحكم بسلوك الإيقاف المبكر عبر بارامترات ``early_stopping``، و``scoring``، و``validation_fraction``، و``n_iter_no_change``، و``tol``. من الممكن إيقافه مبكرًا باستخدام :term:`scorer` عشوائي، أو مجرد استخدام خسارة التدريب أو التحقق. لاحظ أنه لأسباب فنية، استخدام دالة قابلة للاستدعاء كمسجل أبطأ بكثير من استخدام الخسارة. بشكلٍ افتراضي، يتم تنفيذ الإيقاف المبكر إذا كان هناك ما لا يقل عن 10,00۰ عينة في مجموعة التدريب، وذلك باستخدام خسارة التحقق.

.. _nan_support_hgbt:

دعم القيم المفقودة
^^^^^^^^^^^^^^^^^^^^^^

لدى :class:`HistGradientBoostingClassifier` و:class:`HistGradientBoostingRegressor` دعم مدمج للقيم المفقودة (NaNs).
    
---

خلال التدريب، يتعلم منشئ الشجرة في كل نقطة انقسام ما إذا كان يجب توجيه العينات التي تحتوي على قيم مفقودة إلى الطفل الأيسر أو الأيمن، بناءً على المكسب المحتمل. عند التنبؤ، يتم تعيين العينات ذات القيم المفقودة إلى الطفل الأيسر أو الأيمن بناءً على ذلك::

  >>> from sklearn.ensemble import HistGradientBoostingClassifier
  >>> import numpy as np

  >>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)
  >>> y = [0, 0, 1, 1]

  >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)
  >>> gbdt.predict(X)
 array([0, 0, 1, 1])

عندما يكون نمط فقدان البيانات تنبؤيًا، يمكن إجراء عمليات الانقسام بناءً على ما إذا كانت قيمة الميزة مفقودة أم لا::

  >>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)
  >>> y = [0, 1, 0, 0, 1]
  >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,
  ...                                       max_depth=2,
  ...                                       learning_rate=1,
  ...                                       max_iter=1).fit(X, y)
  >>> gbdt.predict(X)
 array([0, 1, 0, 0, 1])

إذا لم يتم encontré أي قيم مفقودة لميزة معينة أثناء التدريب،

...

ثم يتم تعيين العينات ذات القيم المفقودة إلى أي طفل لديه معظم العينات.

.. rubric:: الأمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`

.. _sw_hgbdt:

دعم وزن العينة
^^^^^^^^^^^^^^^^^^^^^

يدعم :class:`HistGradientBoostingClassifier` و: class:`HistGradientBoostingRegressor` أوزان العينات أثناء: term:`fit`.

يوضح المثال التالي أن العينات التي يكون وزن العينة صفرًا يتم تجاهلها:

    >>> X = [[1, 0],
    ...      [1, 0],
    ...      [1, 0],
    ...      [0, 1]]
    >>> y = [0, 0, 1, 0]
    >>> # تجاهل أول عينتين تدريبيتين عن طريق تعيين وزنهما إلى 0
    >>> sample_weight = [0, 0, 1, 1]
    >>> gb = HistGradientBoostingClassifier(min_samples_leaf=1)
    >>> gb.fit(X, y, sample_weight=sample_weight)
    HistGradientBoostingClassifier(...)
    >>> gb.predict([[1, 0]])
    array([1])
    >>> gb.predict_proba([[1, 0]])[0, 1]
    0.99...

كما ترون، فإن `[1، 0]` تم تصنيفها بشكل مريح على أنها `1` نظرًا لتجاهل العينات الأولى بسبب أوزان عيناتها.

تفاصيل التنفيذ: إن مراعاة أوزان العينات تعادل
ضرب التدرجات (والتشتت) بأوزان العينة. لاحظ أن
مرحلة التصنيف (على وجه التحديد حساب النسب المئوية) لا تأخذ الأوزان في الاعتبار.

.. _categorical_support_gbdt:

دعم الميزات الفئوية
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

يدعم :class:`HistGradientBoostingClassifier` و: class:`HistGradientBoostingRegressor` بشكل أصلي الميزات الفئوية: يمكنهما النظر في الانقسامات في البيانات غير المرتبة والفئوية.

بالنسبة لمجموعات البيانات التي تحتوي على ميزات فئوية، غالبًا ما يكون استخدام الدعم الفئوي الأصلي
أفضل من الاعتماد على الترميز أحادي-الساخن
(:class:`~sklearn.preprocessing.OneHotEncoder`)، لأن الترميز أحادي-الساخن يتطلب مزيدًا من عمق الشجرة لتحقيق تقسيمات مكافئة. من الأفضل أيضًا الاعتماد على الدعم الفئوي الأصلي بدلاً من التعامل مع الميزات الفئوية كمستمرة (ترتيبية)، والتي تحدث لبيانات فئوية مشفرة بشكل ترتيبي، لأن الفئات هي كميات اسمية لا يهم ترتيبها.

لتمكين الدعم الفئوي، يمكن تمرير قناع boolean إلى
`categorical_features` المعلمة، تشير إلى الميزة التي هي فئوية. في ما يلي، ستُعامل الميزة الأولى على أنها فئوية والميزة الثانية كعددية::

  >>> gbdt = HistGradientBoostingClassifier(categorical_features=[True, False])

بشكل مكافئ، يمكن للمرء أن يمرر قائمة من الأعداد الصحيحة التي تشير إلى مؤشرات
الميزات الفئوية::

  >>> gbdt = HistGradientBoostingClassifier(categorical_features=[0])

عندما يكون الإدخال DataFrame، من الممكن أيضًا تمرير قائمة بأسماء الأعمدة::

  >>> gbdt = HistGradientBoostingClassifier(categorical_features=["site", "manufacturer"])

أخيرًا، عند استخدام DataFrame يمكننا استخدام
`categorical_features="from_dtype"` وحينها ستعامل جميع الأعمدة مع dtype الفئوية كميزات فئوية.

يجب أن تكون cardinality لكل ميزة فئوية أقل من معلمة `max_bins`. للحصول على مثال باستخدام التدرج المعزز المستند إلى الرسم البياني على الميزات الفئوية، انظر
:ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py`.

إذا كانت هناك قيم مفقودة خلال التدريب، فسيتم معاملة القيم المفقودة كفئة مناسبة. إذا لم تكن هناك قيم مفقودة خلال التدريب،
فحينئذٍ في وقت التنبؤ، يتم تعيين القيم المفقودة إلى عقدة الطفل التي تحتوي على معظم العينات (مثل الميزات المستمرة). عند التنبؤ،
يتم التعامل مع الفئات التي لم يتم العثور عليها خلال وقت الملاءمة على أنها قيم مفقودة.

.. dropdown:: العثور على الانقسام مع الميزات الفئوية

  الطريقة الأساسية للنظر في الانقسامات الفئوية في شجرة هي النظر في كل منها
  :math:`2^{K - 1} - 1` partitions، حيث :math:`K` هو عدد
  الفئات. هذا يمكن أن يصبح سريعًا محظورًا عندما يكون :math:`K` كبيرًا.
  والخير هو، بما أن أشجار التدرج المعزز هي دائما أشجار الانحدار (حتى
  لمشاكل التصنيف)، هناك استراتيجية أسرع يمكن أن تنتج عنها انقسامات مكافئة. أولاً، يتم فرز فئات الميزة وفقًا لتباين الهدف، لكل فئة `k`. بمجرد فرز الفئات، يمكن للمرء أن يفكر في *التقسيمات المستمرة*، أي التعامل مع الفئات
  كما لو كانت قيمًا مستمرة مرتبة (انظر فيشر [Fisher195۸]_ لإثبات رسمي). نتيجة لذلك، لا يلزم مراعاة سوى انقسامات :math:`K - 1` بدلاً من :math:`2^{K - 1} - 1`. الفرز الأولي هو
  :math:`\mathcal{O}(K \log(K))` operation، مما يؤدي إلى مجمع إجمالي
  :math:`\mathcal{O}(K \log(K) + K)`, instead of :math:`\mathcal{O}(2^K)`.

.. rubric:: الأمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py`

.. _monotonic_cst_gbdt:

قيود أحادية التناظر
^^^^^^^^^^^^^^^^^^^^^
يعتمد الأمر على المشكلة المطروحة، فقد يكون لديك معرفة مسبقة تشير إلى أن ميزة معينة يجب أن يكون لها بشكل عام تأثير إيجابي (أو سلبي) على القيمة المستهدفة. على سبيل المثال، مع ثبات كل العوامل الأخرى، يجب أن يزيد التصنيف الائتماني الأعلى من احتمال الحصول على الموافقة على قرض. تسمح لك قيود أحادية التزايد بتضمين هذه المعرفة المسبقة في النموذج.

بالنسبة للمتنبئ :math:`F` مع ميزتين:

- يكون قيد الزيادة أحادية على الشكل التالي:

  .. math::
      x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2)

- يكون قيد النقصان أحادية على الشكل التالي:

  .. math::
      x_1 \leq x_1' \implies F(x_1, x_2) \geq F(x_1', x_2)

يمكنك تحديد قيد أحادي على كل ميزة باستخدام معيار `monotonic_cst`. لكل ميزة، تشير قيمة 0 إلى عدم وجود قيد، بينما تشير 1 و -1 إلى قيد الزيادة أحادية والانخفاض أحادية على التوالي::

  >>> from sklearn.ensemble import HistGradientBoostingRegressor

  ... # زيادة أحادية، نقصان أحادية، وعدم وجود قيد على الميزات الثلاث
  >>> gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])

في سياق التصنيف الثنائي، يعني فرض قيد الزيادة أحادية (الانخفاض أحادية)  أن القيم الأعلى للميزة من المفترض أن يكون لها تأثير إيجابي (سلبي) على احتمال أن تنتمي العينات إلى الفئة الإيجابية.

ومع ذلك، فإن القيود الأحادية تقيد بشكل هامشي تأثيرات الميزات على الإخراج. على سبيل المثال، لا يمكن استخدام قيود الزيادة أحادية والانخفاض أحادية لتطبيق قيد النمذجة التالي:

.. math::
    x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2')

كما أنه لا يتم دعم القيود الأحادية لتصنيف متعدد الطبقات.

.. note::
    نظرًا لأن الفئات عبارة عن كميات غير مرتبة، فلا يمكن فرض قيود أحادية على الميزات الفئوية.

.. rubric:: الأمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_monotonic_constraints.py`
* :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`

.. _interaction_cst_hgbt:

قيود التفاعل
^^^^^^^^^^^^^^^^^^^^^^^

في البداية، يُسمح للأشجار ذات التعزيز التدرجي للمخطط باستخدام أي ميزة لتقسيم عقدة إلى عقد فرعية. يؤدي هذا إلى إنشاء ما يسمى بالتفاعلات بين الميزات، أي استخدام ميزات مختلفة كقسمة على طول فرع. في بعض الأحيان، نرغب في تقييد التفاعلات المحتملة، راجع [Mayer2022]_. يمكن القيام بذلك بواسطة معامل ``interaction_cst``، حيث يمكنك تحديد مؤشرات الميزات المسموح لها بالتفاعل.
على سبيل المثال، مع 3 ميزات إجمالية، تمنع ``interaction_cst=[{0}, {1}, {2}]`` جميع التفاعلات.
تحدد القيود ``[{0, 1}, {1, 2}]`` مجموعتين من الميزات التي يمكن أن تتفاعل. يُسمح للميزات 0 و 1 بالتفاعل مع بعضها البعض، وكذلك الميزات 1 و 2. ولكن لاحظ أنه لا يُسمح للميزات 0 و 2 بالتفاعل.
يمثل ما يلي شجرة والتقسيمات الممكنة للشجرة:

.. code-block:: none

      1      <- يمكن تطبيق مجموعتي القيود من الآن فصاعدًا
     / \
    1   2    <- لا يزال التقسيم الأيسر يفي بمجموعتي القيود.
   / \ / \      التقسيم الصحيح في الميزة 2 لديه فقط المجموعة {1، 2} من الآن فصاعدًا.

يستخدم LightGBM نفس المنطق للمجموعات المتداخلة.

لاحظ أن الميزات غير المدرجة في ``interaction_cst`` يتم تعيينها تلقائيًا إلى مجموعة تفاعل لأنفسهم. مع 3 ميزات أخرى، هذا يعني أن ``[{0}]`` تعادل ``[{0}, {1, 2}]``.

.. rubric:: الأمثلة

* :ref:`sphx_glr_auto_examples_inspection_plot_partial_dependence.py`

.. rubric:: المراجع

.. [Mayer2022] م. ماير، س. سي. بورسا، م. هوسلي، ود. ف. سكونياميغليو.
    2022. :doi:`تطبيقات التعلم الآلي لتقييم الأراضي والهياكل
    <10.3390/jrfm15050193>`.
    مجلة إدارة المخاطر والتمويل 15، رقم 5: 193

التوازي على مستوى منخفض
^^^^^^^^^^^^^^^^^^^^^


يستخدم :class:`HistGradientBoostingClassifier` و
:class:`HistGradientBoostingRegressor` OpenMP
للتوازي من خلال Cython. لمزيد من التفاصيل حول كيفية التحكم في عدد الخيوط، يرجى الرجوع إلى ملاحظاتنا حول :ref:`التوازي`.

الأجزاء التالية متوازية:

- خريطة العينات من القيم الحقيقية إلى الصناديق ذات القيم الصحيحة (إيجاد عتبات الصناديق هو متسلسل)
- بناء التوزيعات متوازي على الميزات
- إيجاد أفضل نقطة تقسيم في عقدة متوازية على الميزات
- أثناء التجهيز، خريطة العينات في الأطفال الأيسر والأيمن متوازية على العينات
- حساب التدرج والهيسية متوازية على العينات
- التنبؤ متوازي على العينات

.. _لماذا_هو_أسرع:

لماذا هو أسرع
^^^^^^^^^^^^^^^

إن عنق الزجاجة في إجراء التعزيز التدرجي هو بناء أشجار القرار. يتطلب بناء شجرة قرار تقليدية (كما هو الحال في GBDTs الأخرى :class:`GradientBoostingClassifier` و :class:`GradientBoostingRegressor`) فرز العينات في كل عقدة (لكل ميزة). الفرز مطلوب حتى يمكن حساب الكسب المحتمل لنقطة التقسيم بكفاءة. وبالتالي فإن تقسيم عقدة واحدة له تعقيد :math:`\mathcal{O}(n_\text{features} \times n \log(n))` حيث :math:`n` هي عدد العينات في العقدة.

:class:`HistGradientBoostingClassifier` و
:class:`HistGradientBoostingRegressor`، في المقابل، لا يتطلبان فرز قيم الميزات وبدلاً من ذلك يستخدمون بنية بيانات تسمى توزيعًا، حيث تكون العينات مرتبة ضمنيًا. بناء التوزيع له تعقيد :math:`\mathcal{O}(n)`، لذلك فإن إجراء تقسيم العقدة له تعقيد :math:`\mathcal{O}(n_\text{features} \times n)`، وهو أقل بكثير من السابق. بالإضافة إلى ذلك، بدلاً من النظر في :math:`n` نقاط تقسيم، فإننا ننظر فقط في ``max_bins`` نقاط تقسيم، والتي قد تكون أصغر بكثير.

من أجل بناء التوزيعات، يجب وضع البيانات المدخلة `X` في صناديق ذات قيم صحيحة. تتطلب عملية التقسيم إلى صناديق هذه فرز قيم الميزات، ولكنها تحدث مرة واحدة فقط في بداية عملية التعزيز (ليس في كل عقدة، كما هو الحال في :class:`GradientBoostingClassifier` و :class:`GradientBoostingRegressor`).

أخيرًا، العديد من أجزاء تنفيذ :class:`HistGradientBoostingClassifier` و :class:`HistGradientBoostingRegressor` تتم بشكل متوازي.

.. rubric:: المراجع

.. [XGBoost] تيانكي تشن، كارلوس جيسترين، :arxiv:`"XGBoost: A Scalable Tree Boosting System" <1603.02754>`

.. [LightGBM] كيه وآخرون. `"LightGBM: A Highly Efficient Gradient BoostingDecision Tree" <https://papers.nips.cc/paper/ 6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree>`_

.. [Fisher1958] فيشر، دبليو.دي. (1958). `"On Grouping for Maximum Homogeneity" <http://csiss.ncgia.ucsb.edu/SPACE/workshops/2004/SAC/files/fisher.pdf>`_ مجلة جمعية الإحصائيين الأمريكية، 53، 789-798.

:class:`GradientBoostingClassifier` و :class:`GradientBoostingRegressor`

يتم وصف استخدام ومعلمات :class:`GradientBoostingClassifier` و :class:`GradientBoostingRegressor` أدناه. أهم معلمتين لهذين المقدرين هما `n_estimators` و `learning_rate`.

.. dropdown:: تصنيف

  يدعم :class:`GradientBoostingClassifier` كل من التصنيف الثنائي والمتعدد الفئات.
  يظهر المثال التالي كيفية ملائمة مصنف التدرج المعزز
  مع 100 قرار كمتعلمين ضعفاء::

      >>> from sklearn.datasets import make_hastie_10_2
      >>> from sklearn.ensemble import GradientBoostingClassifier

      >>> X, y = make_hastie_10_2(random_state=0)
      >>> X_train, X_test = X[:2000], X[2000:]
      >>> y_train, y_test = y[:2000], y[2000:]

      >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
      ...     max_depth=1, random_state=0).fit(X_train, y_train)
      >>> clf.score(X_test, y_test)
      0.913...

  يتم التحكم في عدد المتعلمين الضعفاء (أي أشجار الانحدار) بواسطة
  المعلمة ``n_estimators``؛ :ref:`حجم كل شجرة
  <gradient_boosting_tree_size>` يمكن التحكم فيه إما عن طريق ضبط عمق الشجرة
  عبر ``max_depth`` أو عن طريق ضبط عدد عقد الأوراق عبر
  ``ماكس_ليف_نوديس``. يمثل ``learning_rate`` معلمة فرط في النطاق
  (0.0, 1.0] التي تتحكم في الإفراط في الملاءمة عبر :ref:`انكماش
  <gradient_boosting_shrinkage>` .

  .. note::

    يتطلب التصنيف مع أكثر من فصلين الحث
    ``n_classes`` أشجار الانحدار في كل تكرار،
    وبالتالي، فإن إجمالي عدد الأشجار المستحثة يساوي
    ``n_classes * n_estimators``. بالنسبة لمجموعات البيانات ذات العدد الكبير
    من الفئات نوصي بشدة باستخدام
    :class:`HistGradientBoostingClassifier` كبديل لـ
    :class:`GradientBoostingClassifier` .

.. dropdown:: الانحدار

  يدعم :class:`GradientBoostingRegressor` عددًا من
  :ref:`functions loss مختلفة <gradient_boosting_loss>`
  للانحدار والتي يمكن تحديدها عبر الوسيطة
  ``loss``؛ وظيفة الخسارة الافتراضية للانحدار هي الخطأ المربع
  (``'squared_error'``).

  ::

      >>> import numpy as np
      >>> from sklearn.metrics import mean_squared_error
      >>> from sklearn.datasets import make_friedman1
      >>> from sklearn.ensemble import GradientBoostingRegressor

      >>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)
      >>> X_train, X_test = X[:200], X[200:]
      >>> y_train, y_test = y[:200], y[200:]
      >>> est = GradientBoostingRegressor(
      ...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,
      ...     loss='squared_error'
      ... ).fit(X_train, y_train)
      >>> mean_squared_error(y_test, est.predict(X_test))
      5.00...

  يوضح الشكل أدناه نتائج تطبيق :class:`GradientBoostingRegressor`
  مع خسارة مربعات أصغر و 500 متعلم أساسي لمجموعة بيانات السكري
  (:func:`sklearn.datasets.load_diabetes`).
  يظهر الرسم البياني خطأ القطار والاختبار في كل تكرار.
  يتم تخزين خطأ القطار في كل تكرار في
  السمة `train_score_` لنموذج التعزيز التدرجي.
  يمكن الحصول على خطأ الاختبار في كل تكرار
  عبر طريقة :meth:`~GradientBoostingRegressor.staged_predict` التي تعيد
  مولدًا ينتج التنبؤات في كل مرحلة. يمكن استخدام الرسوم البيانية مثل هذه لتحديد
  العدد الأمثل للأشجار (أي ``n_estimators``) عن طريق التوقف المبكر.

  .. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_regression_001.png
    :target: ../auto_examples/ensemble/plot_gradient_boosting_regression.html
    :align: center
    :scale: 75

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`
* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_oob.py`

.. _gradient_boosting_warm_start:

تركيب متعلمين ضعفاء إضافيين
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

يدعم كل من :class:`GradientBoostingRegressor` و :class:`GradientBoostingClassifier`
``warm_start=True`` الذي يسمح لك بإضافة المزيد من المقدرين إلى
نموذج مجهز بالفعل.
    

(ملاحظة: لم تتم ترجمة النص البرمجي وعناصر التحكم الخاصة بالتصميم مثل :class:` و :ref:` و :func:` و :meth:` و :sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py` و :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_oob.py` لكونها عناصر خاصة بالتصميم والتنسيق وليست نصًا يحتاج إلى الترجمة)

هذا نص بتنسيق RST أريد ترجمته إلى اللغة العربية:

```python
>>> import numpy as np
>>> from sklearn.metrics import mean_squared_error
>>> from sklearn.datasets import make_friedman1
>>> from sklearn.ensemble import GradientBoostingRegressor

>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)
>>> X_train, X_test = X[:200], X[200:]
>>> y_train, y_test = y[:200], y[200:]
>>> est = GradientBoostingRegressor(
...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,
...     loss='squared_error'
... )
>>> est = est.fit(X_train, y_train)  # fit with 100 trees
>>> mean_squared_error(y_test, est.predict(X_test))
5.00...
>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and increase num of trees
>>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est
>>> mean_squared_error(y_test, est.predict(X_test))
3.84...
```

**التحكم في حجم الشجرة**

يحدد حجم أشجار الانحدار الأساسية مستوى تفاعلات المتغيرات التي يمكن التقاطها بواسطة نموذج Gradient Boosting. بشكل عام، يمكن لشجرة بعمق `h` التقاط تفاعلات من الترتيب `h`. هناك طريقتان يتم بهما التحكم في حجم أشجار الانحدار الفردية.

إذا حددت `max_depth=h`، فستتم زراعة أشجار ثنائية كاملة بعمق `h`. سيكون لهذه الأشجار (على الأكثر) `2**h` عقدة ورقية و`2**h - 1` عقدة تقسيم.

بدلاً من ذلك، يمكنك التحكم في حجم الشجرة عن طريق تحديد عدد عقد الأوراق عبر المعلمة `max_leaf_nodes`. في هذه الحالة، سيتم زراعة الأشجار باستخدام بحث أفضل أولاً حيث سيتم توسيع العقد ذات أعلى تحسين في النقاء أولاً. يكون للشجرة ذات `max_leaf_nodes=k` `k - 1` عقد تقسيم وبالتالي يمكنها نمذجة التفاعلات حتى الترتيب `max_leaf_nodes - 1`.

وجدنا أن `max_leaf_nodes=k` يعطي نتائج مماثلة لـ `max_depth=k-1` ولكنه أسرع بكثير للتدريب على حساب خطأ تدريب أعلى قليلاً. المعلمة `max_leaf_nodes` تقابل المتغير `J` في الفصل الخاص بتعزيز التدرج في [Friedman2001]\_ وهي مرتبطة بالمعلمة `interaction.depth` في حزمة gbm في R حيث `max_leaf_nodes == interaction.depth + 1`.

**الصياغة الرياضية**

نقدم أولاً GBRT للانحدار، ثم نوضح حالة التصنيف.

* **الانحدار**

 نماذج GBRT المضافة للتوقع :math:`\hat{y}_i` لمدخل معين :math:`x_i` هي بالشكل التالي:

 .. math::

    \hat{y}_i = F_M(x_i) = \sum_{m=1}^{M} h_m(x_i)

 حيث إن :math:`h_m` هي تقييمات تسمى *المتعلمون الضعفاء* في سياق التعزيز. يستخدم Gradient Tree Boosting [قرارات شجرة الانحدار] ذات الحجم الثابت كمتعلمين ضعفاء. القيمة الثابتة M تقابل معلمة `n_estimators`.

 على غرار خوارزميات التعزيز الأخرى، يتم بناء GBRT بطريقة متكررة:

 .. math::

    F_m(x) = F_{m-1}(x) + h_m(x),

 حيث يتم تركيب الشجرة الجديدة :math:`h_m` من أجل تقليل مجموع خسائر :math:`L_m`، نظراً لمجموعة :math:`F_{m-1}` السابقة:

 .. math::

    h_m =  \arg\min_{h} L_m = \arg\min_{h} \sum_{i=1}^{n}
    l(y_i, F_{m-1}(x_i) + h(x_i)),

 حيث :math:`l(y_i, F(x_i))` يتم تحديده بواسطة معلمة `loss`، المفصلة في القسم التالي.

 بشكل افتراضي، يتم اختيار النموذج الأولي :math:`F_{0}` كثابت يقلل الخسارة: لخسارة مربعات أقل، هذا هو المتوسط التجريبي لقيم الهدف. يمكن أيضًا تحديد النموذج الأولي عبر وسيط ``init``.

 باستخدام تقريب تايلور من الدرجة الأولى، يمكن تقريب قيمة :math:`l` على النحو التالي:

 .. math::

    l(y_i, F_{m-1}(x_i) + h_m(x_i)) \approx
    l(y_i, F_{m-1}(x_i))
    + h_m(x_i)
    \left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m - 1}}.

 .. note::

    باختصار، تقريب تايلور من الدرجة الأولى يقول أن
    :math:`l(z) \approx l(a) + (z - a) \frac{\partial l}{\partial z}(a)`.
    هنا، :math:`z` يتوافق مع :math:`F_{m - 1}(x_i) + h_m(x_i)`، و
    :math:`a` يتوافق مع :math:`F_{m-1}(x_i)`

 الكمية :math:`\left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)}
 \right]_{F=F_{m - 1}}` هي مشتق الخسارة بالنسبة لمعلماتها الثانية، ويتم تقييمها عند :math:`F_{m-1}(x)`. من السهل حسابها لأي :math:`F_{m - 1}(x_i)` معين بشكل مغلق لأن الخسارة قابلة للاختلاف. سنشير إليه بـ :math:`g_i`.

 بعد إزالة المصطلحات الثابتة، لدينا:

 .. math::

    h_m \approx \arg\min_{h} \sum_{i=1}^{n} h(x_i) g_i

 يتم تقليل هذا إذا تم تركيب :math:`h(x_i)` للتنبؤ بالقيمة التي تتناسب مع التدرج السلبي :math:`-g_i`. لذلك، في كل تكرار، يتم تركيب **المقدر** :math:`h_m` للتنبؤ بالتدرجات السلبية للعينات. يتم تحديث التدرجات في كل تكرار. يمكن اعتبار هذا نوعاً من الانحدار التدريجي في الفضاء الوظيفي.

 .. note::

    بالنسبة لبعض الخسارات، مثل ``'absolute_error'`` حيث التدرجات
    هي :math:`\pm 1`، فإن القيم التي يتنبأ بها :math:`h_m` المجهز ليست
    دقيقة بما فيه الكفاية: لا يمكن للشجرة إخراج إلا قيم صحيحة. نتيجة لذلك، يتم تعديل قيم أوراق الشجرة :math:`h_m` بمجرد تركيب الشجرة، بحيث تقلل قيم الأوراق من الخسارة :math:`L_m`. يكون
    التحديث تابعاً للخسارة: بالنسبة لخسارة الخطأ المطلق، يتم تحديث قيمة
    ورقة إلى الوسيط المتوسط للعينات في تلك الورقة.

* **التصنيف**

 تعزيز التدرج للتصنيف مشابه جداً لحالة الانحدار. ومع ذلك، فإن مجموع الأشجار :math:`F_M(x_i) = \sum_m h_m(x_i)` ليس متجانساً مع التوقع: لا يمكن أن يكون فئة، لأن الأشجار تتوقع قيم مستمرة.

 يعد التعيين من القيمة :math:`F_M(x_i)` إلى فئة أو احتمال خسارة تابع. بالنسبة للخسارة اللوغاريتمية، يتم نمذجة الاحتمال الذي ينتمي إليه :math:`x_i` إلى الفئة الإيجابية على أنه :math:`p(y_i = 1 | x_i) = \sigma(F_M(x_i))` حيث :math:`\sigma` هي دالة سيجمويد أو expit.

بالنسبة لتصنيف متعدد الفئات، يتم بناء K شجرة (لفئات K) في كل تكرار من M. يتم نمذجة احتمال أن ينتمي x\_i إلى الفئة k كـ softmax لقيم F\_\{M,k\}(x\_i).

لاحظ أنه حتى بالنسبة لمهمة التصنيف، فإن التقدير الفرعي h\_m لا يزال جهازًا للتقدير وليس مصنفًا. هذا لأن التقديرات الفرعية مدربة للتنبؤ (بشكل سلبي) بالتدرجات ، وهي دائمًا كميات مستمرة.

.. _gradient\_boosting\_loss:

دوال الخسارة
^^^^^^^^^^^^^^

يتم دعم دوال الخسارة التالية ويمكن تحديدها باستخدام معلمة ``loss``:

.. dropdown:: التراجع

  * خطأ مربع (``'squared\_error'``): الخيار الطبيعي للتراجع نظرًا لخصائصه الحسابية الفائقة. النموذج الأولي مقدم من متوسط قيم الهدف.
  * خطأ مطلق (``'absolute\_error'``): دالة خسارة قوية للتراجع. النموذج الأولي مقدم من متوسط قيم الهدف.
  * هوبر (``'huber'``): دالة خسارة قوية أخرى تجمع بين أقل المربعات وأقل انحراف مطلق ؛ استخدم ``alpha`` للتحكم في الحساسية فيما يتعلق بالقيم المتطرفة (انظر [Friedman2001]\_ لمزيد من التفاصيل).
  * Quantile (``'quantile'``): دالة خسارة لتراجع Quantile. استخدم ``0 < alpha < 1`` لتحديد الكمية. يمكن استخدام دالة الخسارة هذه لإنشاء فترات تنبؤ (انظر: ref: `sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_quantile.py`).

.. dropdown:: التصنيف

  * خسارة ثنائية (``'log-loss'``): دالة خسارة للمدى السلبي للمتغير العشوائي المعمم للمدى الثنائي للتصنيف الثنائي. يوفر تقديرات احتمالية. النموذج الأولي مقدم من معامل التناسب اللوغاريتمي.
  * خسارة متعددة (``'log-loss'``): دالة خسارة للمدى السلبي للمتغير العشوائي المعمم للمدى متعدد الفئات للتصنيف متعدد الفئات مع ``n\_classes`` فئات متبادلة. يوفر تقديرات احتمالية. النموذج الأولي مقدم من الاحتمال المسبق لكل فئة. في كل تكرار ، يجب إنشاء أشجار الانحدار ``n\_classes`` مما يجعل GBRT غير فعال لمجموعات البيانات التي تحتوي على عدد كبير من الفئات.
  * خسارة أسيّة (``'exponential'``): نفس دالة الخسارة كـ :class:`AdaBoostClassifier`. أقل قوة ضد الأمثلة غير المعلمة بشكل صحيح من ``'log-loss'`` ؛ يمكن استخدامها فقط للتصنيف الثنائي.

.. _gradient\_boosting\_shrinkage:

الانكماش عبر معدل التعلم
^^^^^^^^^^^^^^^^^^^^^^^^^^^

[Friedman2001]\_ اقترح استراتيجية تنظيمية بسيطة تقوم بضرب مساهمة كل متعلم ضعيف في عامل ثابت :math:`\nu`:

.. math::

    F_m(x) = F_{m-1}(x) + \nu h_m(x)

المعلمة :math:`\nu` تسمى أيضًا **معدل التعلم** لأنها تضاعف طول خطوة إجراء الانحدار المتدرج ؛ يمكن تعيينه عبر معلمة ``learning\_rate``.

يتفاعل معامل ``learning\_rate`` بقوة مع معامل ``n\_estimators`` ، عدد المتعلمين الضعفاء ليتناسبوا. تتطلب القيم الأصغر لـ ``learning\_rate`` أعدادًا أكبر من المتعلمين الضعفاء للحفاظ على خطأ تدريب ثابت. تشير الأدلة التجريبية إلى أن القيم الصغيرة لـ ``learning\_rate`` تفضل خطأ اختبار أفضل. [HTF]\_ يوصي بتعيين معدل التعلم إلى ثابت صغير (على سبيل المثال ``learning\_rate <= 0.1``) واختيار ``n\_estimators`` كبير بما يكفي للتطبيق التوقف المبكر ، انظر :ref:`sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_early\_stopping.py` للحصول على مناقشة أكثر تفصيلاً للتفاعل بين ``learning\_rate`` و ``n\_estimators`` انظر [R2007]\_.

التمثيل الفرعي
^^^^^^^^^^^^

[Friedman2002]\_ اقترح التدرج المعزز العشوائي ، الذي يجمع بين التدرج المعزز ومتوسط Bootstrap (bagging). في كل تكرار ، يتم تدريب المصنف الأساسي على جزء ``subsample`` من بيانات التدريب المتاحة. يتم رسم عينة فرعية بدون استبدال. القيمة النموذجية لـ ``subsample`` هي 0.5.

توضح الصورة أدناه تأثير الانكماش والتمثيل الفرعي على جودة نموذج الملاءمة. يمكننا أن نرى بوضوح أن الانكماش يتفوق على عدم الانكماش. يمكن للتمثيل الفرعي مع الانكماش أن يزيد من دقة النموذج. من ناحية أخرى ، فإن التمثيل الفرعي دون انكماش يؤدي بشكل سيء.

.. figure:: ../auto\_examples/ensemble/images/sphx\_glr\_plot\_gradient\_boosting\_regularization\_001.png
   :target: ../auto\_examples/ensemble/plot\_gradient\_boosting\_regularization.html
   :align: center
   :scale: 75

هناك استراتيجية أخرى لتقليل التباين وهي التمثيل الفرعي للميزات المماثل للانقسامات العشوائية في :class:`RandomForestClassifier`. يمكن التحكم في عدد الميزات الفرعية عبر معلمة ``max\_features``.

.. ملاحظة:: يمكن أن يؤدي استخدام قيمة ``max\_features`` صغيرة إلى تقليل وقت التشغيل بشكل كبير.

يسمح التدرج المعزز العشوائي بحساب تقديرات خارج الحقيبة لاختبار الانحراف عن طريق حساب التحسن في الانحراف على الأمثلة غير المضمنة في عينة Bootstrap (أي أمثلة خارج الحقيبة). يتم تخزين التحسينات في السمة `oob\_improvement\_`. يحمل ``oob\_improvement\_[i]`` التحسن من حيث الخسارة على عينات OOB إذا قمت بإضافة المرحلة i إلى التنبؤات الحالية. يمكن استخدام تقديرات خارج الحقيبة لاختيار النموذج ، على سبيل المثال لتحديد عدد التكرارات الأمثل. عادة ما تكون تقديرات خارج الحقيبة متشائمة جدًا ، وبالتالي نوصي باستخدام التحقق المتقاطع بدلاً من ذلك ولا تستخدم خارج الحقيبة إلا إذا كان التحقق المتقاطع يستغرق وقتًا طويلاً.

.. rubric:: أمثلة

* :ref:`sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_regularization.py`
* :ref:`sphx\_glr\_auto\_examples\_ensemble\_plot\_gradient\_boosting\_oob.py`
* :ref:`sphx\_glr\_auto\_examples\_ensemble\_plot\_ensemble\_oob.py`

التفسير بأهمية الميزة
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

يمكن تفسير أشجار القرار الفردية بسهولة عن طريق تصور بنية الشجرة. ومع ذلك ، فإن نماذج التدرج المعزز تتكون من مئات أشجار الانحدار وبالتالي لا يمكن تفسيرها بسهولة عن طريق الفحص البصري للأشجار الفردية. لحسن الحظ ، تم اقتراح عدد من التقنيات لتلخيص وتفسير نماذج التدرج المعزز.
    

غالبًا لا تساهم الميزات بالتساوي في توقع الاستجابة المستهدفة ؛ في العديد من المواقف ، تكون معظم الميزات غير ذات صلة في الواقع.

عند تفسير نموذج ، فإن السؤال الأول عادةً هو: ما هي تلك الميزات المهمة وكيف تساهم في توقع الاستجابة المستهدفة؟

تؤدي أشجار القرار الفردية بشكل جوهري اختيار الميزات عن طريق تحديد نقاط الانقسام المناسبة. يمكن استخدام هذه المعلومات لقياس أهمية كل ميزة ؛ الفكرة الأساسية هي: كلما زاد استخدام الميزة في نقاط الانقسام في الشجرة ، زادت أهمية هذه الميزة. يمكن توسيع هذا المفهوم لأهمية الميزة إلى مجموعات أشجار القرار عن طريق حساب متوسط أهمية الميزة المستندة إلى النجاسة لكل شجرة (انظر :ref:`random_forest_feature_importance` لمزيد من التفاصيل).

يمكن الوصول إلى درجات أهمية الميزة لنموذج التدرج المعزز المناسب عبر خاصية ``feature_importances_``::

    >>> from sklearn.datasets import make_hastie_10_2
    >>> from sklearn.ensemble import GradientBoostingClassifier

    >>> X, y = make_hastie_10_2(random_state=0)
    >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    ...     max_depth=1, random_state=0).fit(X, y)
    >>> clf.feature_importances_
    array([0.10 ..., 0.10 ..., 0.11 ..., ...

لاحظ أن حساب أهمية الميزة هذا يعتمد على الانتروبي، وهو يختلف عن :func:`sklearn.inspection.permutation_importance` الذي يعتمد على التباديل للميزات.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`

.. rubric:: مراجع

.. [Friedman2001] Friedman، JH (2001). :doi:`Greedy function approximation: A gradient
   boosting machine <10.1214/aos/1013203451>`.
   Annals of Statistics، 29، 1189-1232.

.. [Friedman2002] Friedman، JH (2002). `Stochastic gradient boosting.
   <https://statweb.stanford.edu/~jhf/ftp/stobst.pdf>`_.
   Computational Statistics & Data Analysis، 38، 367-378.

.. [R2007] G. Ridgeway (2006). `Generalized Boosted Models: A guide to the gbm
   package <https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf>`_

.. _forest:

غابات عشوائية وغيرها من مجموعات الأشجار العشوائية

الوحدة :mod:`sklearn.ensemble` تتضمن طريقتين للتقدير المستند إلى متوسط ​مبنيتان على أشجار القرار العشوائية :ref:`decision trees <tree>`: طريقة RandomForest وخوارزمية Extra-Trees. كلتا الخوارزميتين هما تقنيات perturb-and-combine [B1998]_ مصممة خصيصًا للأشجار. هذا يعني أنه يتم إنشاء مجموعة متنوعة من التصنيفات من خلال إدخال العشوائية في بناء التصنيف. يتم إعطاء توقع المجموعة كمتوسط ​​توقعات التصنيفات الفردية.

مثل التصنيفات الأخرى، يجب تركيب مصنفات الغابة مع صفيفين: صفيف X كثيف أو متناثر بالشكل ``(n_samples, n_features)`` يحمل عينات التدريب، وصفيف Y بالشكل ``(n_samples,)`` يحمل قيم الهدف (تسميات الفئة) لعينات التدريب::

    >>> from sklearn.ensemble import RandomForestClassifier
    >>> X = [[0, 0], [1, 1]]
    >>> Y = [0, 1]
    >>> clf = RandomForestClassifier(n_estimators=10)
    >>> clf = clf.fit(X, Y)

مثل :ref:`decision trees <tree>`, تمتد غابات الأشجار أيضًا إلى :ref:`multi-output problems <tree_multioutput>`  (إذا كان Y عبارة عن صفيف بالشكل ``(n_samples, n_outputs)``).

الغابات العشوائية
--------------

في الغابات العشوائية (انظر فصول :class:`RandomForestClassifier` و :class:`RandomForestRegressor`), يتم بناء كل شجرة في المجموعة من عينة يتم سحبها مع الاستبدال (أي عينة bootsrap) من مجموعة التدريب.

علاوة على ذلك، عند تقسيم كل عقدة أثناء بناء الشجرة، يتم العثور على أفضل تقسيم من خلال بحث شامل عن قيم الميزات الخاصة بجميع ميزات الإدخال أو مجموعة فرعية عشوائية بحجم ``max_features``. (انظر :ref:`parameter tuning guidelines <random_forest_parameters>` للمزيد من التفاصيل.)

الغرض من هاتين المصدرين من العشوائية هو تقليل تباين معدل الغابة. في الواقع، عادة ما تظهر أشجار القرار الفردية تباينًا عاليًا وتميل إلى الإفراط في التناسب. تؤدي العشوائية المحقونة في الغابات إلى أشجار قرار ذات أخطاء تنبؤ مرتبطة إلى حد ما. من خلال أخذ متوسط ​​تلك التنبؤات، يمكن إلغاء بعض الأخطاء. تحقق الغابات العشوائية انخفاضًا في التباين من خلال الجمع بين الأشجار المتنوعة، وأحيانًا على حساب زيادة طفيفة في التحيز. في الممارسة العملية، يكون انخفاض التباين كبيرًا في الغالب وبالتالي ينتج عنه نموذج أفضل بشكل عام.

على النقيض من المنشور الأصلي [B2001]_، يجمع تنفيذ scikit-learn بين التصنيفات من خلال حساب متوسط ​​تنبؤاتها الاحتمالية، بدلاً من السماح لكل مصنف بالتصويت لفئة واحدة.

بديل تنافسي للغابات العشوائية هو :ref:`histogram_based_gradient_boosting` (HGBT) نماذج:

-  بناء الأشجار: تعتمد الغابات العشوائية عادةً على أشجار عميقة (تتجاوز بشكل فردي) التي تستخدم الكثير من الموارد الحسابية، لأنها تتطلب العديد من عمليات التقسيم وتقييمات المرشحين. تبني نماذج التعزيز أشجار ضحلة (تتناسب بشكل فردي) وهي أسرع في التناسب والتنبؤ.

-  التعزيز المتسلسل: في HGBT، يتم بناء أشجار القرار بشكل متسلسل، حيث يتم تدريب كل شجرة على تصحيح الأخطاء التي ارتكبتها الأشجار السابقة. هذا يسمح لهم بتحسين أداء النموذج بشكل تكراري باستخدام عدد قليل نسبيًا من الأشجار. في المقابل، تستخدم الغابات العشوائية أغلبية التصويت للتنبؤ بالنتيجة، والتي يمكن أن تتطلب عددًا أكبر من الأشجار لتحقيق نفس المستوى من الدقة.

-  التحزيم الفعال: يستخدم HGBT خوارزمية تحزيم فعالة يمكنها التعامل مع مجموعات البيانات الكبيرة مع عدد كبير من الميزات. يمكن لخوارزمية التحزيم معالجة البيانات مسبقًا لتسريع بناء الشجرة اللاحق (انظر :ref:`Why it's faster <Why_it's_faster>`). في المقابل، لا يستخدم تنفيذ الغابات العشوائية scikit-learn التحزيم ويعتمد على التقسيم الدقيق، والذي يمكن أن يكون مكلفًا حسابيًا.

بشكل عام، تعتمد التكلفة الحسابية لـ HGBT مقابل RF على الخصائص المحددة لمجموعة البيانات ومهمة النمذجة. إنها فكرة جيدة أن تجربهما النموذجين وتقارن أدائهما والكفاءة الحسابية في مشكلتك المحددة لتحديد النموذج الأفضل.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`

الأشجار العشوائية للغاية
--------------------------

في الأشجار العشوائية للغاية (انظر فصول :class:`ExtraTreesClassifier` و :class:`ExtraTreesRegressor`), تذهب العشوائية إلى أبعد من ذلك في طريقة حساب التقسيمات. كما هو الحال في الغابات العشوائية، يتم استخدام مجموعة فرعية عشوائية من ميزات المرشح، ولكن بدلاً من البحث عن أكثر العتبات تمييزًا، يتم رسم عتبات عشوائية لكل ميزة مرشح ويتم اختيار أفضل هذه العتبات التي تم إنشاؤها عشوائيًا باعتبارها قاعدة التقسيم. هذا يسمح عادةً بتقليل تباين النموذج أكثر قليلاً، على حساب زيادة طفيفة في الانحياز::

    >>> from sklearn.model_selection import cross_val_score
    >>> from sklearn.datasets import make_blobs
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.ensemble import ExtraTreesClassifier
    >>> from sklearn.tree import DecisionTreeClassifier

    >>> X, y = make_blobs(n_samples=10000, n_features=10, centers=100,
    ...     random_state=0)

    >>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,
    ...     random_state=0)
    >>> scores = cross_val_score(clf, X, y, cv=5)
    >>> scores.mean()
    0.98...

    >>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
    ...     min_samples_split=2, random_state=0)
    >>> scores = cross_val_score(clf, X, y, cv=5)
    >>> scores.mean()
    0.999...

    >>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
    ...     min_samples_split=2, random_state=0)
    >>> scores = cross_val_score(clf, X, y, cv=5)
    >>> scores.mean() > 0.999
    True

.. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_forest_iris_001.png
    :target: ../auto_examples/ensemble/plot_forest_iris.html
    :align: center
    :scale: 75%

.. _random_forest_parameters:

المعلمات

    
feitozeco/done-python/bot.py
from telegram.ext import Updater, CommandHandler, MessageHandler, Filters
import responses as R

def start_command(update, context):
    update.message.reply_text('شكرا لاستخدامي بامكانك الان ارسالي النصوص والمعادلات والرموز الشفرة البرمجية و ساكتب لك الرد المناسب')

def help_command(update, context):
    update.message.reply_text('اكتب النص البرمجي او الرياضي او معادلة وسارد لك الرد المناسب ')

def handle_message(update, context):
    text = str(update.message.text).lower()
    response = R.sample_responses(text)

    update.message.reply_text(response)

def error(update, context):
    print(f'Update {update} caused error {context.error}')

def main():
    updater = Updater('bot token here', use_context=True)

    dp = updater.dispatcher

    dp.add_handler(CommandHandler('start', start_command))
    dp.add_handler(CommandHandler('help', help_command))

    dp.add_handler(MessageHandler(Filters.text, handle_message))

    dp.add_error_handler(error)


البارامترات الرئيسية التي يجب ضبطها عند استخدام هذه الأساليب هي ``n_estimators`` و ``max_features``. الأول هو عدد الأشجار في الغابة. كلما زاد عدد الأشجار، كلما كان أفضل، ولكن سيستغرق الأمر وقتًا أطول للحساب. بالإضافة إلى ذلك، لاحظ أن النتائج لن تتحسن بشكل كبير بعد عدد حرج من الأشجار. الأخير هو حجم المجموعات العشوائية من الميزات التي يجب أخذها في الاعتبار عند تقسيم العقدة. كلما انخفضت، زادت تقليل التباين، ولكن أيضًا زاد التحيز. القيم الافتراضية الجيدة التجريبية هي ``max_features=1.0`` أو ما يعادلها ``max_features=None`` (دائمًا ما تفكر في جميع الميزات بدلاً من مجموعة فرعية عشوائية) لمشاكل الانحدار، و ``max_features="sqrt"`` (باستخدام مجموعة فرعية عشوائية من الحجم ``sqrt(n_features)``) لمهام التصنيف (حيث ``n_features`` هو عدد الميزات في البيانات). القيمة الافتراضية لـ ``max_features=1.0`` تعادل الأشجار في الحقيبة ويمكن تحقيق المزيد من العشوائية عن طريق تعيين قيم أصغر (على سبيل المثال، 0.3 هي افتراضي نموذجي في الأدبيات). غالبًا ما يتم تحقيق نتائج جيدة عند تعيين ``max_depth=None`` مع ``min_samples_split=2`` (أي عند تطوير الأشجار بالكامل). ومع ذلك، ضع في اعتبارك أن هذه القيم عادةً ما تكون غير مثالية، وقد تؤدي إلى نماذج تستهلك الكثير من ذاكرة الوصول العشوائي. يجب دائمًا التحقق من صحة أفضل قيم للبارامترات. بالإضافة إلى ذلك، لاحظ أنه في الغابات العشوائية ، يتم استخدام عيناتbootstrap بشكل افتراضي (``bootstrap=True``) بينما الاستراتيجية الافتراضية للأشجار الإضافية هي استخدام مجموعة البيانات بأكملها (``bootstrap=False``). عند استخدام عينات bootstrap ، يمكن تقدير خطأ التعميم على العينات المتبقية أو العينات خارج الحقيبة. يمكن تمكين هذا عن طريق تعيين ``oob_score=True``.

.. note::

     حجم النموذج مع البارامترات الافتراضية هو :math:`O( M * N * log (N) )`, حيث :math:`M` هو عدد الأشجار و :math:`N` هو عدد العينات. لتقليل حجم النموذج، يمكنك تغيير هذه البارامترات: ``min_samples_split``, ``max_leaf_nodes``, ``max_depth`` و ``min_samples_leaf``.

التوازي
-------

أخيرًا، تتميز هذه الوحدة أيضًا بالتوازي في بناء الأشجار والحساب المتوازي للتنبؤات من خلال بارامتر ``n_jobs``. إذا كان ``n_jobs=k``، فسيتم تقسيم العمليات إلى ``k`` وظيفة، وتشغيلها على ``k`` نواة للجهاز. إذا كان ``n_jobs=-1``، فسيتم استخدام جميع النوى المتاحة على الجهاز. لاحظ أنه بسبب زيادة حمل الاتصال بين العمليات، قد لا يكون الإسراع خطيًا (أي أن استخدام ``k`` وظيفة لن يكون للأسف ``k`` أسرع). ومع ذلك، لا يزال من الممكن تحقيق تسريع كبير عند بناء عدد كبير من الأشجار، أو عندما يتطلب بناء شجرة واحدة قدرًا معقولاً من الوقت (على سبيل المثال، في مجموعات البيانات الكبيرة).

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_iris.py`
* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_importances_faces.py`
* :ref:`sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py`

.. rubric:: المراجع

.. [B2001] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.

.. [B1998] L. Breiman, "Arcing Classifiers", Annals of Statistics 1998.

* P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees", Machine Learning, 63(1), 3-42, 2006.

.. _random_forest_feature_importance:

تقييم أهمية الميزة
    

يمكن استخدام الرتبة النسبية (أي العمق) لخاصية ما كمحور قرار في شجرة لتقييم الأهمية النسبية لهذه الخاصية فيما يتعلق بقابلية توقع متغير الهدف. تساهم الخصائص المستخدمة في أعلى الشجرة في قرار التنبؤ النهائي لجزء أكبر من عينات الإدخال. وبالتالي، يمكن استخدام **الجزء المتوقع من العينات** التي تساهم فيها كإحصائية لتقدير **الأهمية النسبية للخصائص**. في مكتبة scikit-learn، يتم دمج جزء العينات التي تساهم فيها خاصية ما مع انخفاض الشوائب الناجم عن تقسيمها لإنشاء تقدير موحد للقوة التنبؤية لتلك الخاصية.

عن طريق **حساب المتوسط** للتقديرات الخاصة بالقدرة التنبؤية عبر العديد من الأشجار العشوائية، يمكن **تقليل التباين** لهذا التقدير واستخدامه في اختيار الميزات. يعرف هذا باسم متوسط الانخفاض في النجاسة، أو MDI. راجع [L2014]_ للحصول على مزيد من المعلومات حول MDI وتقييم أهمية الميزة مع الغابات العشوائية.

.. تحذير::

  تعاني تقديرات أهمية الميزات المعتمدة على النجاسة المحسوبة على نماذج الأشجار من عيبين قد يؤديان إلى استنتاجات مضللة. أولاً، يتم حسابها على إحصائيات مشتقة من مجموعة بيانات التدريب، وبالتالي **لا تخبرنا بالضرورة بالميزات الأكثر أهمية لتحقيق تنبؤات جيدة على مجموعة بيانات منفصلة**. وثانيًا، **تفضل الميزات ذات التعداد العالي**، أي الميزات التي تحتوي على العديد من القيم الفريدة.
  :ref:`permutation_importance` هو بديل لأهمية الميزات المستندة إلى النجاسة لا يعاني من هذه العيوب. يتم استكشاف هاتين الطريقتين للحصول على أهمية الميزة في:
  :ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance.py`.

يظهر المثال التالي تمثيلًا ملونًا للأهمية النسبية لكل بكسل فردي لمهمة التعرف على الوجه باستخدام نموذج :class:`ExtraTreesClassifier`.

.. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_forest_importances_faces_001.png
   :target: ../auto_examples/ensemble/plot_forest_importances_faces.html
   :align: center
   :scale: 75

في الممارسة العملية، يتم تخزين هذه التقديرات كسمة تسمى ``feature_importances_`` في النموذج الملائم. هذا عبارة عن مصفوفة بأبعاد ``(n_features,)`` وقيم موجبة ومجموعها 1.0. كلما زادت القيمة، زادت أهمية مساهمة الخاصية المطابقة لوظيفة التنبؤ.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_importances_faces.py`
* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_importances.py`

.. rubric:: المراجع

.. [L2014] G. Louppe, :arxiv:`"Understanding Random Forests: From Theory to
   Practice" <1407.750

... (المقال طويل جدًا لذلك قمت بترجمة جزء منه فقط. هل تريدني أن أواصل الترجمة؟)

(ملاحظة: تم الحفاظ على رموز التنسيق الخاصة بنص RST في الترجمة، مثل ``**`` و ``:class:`` و ``:ref:`` وغيرها، لتسهيل إعادة تنسيق النص بعد الترجمة إذا لزم الأمر.)

في خوارزميات التعلم الجماعي، تشكل طرق الأكياس فئة من الخوارزميات التي تبني عدة حالات من مُقدر الصندوق الأسود على مجموعات فرعية عشوائية من مجموعة التدريب الأصلية ثم تجمع توقعاتها الفردية لتشكيل توقع نهائي. يتم استخدام هذه الطرق كوسيلة لتقليل تباين مُقدر الأساس (مثل شجرة القرار)، عن طريق إدخال التوزيع العشوائي في إجراء البناء الخاص بها ثم عمل مجموعة منه. في العديد من الحالات، تشكل طرق الأكياس طريقة بسيطة للغاية للتحسين بالنسبة لنموذج واحد، دون الحاجة إلى تكييف خوارزمية الأساس الأساسية. نظرًا لأنها توفر طريقة لتقليل الإفراط في التخصيص، تعمل طرق الأكياس بشكل أفضل مع النماذج القوية والمعقدة (مثل أشجار القرار المطورة بالكامل)، على النقيض من طرق التعزيز التي تعمل عادةً بشكل أفضل مع النماذج الضعيفة (مثل أشجار القرار الضحلة).

تأتي طرق الأكياس بنكهات عديدة ولكنها تختلف في الغالب عن بعضها البعض في طريقة رسمها لمجموعات فرعية عشوائية من مجموعة التدريب:

* عندما يتم رسم مجموعات فرعية عشوائية من مجموعة البيانات كمجموعات فرعية عشوائية من العينات، maka يُعرف هذا الخوارزمية باسم Pasting [B1999]_.

* عندما يتم سحب العينات مع الاستبدال، maka يُعرف الأسلوب باسم Bagging [B1996]_.

* عندما يتم رسم مجموعات فرعية عشوائية من مجموعة البيانات كمجموعات فرعية عشوائية من الميزات، maka يُعرف الأسلوب باسم Random Subspaces [H1998]_.

* أخيرًا، عندما يتم بناء مُقدر الأساس على مجموعات فرعية من كل من العينات

...

* والميزات، maka يُعرف الأسلوب باسم Random Patches [LG2012]_.

في سكيكيت-ليرن، يتم تقديم طرق الأكياس كمقدر فوقي موحد :class:`BaggingClassifier` (على التوالي :class:`BaggingRegressor`)، مع الأخذ في الاعتبار مُقدر محدد من قبل المستخدم إلى جانب المعلمات التي تحدد الإستراتيجية لرسم مجموعات فرعية عشوائية. على وجه الخصوص، يتحكم ``max_samples`` و ``max_features`` في حجم المجموعات الفرعية (من حيث العينات والميزات)، بينما يتحكم ``bootstrap`` و ``bootstrap_features`` في ما إذا كانت العينات والميزات يتم سحبها مع الاستبدال أو بدونه. عند استخدام مجموعة فرعية من العينات المتاحة، يمكن تقدير دقة التعميم باستخدام عينات خارج الحقيبة عن طريق ضبط ``oob_score=True``. على سبيل المثال، يوضح المقتطف أدناه كيفية إنشاء مجموعة أكياس من مُقدري :class:`~sklearn.neighbors.KNeighborsClassifier`، تم بناء كل منها على مجموعات فرعية عشوائية من 50٪ من العينات و 50٪ من الميزات.

    >>> from sklearn.ensemble import BaggingClassifier
    >>> from sklearn.neighbors import KNeighborsClassifier
    >>> bagging = BaggingClassifier(KNeighborsClassifier(),
    ...                             max_samples=0.5, max_features=0.5)

.. rubric :: أمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_bias_variance.py`

.. rubric :: المراجع

.. [B1999] L. Breiman, "Pasting small votes for classification in large
   databases and on-line", Machine Learning, 36(1), 85-103, 1999.

.. [B1996] L. Breiman, "Bagging predictors", Machine Learning, 24(2),
   123-140, 1996.

.. [H1998] T. Ho, "The random subspace method for constructing decision
   forests", Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998.

.. [LG2012] G. Louppe and P. Geurts, "Ensembles on Random Patches",
   Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.



.. _voting_classifier:

مصنف التصويت


الفكرة وراء :class:`VotingClassifier` هي دمج مُصنّفات التعلم الآلي المفاهيمية المختلفة واستخدام التصويت بالأغلبية أو متوسط الاحتمالات المتوقعة (تصويت ناعم) للتنبؤ بملصقات الفئات. يمكن أن يكون مثل هذا المصنف مفيدًا لمجموعة من النماذج التي تعمل بشكل جيد بالتساوي من أجل موازنة نقاط ضعفها الفردية.

ملصقات الفئة الأغلبية (تصويت بالأغلبية/تصويت صعب)
--------------------------------------------

في التصويت بالأغلبية، يكون ملصق الفئة المتوقع لعينة معينة هو ملصق الفئة الذي يمثل الأغلبية (النمط) لملصقات الفئة التي يتوقعها كل مصنف فردي.

على سبيل المثال، إذا كان التوقع لعينة معينة هو

- المصنف 1 -> الفئة 1
- المصنف 2 -> الفئة 1
- المصنف 3 -> الفئة 2

سيقوم VotingClassifier (مع ``voting='hard'``) بتصنيف العينة على أنها "الفئة 1" بناءً على ملصق الفئة الأغلبية.

في حالات التعادل، سيختار :class:`VotingClassifier` الفئة بناءً على ترتيب الفرز التصاعدي. على سبيل المثال، في السيناريو التالي

- المصنف 1 -> الفئة 2
- المصنف 2 -> الفئة 1

سيتم تعيين ملصق الفئة 1 للعينة.

الاستخدام
-----

يوضح المثال التالي كيفية ملائمة مصنف قاعدة الأغلبية::

   >>> from sklearn import datasets
   >>> from sklearn.model_selection import cross_val_score
   >>> from sklearn.linear_model import LogisticRegression
   >>> from sklearn.naive_bayes import GaussianNB
   >>> from sklearn.ensemble import RandomForestClassifier
   >>> from sklearn.ensemble import VotingClassifier

   >>> iris = datasets.load_iris()
   >>> X, y = iris.data[:, 1:3], iris.target

   >>> clf1 = LogisticRegression(random_state=1)
   >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
   >>> clf3 = GaussianNB()

   >>> eclf = VotingClassifier(
   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
   ...     voting='hard')

   >>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):
   ...     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)
   ...     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
   Accuracy: 0.95 (+/- 0.04) [Logistic Regression]
   Accuracy: 0.94 (+/- 0.04) [Random Forest]
   Accuracy: 0.91 (+/- 0.04) [naive Bayes]
   Accuracy: 0.95 (+/- 0.04) [Ensemble]


متوسط الاحتمالات المرجح (تصويت ناعم)
--------------------------------------------

على النقيض من التصويت بالأغلبية (تصويت صعب)، يعيد التصويت الناعم ملصق الفئة كـ argmax لمجموع الاحتمالات المتوقعة.

يمكن تعيين أوزان محددة لكل مصنف عبر معامل ``weights``. عند توفير الأوزان، يتم جمع احتمالات الفئة المتوقعة لكل مصنف، وضربها في وزن المصنف، ومتوسطها. بعد ذلك، يتم اشتقاق ملصق الفئة النهائي من ملصق الفئة بأعلى متوسط احتمال.

لتوضيح ذلك بمثال بسيط، لنفترض أن لدينا 3 مصنفات ومشاكل تصنيف من 3 فئات حيث نعين أوزانًا متساوية لجميع المصنفات: w1=1، w2=1، w3=1.

بعد ذلك، سيتم حساب متوسط الاحتمالات المرجح لعينة ما على النحو التالي:

================  ==========    ==========      ==========
المصنف           الفئة 1       الفئة 2         الفئة 3
================  ==========    ==========      ==========
المصنف 1	      w1 * 0.2      w1 * 0.5        w1 * 0.3
المصنف 2	      w2 * 0.6      w2 * 0.3        w2 * 0.1
المصنف 3          w3 * 0.3      w3 * 0.4        w3 * 0.3
المتوسط المرجح    0.37	        0.4             0.23
================  ==========    ==========      ==========

هنا، يكون ملصق الفئة المتوقع هو 2، لأنه يحتوي على أعلى متوسط احتمال.

يوضح المثال التالي كيف يمكن تغيير مناطق القرار عند استخدام :class:`VotingClassifier` الناعم بناءً على Support Vector Machine الخطي، وشجرة القرار، ومصنف K-nearest neighbor::

   >>> from sklearn import datasets
   >>> from sklearn.tree import DecisionTreeClassifier
   >>> from sklearn.neighbors import KNeighborsClassifier
   >>> from sklearn.svm import SVC
   >>> from itertools import product
   >>> from sklearn.ensemble import VotingClassifier

   >>> # Loading some example data
   >>> iris = datasets.load_iris()
   >>> X = iris.data[:, [0, 2]]
   >>> y = iris.target

   >>> # Training classifiers
   >>> clf1 = DecisionTreeClassifier(max_depth=4)
   >>> clf2 = KNeighborsClassifier(n_neighbors=7)
   >>> clf3 = SVC(kernel='rbf', probability=True)
   >>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],
   ...                         voting='soft', weights=[2, 1, 2])

   >>> clf1 = clf1.fit(X, y)
   >>> clf2 = clf2.fit(X, y)
   >>> clf3 = clf3.fit(X, y)
   >>> eclf = eclf.fit(X, y)

.. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_voting_decision_regions_001.png
    :target: ../auto_examples/ensemble/plot_voting_decision_regions.html
    :align: center
    :scale: 75%

الاستخدام

    

للتنبؤ بعلامات الفئة بناءً على احتمالات الفئة المتوقعة (يجب أن يدعم مقدرو سكيت-ليرن في فئة VotingClassifier طريقة ``predict_proba``)::

   >>> eclf = VotingClassifier(
   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
   ...     voting='soft'
   ... )

اختياريًا، يمكن توفير أوزان للمصنفات الفردية::

   >>> eclf = VotingClassifier(
   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
   ...     voting='soft', weights=[2,5,1]
   ... )

.. dropdown:: استخدام :class:`VotingClassifier` مع :class:`~sklearn.model_selection.GridSearchCV`

  يمكن أيضًا استخدام :class:`VotingClassifier` مع :class:`~sklearn.model_selection.GridSearchCV` من أجل ضبط المعلمات الفائقة للمقدرين الفرديين::

    >>> from sklearn.model_selection import GridSearchCV
    >>> clf1 = LogisticRegression(random_state=1)
    >>> clf2 = RandomForestClassifier(random_state=1)
    >>> clf3 = GaussianNB()
    >>> eclf = VotingClassifier(
    ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
    ...     voting='soft'
    ... )

    >>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}

    >>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
    >>> grid = grid.fit(iris.data, iris.target)

.. _voting_regressor:

مُقرِّب التصويت

(ملاحظة: لم تترجم الأقسام الفرعية لأنها لا تحتوي على نص بخلاف العنوان، وإذا كنت تريد ترجمة بقية المستند، فيرجى تزويدي به وسأكون سعيدًا بمساعدتك.)

الفكرة وراء :class:`VotingRegressor` هي الجمع بين مفاهيم مختلفة للمنبئات الانحدارية في التعلم الآلي وإرجاع المتوسط المتوقع للقيم. يمكن أن يكون هذا المنبئ الانحداري مفيدًا لمجموعة من النماذج عالية الأداء بشكل متساوٍ من أجل موازنة نقاط ضعفها الفردية.

الاستخدام
-----

يوضح المثال التالي كيفية تدريب VotingRegressor::

   >>> من sklearn.datasets استيراد load_diabetes
   >>> من sklearn.ensemble استيراد GradientBoostingRegressor
   >>> من sklearn.ensemble استيراد RandomForestRegressor
   >>> من sklearn.linear_model استيراد LinearRegression
   >>> من sklearn.ensemble استيراد VotingRegressor

   >>> # تحميل بعض البيانات المثال
   >>> X، y = load_diabetes(return_X_y=True)

   >>> # تدريب المصنفات
   >>> reg1 = GradientBoostingRegressor(random_state=1)
   >>> reg2 = RandomForestRegressor(random_state=1)
   >>> reg3 = LinearRegression()
   >>> ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])
   >>> ereg = ereg.fit(X, y)

.. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_voting_regressor_001.png
    :target: ../auto_examples/ensemble/plot_voting_regressor.html
    :align: center
    :scale: 75%

.. rubric:: الأمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_voting_regressor.py`

.. _stacking:

تكديس التعميم

    

التكديس العام هو طريقة للجمع بين المقدِّرات لتقليل تحيزاتها [W1992] [HTF]_. على وجه التحديد، يتم تكديس توقعات كل مقدِّر فردي معًا واستخدامها كمدخلات لمقدِّر نهائي لحساب التوقع. يتم تدريب هذا المقدِّر النهائي من خلال التحقق المتقاطع.

توفر :class:`StackingClassifier` و :class:`StackingRegressor` هذه الاستراتيجيات التي يمكن تطبيقها على مشاكل التصنيف والانحدار.

معلمة `estimators` تتوافق مع قائمة المقدِّرات التي يتم تكديسها معًا بالتوازي على بيانات الإدخال. يجب إعطاؤها كقائمة من الأسماء والمقدِّرات::

  >>> from sklearn.linear_model import RidgeCV, LassoCV
  >>> from sklearn.neighbors import KNeighborsRegressor
  >>> estimators = [('ridge', RidgeCV()),
  ...               ('lasso', LassoCV(random_state=42)),
  ...               ('knr', KNeighborsRegressor(n_neighbors=20,
  ...                                           metric='euclidean'))]

سيستخدم `final_estimator` توقعات `estimators` كمدخلات. يجب أن يكون مصنفًا أو أداة اسناد عند استخدام :class:`StackingClassifier` أو :class:`StackingRegressor`، على التوالي::

  >>> from sklearn.ensemble import GradientBoostingRegressor
  >>> from sklearn.ensemble import StackingRegressor
  >>> final_estimator = GradientBoostingRegressor(
  ...     n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,
  ...     random_state=42)
  >>> reg = StackingRegressor(
  ...     estimators=estimators,
  ...     final_estimator=final_estimator)

لتدريب `estimators` و `final_estimator`، يجب استدعاء طريقة `fit` على بيانات التدريب::

  >>> from sklearn.datasets import load_diabetes
  >>> X, y = load_diabetes(return_X_y=True)
  >>> from sklearn.model_selection import train_test_split
  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,
  ...                                                     random_state=42)
  >>> reg.fit(X_train, y_train)
  StackingRegressor(...)

أثناء التدريب، يتم تركيب `estimators` على كل بيانات التدريب `X_train`. سيتم استخدامها عند استدعاء `predict` أو `predict_proba`. للتعميم وتجنب الإفراط في التجهيز، يتم تدريب `final_estimator` على العينات الخارجية باستخدام :func:`sklearn.model_selection.cross_val_predict` داخليًا.

بالنسبة إلى :class:`StackingClassifier`، لاحظ أن إخراج ``estimators`` يتم التحكم فيه بواسطة معلمة `stack_method` ويتم استدعاؤه بواسطة كل مقدِّر. هذه المعلمة هي إما سلسلة، أي أسماء طرق المقدِّر، أو `'auto'` الذي سيساعد في التعرف تلقائيًا على طريقة متاحة بناءً على التوافر، واختبارها حسب ترتيب التفضيل: `predict_proba`، `decision_function` و `predict`.

يمكن استخدام :class:`StackingRegressor` و :class:`StackingClassifier` كأي أداة اسناد أو مصنف آخر، مع كشف طريقة `predict` أو `predict_proba` أو `decision_function`، على سبيل المثال::

   >>> y_pred = reg.predict(X_test)
   >>> from sklearn.metrics import r2_score
   >>> print('R2 score: {:.2f}'.format(r2_score(y_test, y_pred)))
   R2 score: 0.53

لاحظ أنه من الممكن أيضًا الحصول على مخرجات `estimators` المكدسة باستخدام طريقة `transform`::

  >>> reg.transform(X_test[:5])
 array([[142..., 138..., 146...],
         [179..., 182..., 151...],
         [139..., 132..., 158...],
         [286..., 292..., 225...],
         [126..., 124..., 164...]])

في الممارسة العملية، يتوقع المنبئ التكديسي جيدًا مثل أفضل منبئ للطبقة الأساسية وحتى في بعض الأحيان يتفوق عليه من خلال الجمع بين نقاط القوة المختلفة لهذه التنبؤات. ومع ذلك، فإن تدريب منبئ التكديس مكلف حسابيًا.

.. note::
   بالنسبة إلى :class:`StackingClassifier`، عند استخدام `stack_method_='predict_proba'`، يتم إسقاط العمود الأول عند وجود مشكلة تصنيف ثنائي. في الواقع، كلا عمودَي الاحتمال المتوقعان من قبل كل مقدِّر مترابطان تمامًا.

.. note::
   يمكن تحقيق طبقات التكديس المتعددة عن طريق تعيين `final_estimator` إلى :class:`StackingClassifier` أو :class:`StackingRegressor`::

    >>> final_layer_rfr = RandomForestRegressor(
    ...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)
    >>> final_layer_gbr = GradientBoostingRegressor(
    ...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)
    >>> final_layer = StackingRegressor(
    ...     estimators=[('rf', final_layer_rfr),
    ...                 ('gbrt', final_layer_gbr)],
    ...     final_estimator=RidgeCV()
    ...     )
    >>> multi_layer_regressor = StackingRegressor(
    ...     estimators=[('ridge', RidgeCV()),
    ...                 ('lasso', LassoCV(random_state=42)),
    ...                 ('knr', KNeighborsRegressor(n_neighbors=20,
    ...                                             metric='euclidean'))],
    ...     final_estimator=final_layer
    ... )
    >>> multi_layer_regressor.fit(X_train, y_train)
    StackingRegressor(...)
    >>> print('R2 score: {:.2f}'
    ...       .format(multi_layer_regressor.score(X_test, y_test)))
    R2 score: 0.53

.. rubric:: المراجع

.. [W1992] وولبرت، ديفيد إتش "التعميم المكدس." الشبكات العصبية 5.2 (1992): 241-259.



.. _adaboost:

آدا بوست

    

  الوحدة :mod:`sklearn.ensemble` تشمل خوارزمية AdaBoost المشهورة، والتي تم تقديمها عام 1995 من قبل فريوند وشابيرو [FS1995]_.

    المبدأ الأساسي لـ AdaBoost هو تركيب سلسلة من المتعلمين الضعفاء (أي النماذج التي تكون أفضل قليلاً من التخمين العشوائي، مثل أشجار القرار الصغيرة) على نسخ معدلة بشكل متكرر من البيانات. ثم يتم دمج التنبؤات من جميعها من خلال تصويت الأغلبية الموزون (أو المجموع) لإنتاج التنبؤ النهائي. تتكون تعديلات البيانات في كل تكرار يسمى تكرار التعزيز من تطبيق الأوزان :math:`w_1`، :math:`w_2`، ...، :math:`w_N` على كل من عينات التدريب. في البداية، يتم ضبط جميع هذه الأوزان على :math:`w_i = 1/N`، بحيث تقوم الخطوة الأولى ببساطة بتدريب متعلم ضعيف على البيانات الأصلية. لكل تكرار لاحق، يتم تعديل أوزان العينات بشكل فردي ويتم إعادة تطبيق خوارزمية التعلم على البيانات الموزونة مجدداً. في خطوة معينة، يتم زيادة أوزان عينات التدريب التي تم التنبؤ بها بشكل غير صحيح بواسطة النموذج المعزز المستحث في الخطوة السابقة، في حين يتم إنقاص الأوزان للعينات التي تم التنبؤ بها بشكل صحيح. مع تقدم التكرارات، تتلقى الأمثلة الصعبة للتنبؤ تأثيراً متزايداً. وبالتالي، يُجبر كل متعلم ضعيف لاحق على التركيز على الأمثلة التي فاتتها المتعلمات السابقة في التسلسل [HTF]_.

    .. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_adaboost_multiclass_001.png
       :target: ../auto_examples/ensemble/plot_adaboost_multiclass.html
       :align: center
       :scale: 75

    يمكن استخدام AdaBoost لكل من مشاكل التصنيف والانحدار:

    - بالنسبة للتصنيف متعدد الفئات، تقوم :class:`AdaBoostClassifier` بتنفيذ AdaBoost.SAMME [ZZRH2009]_.

    - بالنسبة للانحدار، تقوم :class:`AdaBoostRegressor` بتنفيذ AdaBoost.R2 [D1997]_.

    الاستخدام
    -----

    يظهر المثال التالي كيفية تركيب مصنف AdaBoost مع 100 متعلم ضعيف::

        >>> from sklearn.model_selection import cross_val_score
        >>> from sklearn.datasets import load_iris
        >>> from sklearn.ensemble import AdaBoostClassifier

        >>> X, y = load_iris(return_X_y=True)
        >>> clf = AdaBoostClassifier(n_estimators=100, algorithm="SAMME",)
        >>> scores = cross_val_score(clf, X, y, cv=5)
        >>> scores.mean()
        0.9...

    يتم التحكم في عدد المتعلمين الضعفاء بواسطة المعلمة ``n_estimators``. يتحكم معامل ``learning_rate`` في مساهمة المتعلمين الضعفاء في المجموعة النهائية. بشكل افتراضي، المتعلمين الضعفاء هم قرارات جذعية. يمكن تحديد متعلمين ضعفاء مختلفين من خلال معامل ``estimator``.
    المعلمات الرئيسية التي يجب ضبطها للحصول على نتائج جيدة هي ``n_estimators`` وتعقيد مقدري الأساس (مثل عمقها ``max_depth`` أو الحد الأدنى لعدد العينات المطلوب مراعاتها في الانقسام ``min_samples_split``).

    .. rubric:: أمثلة

    * :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_multiclass.py` يُظهر أداء AdaBoost على مشكلة متعددة الفئات.

    * :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_twoclass.py` يُظهر حد القرار وقيم دالة القرار لمشكلة ثنائية الفئة غير قابلة للفصل خطياً باستخدام AdaBoost-SAMME.

    * :ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_regression.py` يُظهر الانحدار باستخدام خوارزمية AdaBoost.R2.

    .. rubric:: المراجع

    .. [FS1995] Y. Freund, and R. Schapire, "A Decision-Theoretic Generalization of
       On-Line Learning and an Application to Boosting", 1997.

    .. [ZZRH2009] J. Zhu, H. Zou, S. Rosset, T. Hastie. "Multi-class AdaBoost", 2009.

    .. [D1997] H. Drucker. "Improving Regressors using Boosting Techniques", 1997.

    .. [HTF] T. Hastie, R. Tibshirani and J. Friedman, "Elements of Statistical Learning
       Ed. 2", Springer, 2009.
