طريقة التجميع: Gradient boosting, random forests, bagging, voting, stacking

**طرق التجميع** تجمع تنبؤات العديد من المُقدرات الأساسية التي تم بناؤها باستخدام خوارزمية تعلم معينة من أجل تحسين قابلية التعميم/الصلابة مقارنة بمقدر واحد.

هناك مثالان شهيران جدا لطرق التجميع هما: Gradient-boosted trees و Random forests.

وبشكل أكثر عمومية، يمكن تطبيق نماذج التجميع على أي متعلم أساسي بخلاف الأشجار، في طرق المتوسطات مثل Bagging methods و model stacking أو Voting، أو في التعزيز، كما هو الحال في AdaBoost.
طريقة "غراديانت تري بوستينغ" أو "غراديانت بوستيد ديكيزن تريز" (جى بى دي تي) هي تعميم لطريقة "البوستينغ" باستخدام دالة خسارة قابلة للاشتقاق، انظر إلى العمل الرائد لـ [Friedman2001]. تعتبر طريقة جى بى دي تي نموذجًا ممتازًا لكل من الانحدار والتصنيف، خاصةً لبيانات الجداول.

**موضوع:** :class:`GradientBoostingClassifier` مقابل :class:`HistGradientBoostingClassifier`:

يوفر سكيت-ليرن تنفيذين لطريقة الجرديانت-بوستيد تريز: :class:`HistGradientBoostingClassifier` مقابل :class:`GradientBoostingClassifier` للتصنيف، وفئات مماثلة للانحدار. يمكن أن يكون الأول أسرع من الأخير **بأضعاف** عندما يكون عدد العينات أكبر من عشرات الآلاف.

تدعم النسخة Hist... بشكلٍ أصيل القيم المفقودة والبيانات الفئوية، مما يلغي الحاجة إلى معالجة مسبقة إضافية مثل الاستيفاء.

قد يُفضل استخدام :class:`GradientBoostingClassifier` و :class:`GradientBoostingRegressor` لعينات صغيرة الحجم، لأن التصنيف إلى فئات قد يؤدي إلى نقاط انقسام تقريبية للغاية في هذا السياق.

.. _histogram_based_gradient_boosting:

طريقة Gradient Boosting المستندة إلى التصنيف
قدمت Scikit-learn 0.21 تنفيذين جديدين لشجرة Gradient Boosted، وهما على وجه التحديد: class: 'HistGradientBoostingClassifier' و class: 'HistGradientBoostingRegressor'، المستوحاة من 'LightGBM' (راجع [LightGBM]).

يمكن أن تكون هذه المقدرات القائمة على المخطط التكراري أسرع **بمقدار كبير** من class: 'GradientBoostingClassifier' و class: 'GradientBoostingRegressor' عندما يكون عدد العينات أكبر من عشرات الآلاف من العينات.

كما أن لديها دعم مدمج للقيم المفقودة، مما يلغي الحاجة إلى imputer.

تقوم هذه المقدرات السريعة أولاً بتقسيم عينات الإدخال "X" إلى صناديق ذات قيم صحيحة (عادة 256 صندوقًا) مما يقلل بشكل كبير من عدد نقاط الانقسام التي يجب مراعاتها، ويتيح للخوارزمية الاستفادة من البنى البيانات المعتمدة على الأعداد الصحيحة (المخططات التكرارية) بدلاً من الاعتماد على القيم المستمرة المرتبة عند بناء الأشجار. تختلف واجهة برمجة التطبيقات الخاصة بهذه المقدرات قليلاً، ولا يتم دعم بعض الميزات من class: 'GradientBoostingClassifier' و class: 'GradientBoostingRegressor' بعد، على سبيل المثال بعض دالات الخسارة.

.. rubric:: أمثلة

* :ref: 'sphx_glr_auto_examples_inspection_plot_partial_dependence.py'
* :ref: 'sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py'

الاستخدام
^^^^^^^

لا تتغير معظم المعلمات من class: 'GradientBoostingClassifier' و class: 'GradientBoostingRegressor'.

الاستثناء هو معلمة "max_iter" التي تحل محل "n_estimators"، وتتحكم في عدد تكرارات عملية التعزيز::

  >>> from sklearn.ensemble import HistGradientBoostingClassifier
  >>> from sklearn.datasets import make_hastie_10_2

  >>> X, y = make_hastie_10_2(random_state=0)
  >>> X_train, X_test = X[:2000], X[2000:]
  >>> y_train, y_test = y[:2000], y[2000:]

  >>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)
  >>> clf.score(X_test, y_test)
  0.8965

تتوفر خسائر **الانحدار** التالية:

- 'squared_error'، وهو دالة الخسارة الافتراضية؛
- 'absolute_error'، وهو أقل حساسية للقيم الشاذة من الخطأ المربع؛
- 'gamma'، وهو مناسب لنمذجة النتائج الإيجابية الصارمة؛
- 'poisson'، وهو مناسب لنمذجة العد والترددات؛
- 'quantile'، والذي يسمح بتقدير كمية شرطية يمكن استخدامها لاحقًا للحصول على فترات تنبؤ.

بالنسبة لل**تصنيف**، فإن 'log_loss' هو الخيار الوحيد. بالنسبة للتصنيف الثنائي، فإنه يستخدم دالة الخسارة اللوغاريتمية الثنائية، المعروفة أيضًا باسم الانحراف الثنائي أو الانتروبيا الثنائية. بالنسبة لـ 'n_classes >= 3'، فإنه يستخدم دالة الخسارة اللوغاريتمية متعددة الفئات، مع الانحراف متعدد الحدود والانتروبيا الفئوية كأسماء بديلة. يتم تحديد إصدار الخسارة المناسب بناءً على 'y' التي تم تمريرها إلى 'fit'.

يمكن التحكم في حجم الأشجار من خلال معلمات "max_leaf_nodes" و "max_depth" و "min_samples_leaf".

يتم التحكم في عدد الصناديق المستخدمة لتقسيم البيانات بواسطة معلمة "max_bins". إن استخدام صناديق أقل يعمل كشكل من أشكال التنظيم. يوصى عمومًا باستخدام أكبر عدد ممكن من الصناديق (255)، وهو الافتراضي.

تعمل معلمة "l2_regularization" كمنظم لدالة الخسارة، وتتوافق مع 'lambda' في التعبير التالي (راجع المعادلة (2) في [XGBoost]):

.. math::

  \mathcal{L}(\phi) = \sum_i l(\hat{y}_i, y_i) + \frac12 \sum_k \lambda ||w_k||^2

.. dropdown:: تفاصيل عن التنظيم L2

  من المهم ملاحظة أن مصطلح الخسارة: math: 'l (\ hat {y} _i، y_i)' يصف
  نصف دالة الخسارة الفعلية فقط باستثناء الخسارة pinball والخطأ المطلق.

  يشير الفهرس: math: 'k' إلى الشجرة k-th في مجموعة الأشجار. في
  حالة مشكلات الانحدار والتصنيف الثنائي، تنمو نماذج التعزيز التدريجي شجرة واحدة
  لكل تكرار، ثم: math: 'k' يعمل حتى 'max_iter'. في حالة
  مشكلات التصنيف متعدد الفئات، تكون القيمة القصوى لفهرس: math: 'k' هي
  'n_classes' :math: '\ times` 'max_iter'.

  إذا: math: 'T_k' يمثل عدد الأوراق في الشجرة k-th، ثم: math: 'w_k'
  هو متجه بطول: math: 'T_k'، والذي يحتوي على قيم الأوراق على شكل 'w
  = -sum_gradient / (sum_hessian + l2_regularization)' (راجع المعادلة (5) في
  [XGBoost]).

  يتم اشتقاق قيم الأوراق: math: 'w_k' عن طريق قسمة مجموع تدرجات
  دالة الخسارة على مجموع مجموع القيم الذاتية. يضيف إضافة التنظيم إلى
  المقام عقوبة على الأوراق ذات القيم الذاتية الصغيرة (المناطق المسطحة)،
  مما يؤدي إلى تحديثات أصغر. ثم تساهم قيم: math: 'w_k' هذه في
  تنبؤ النموذج لعينة معينة تنتهي في الورقة المقابلة. التنبؤ النهائي هو
  مجموع التنبؤ الأساسي ومساهمات كل شجرة. نتيجة هذا المجموع هي
  ثم يتم تحويلها بواسطة دالة الرابط العكسي اعتمادًا على اختيار دالة الخسارة (راجع
  :ref: 'gradient_boosting_formulation').

  لاحظ أن الورقة الأصلية [XGBoost]_ تقدم مصطلحًا: math: '\ gamma \ sum_k
  T_k' الذي يعاقب عدد الأوراق (مما يجعلها نسخة سلسة من
  'max_leaf_nodes') غير مقدمة هنا نظرًا لعدم تنفيذها في scikit-learn؛ في حين أن: math: '\ lambda' يعاقب على حجم تنبؤات الشجرة الفردية قبل إعادة تحجيمها بمعدل التعلم، راجع
  :ref: 'gradient_boosting_shrinkage'.


لاحظ أن **التوقف المبكر ممكن بشكل افتراضي إذا كان عدد العينات أكبر من 10000**. يتم التحكم في سلوك التوقف المبكر من خلال معلمات "early_stopping" و "scoring" و "validation_fraction" و "n_iter_no_change" و "tol". من الممكن التوقف المبكر باستخدام أي مصنف، أو مجرد خسارة التدريب أو التحقق من الصحة. لاحظ أنه لأسباب فنية، يكون استخدام مصنف قابل للاستدعاء أبطأ بكثير من استخدام الخسارة. بشكل افتراضي، يتم تنفيذ التوقف المبكر إذا كان هناك ما لا يقل عن 10000 عينة في مجموعة التدريب، باستخدام خسارة التحقق من الصحة.

.. _nan_support_hgbt:

دعم القيم المفقودة
^^^^^^^^^^^^^^^^^^^^^^

لدى class: 'HistGradientBoostingClassifier' و class: 'HistGradientBoostingRegressor' دعم مدمج للقيم المفقودة (NaNs).

أثناء التدريب، يتعلم منشئ الشجرة في كل نقطة انقسام ما إذا كان يجب إرسال العينات ذات القيم المفقودة إلى الطفل الأيسر أو الأيمن، بناءً على المكسب المحتمل. عند التنبؤ، يتم تعيين العينات ذات القيم المفقودة إلى الطفل الأيسر أو الأيمن بناءً على ذلك::

  >>> from sklearn.ensemble import HistGradientBoostingClassifier
  >>> import numpy as np

  >>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)
  >>> y = [0, 0, 1, 1]

  >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)
  >>> gbdt.predict(X)
  array([0, 0, 1, 1])

عندما يكون نمط القيم المفقودة تنبئيًا، يمكن إجراء الانقسامات بناءً على ما إذا كانت قيمة الميزة مفقودة أم لا::

  >>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)
  >>> y = [0, 1, 0, 0, 1]
  >>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,
  ...                                       max_depth=2,
  ...                                       learning_rate=1,
  ...                                       max_iter=1).fit(X, y)
  >>> gbdt.predict(X)
  array([0, 1, 0, 0, 1])

إذا لم يتم العثور على أي قيم مفقودة لميزة معينة أثناء التدريب، يتم تعيين العينات ذات القيم المفقودة إلى الطفل الذي يحتوي على معظم العينات.

.. rubric:: أمثلة

* :ref: 'sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py'

.. _sw_hgbdt:

دعم وزن العينة
^^^^^^^^^^^^^^^^^^^^^

يدعم class: 'HistGradientBoostingClassifier' و class: 'HistGradientBoostingRegressor' أوزان العينات أثناء: term: 'fit'.

يوضح المثال التالي كيف يتم تجاهل العينات ذات وزن العينة صفر:

    >>> X = [[1, 0],
    ...      [1, 0],
    ...      [1, 0],
    ...      [0, 1]]
    >>> y = [0, 0, 1, 0]
    >>> # تجاهل أول عينات تدريب 2 من خلال تعيين أوزانها إلى 0
    >>> sample_weight = [0, 0, 1, 1]
    >>> gb = HistGradientBoostingClassifier(min_samples_leaf=1)
    >>> gb.fit(X، y، sample_weight=sample_weight)
    HistGradientBoostingClassifier (...)
    >>> gb.predict ([[1،0]])
    array ([1])
    >>> gb.predict_proba ([[1،0]]) [0،1]
    0.99 ...

كما ترون، يتم تصنيف '[1، 0]' بشكل مريح على أنه '1' نظرًا لتجاهل أول عينات تدريب 2 بسبب أوزان العينات الخاصة بها.

تفاصيل التنفيذ: إن أخذ أوزان العينات في الاعتبار يعادل ضرب التدرجات (والقيم الذاتية) بأوزان العينات. لاحظ أن مرحلة التصنيف (بشكل خاص حساب الكميات) لا تأخذ الأوزان في الاعتبار.

.. _categorical_support_gbdt:

دعم الميزات الفئوية
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

لدى class: 'HistGradientBoostingClassifier' و class: 'HistGradientBoostingRegressor' دعم أصلي للميزات الفئوية: يمكنهما مراعاة الانقسامات على البيانات الفئوية غير المرتبة.

بالنسبة لمجموعات البيانات التي تحتوي على ميزات فئوية، غالبًا ما يكون من الأفضل استخدام الدعم الفئوي الأصلي بدلاً من الترميز أحادي الساخن (: class: '~sklearn.preprocessing.OneHotEncoder')، لأن الترميز أحادي الساخن يتطلب المزيد من عمق الشجرة لتحقيق انقسامات مكافئة. من الأفضل عادةً الاعتماد على الدعم الفئوي الأصلي بدلاً من التعامل مع الميزات الفئوية على أنها مستمرة (ترتيبية)، والتي تحدث لبيانات الميزات الفئوية الترتيبية، حيث أن الفئات هي كميات اسمية لا يهم فيها الترتيب.

لتمكين الدعم الفئوي، يمكن تمرير قناع منطقي إلى معلمة "categorical_features"، مما يشير إلى ما إذا كانت الميزة فئوية أم لا. في ما يلي، سيتم التعامل مع الميزة الأولى على أنها فئوية والثانية على أنها رقمية::

  >>> gbdt = HistGradientBoostingClassifier(categorical_features=[True, False])

وبالمثل، يمكن تمرير قائمة من الأعداد الصحيحة التي تشير إلى فهارس الميزات الفئوية::

  >>> gbdt = HistGradientBoostingClassifier(categorical_features=[0])

عندما يكون الإدخال عبارة عن DataFrame، يمكن أيضًا تمرير قائمة بأسماء الأعمدة::

  >>> gbdt = HistGradientBoostingClassifier(categorical_features=["site"، "manufacturer"])

أخيرًا، عندما يكون الإدخال عبارة عن DataFrame، يمكننا استخدام "categorical_features"="from_dtype" في هذه الحالة، سيتم التعامل مع جميع الأعمدة ذات النوع الفئوي 'dtype' على أنها ميزات فئوية.

يجب أن تكون قابلية كل ميزة فئوية أقل من معلمة "max_bins". للحصول على مثال على استخدام التعزيز التدريجي القائم على المخطط التكراري على الميزات الفئوية، راجع
:ref: 'sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py'.

إذا كانت هناك قيم مفقودة أثناء التدريب، فسيتم التعامل مع القيم المفقودة على أنها فئة صحيحة. إذا لم تكن هناك قيم مفقودة أثناء التدريب، فسيتم تعيين القيم المفقودة عند التنبؤ إلى العقدة الفرعية التي تحتوي على معظم العينات (مثل الميزات المستمرة). عند التنبؤ، يتم التعامل مع الفئات التي لم يتم رؤيتها أثناء وقت التجهيز على أنها قيم مفقودة.

.. dropdown:: العثور على الانقسامات مع الميزات الفئوية

  الطريقة المعتادة للنظر في الانقسامات الفئوية في شجرة هي النظر في جميع الأقسام: math: '2 ^ {K - 1} - 1'، حيث: math: 'K' هو عدد
  الفئات. يمكن أن يصبح هذا الأمر سريعًا محظورًا عندما يكون: math: 'K' كبيرًا. لحسن الحظ، نظرًا لأن أشجار التعزيز التدريجي هي دائمًا أشجار انحدار (حتى
  لمشكلات التصنيف)، توجد استراتيجية أسرع يمكن أن تؤدي إلى انقسامات مكافئة. أولاً، يتم فرز الفئات الخاصة بميزة ما وفقًا لتغاير هدف التباين، لكل فئة 'k'. بمجرد فرز الفئات، يمكن اعتبار *التقسيمات المستمرة*، أي معاملة الفئات كما لو كانت قيمًا مستمرة مرتبة (راجع Fisher [Fisher1958]_ لإثبات رسمي). ونتيجة لذلك، يجب مراعاة: math: 'K - 1' فقط من الانقسامات بدلاً من: math: '2 ^ {K - 1} - 1'. الفرز الأولي هو عملية: math: '\ mathcal {O} (K \ log (K))'، مما يؤدي إلى تعقيد إجمالي قدره: math: '\ mathcal {O} (K \ log (K) + K)'، بدلاً من: math: 'O (2 ^ K)'.

.. rubric:: أمثلة

* :ref: 'sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py'

.. _monotonic_cst_gbdt:

القيود الأحادية الاتجاه
^^^^^^^^^^^^^^^^^^^^^
فيما يلي ترجمة للنص المحدد بتنسيق RST إلى اللغة العربية، مع اتباع التعليمات المذكورة:

اعتمادًا على المشكلة التي بين يديك، قد يكون لديك معرفة مسبقة تشير إلى أن سمة معينة يجب أن يكون لها تأثير إيجابي (أو سلبي) بشكل عام على القيمة المستهدفة. على سبيل المثال، مع ثبات العوامل الأخرى، يجب أن يؤدي ارتفاع درجة الائتمان إلى زيادة احتمال الموافقة على القرض. تسمح القيود الاحادية الاتجاه بدمج مثل هذه المعرفة المسبقة في النموذج.

بالنسبة لمتنبئ :math: 'F' بميزتين:

- قيد **الزيادة الأحادية الاتجاه** هو قيد على الشكل:

  .. math::

      x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2)

- قيد **الانخفاض الأُحادي** هو قيد على الشكل:

  .. math::

      x_1 \leq x_1' \implies F(x_1, x_2) \geq F(x_1', x_2)

يمكنك تحديد قيد أحادي الاتجاه لكل ميزة باستخدام معلمة 'monotonic_cst'. بالنسبة لكل ميزة، تشير القيمة 0 إلى عدم وجود قيد، في حين تشير القيمتان 1 و-1 على التوالي إلى قيد الزيادة الأحادية الاتجاه وقيد الانخفاض الأُحادي::

  >>> from sklearn.ensemble import HistGradientBoostingRegressor

  ... # قيد الزيادة الأحادية الاتجاه، والانخفاض الأُحادي، وعدم وجود قيد على الميزات الثلاث
  >>> gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])

في سياق التصنيف الثنائي، يعني فرض قيد الزيادة الأحادية (الانخفاض الأُحادي) أن القيم الأعلى للميزة يفترض أن يكون لها تأثير إيجابي (سلبي) على احتمال انتماء العينات إلى الفئة الإيجابية.

ومع ذلك، فإن القيود الأحادية الاتجاه تقيد بشكل هامشي فقط تأثيرات الميزة على الإخراج. على سبيل المثال، لا يمكن استخدام قيود الزيادة والانخفاض الأحادية الاتجاه لفرض قيد النمذجة التالي:

.. math::

    x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2')

كما أن القيود الأحادية الاتجاه غير مدعومة للتصنيف متعدد الفئات.

.. note::

    نظرًا لأن الفئات هي كميات غير مرتبة، فمن غير الممكن فرض قيود أحادية الاتجاه على الميزات الفئوية.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_monotonic_constraints.py`
* :ref:`sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py`

.. _interaction_cst_hgbt:

قيود التفاعل
^^^^^^^^^^^^^^^^^^^^^^^

بداهة، يُسمح لأشجار التدرج المُعزز بالهستوجرام باستخدام أي ميزة لتقسيم العقدة إلى عقد أطفال. وهذا يخلق ما يسمى بالتفاعلات بين الميزات، أي استخدام ميزات مختلفة كتقسيم على طول فرع. في بعض الأحيان، يرغب المرء في تقييد التفاعلات الممكنة، انظر [Mayer2022] _. يمكن القيام بذلك باستخدام معلمة "interaction_cst"، حيث يمكنك تحديد مؤشرات الميزات المسموح لها بالتفاعل.
على سبيل المثال، مع 3 ميزات في المجموع، "interaction_cst = [{0}، {1}، {2}]" يحظر جميع التفاعلات.
تُحدد القيود "[{0، 1}، {1، 2}]" مجموعتين من الميزات التي يمكن أن تتفاعل. قد تتفاعل الميزتان 0 و1 مع بعضهما البعض، وكذلك الميزتان 1 و2. ولكن لاحظ أنه لا يُسمح للميزتين 0 و2 بالتفاعل.
يوضح ما يلي شجرة والانقسامات الممكنة للشجرة:

.. code-block:: none

      1      <- يمكن تطبيق كل من مجموعات القيود من الآن فصاعدًا
     / \
    1   2    <- الانقسام الأيسر لا يزال يفي بمجموعات القيود كليهما.
   / \ / \      الانقسام الأيمن عند الميزة 2 له فقط المجموعة {1، 2} من الآن فصاعدًا.

يستخدم LightGBM نفس المنطق للمجموعات المتداخلة.

لاحظ أن الميزات غير المدرجة في "interaction_cst" يتم تلقائيًا تعيين مجموعة تفاعل لها. مع 3 ميزات مرة أخرى، وهذا يعني أن "[{0}]" مكافئ لـ "[{0}، {1، 2}]"

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_inspection_plot_partial_dependence.py`

.. rubric:: مراجع

.. [Mayer2022] M. Mayer, S.C. Bourassa, M. Hoesli, and D.F. Scognamiglio.
    2022. :doi:`تطبيقات التعلم الآلي على تقييم الأراضي والمباني
    <10.3390/jrfm15050193>`.
    Journal of Risk and Financial Management 15, no. 5: 193

التوازي منخفض المستوى
^^^^^^^^^^^^^^^^^^^^^


:class:'HistGradientBoostingClassifier' و
:class:'HistGradientBoostingRegressor' يستخدم OpenMP
للتوازي من خلال Cython. لمزيد من التفاصيل حول كيفية التحكم في
عدد الخيوط، يرجى الرجوع إلى ملاحظاتنا: ref:`parallelism`.

الأجزاء التالية متوازية:

- رسم خريطة العينات من القيم الحقيقية إلى العلب ذات القيم الصحيحة (ومع ذلك، فإن العثور على عتبات العلب تسلسلي)
- بناء الهستوجرامات متوازي عبر الميزات
- العثور على أفضل نقطة انقسام في العقدة متوازية عبر الميزات
- أثناء التثبيت، يتم رسم خريطة العينات إلى العقد اليسرى واليمنى متوازية عبر العينات
- حسابات التدرج والهيسيان متوازية عبر العينات
- التنبؤ متوازي عبر العينات

.. _Why_it's_faster:

لماذا هو أسرع
^^^^^^^^^^^^^^^

عنق الزجاجة في إجراء التعزيز التدريجي هو بناء أشجار القرار. يتطلب بناء شجرة قرار تقليدية (كما هو الحال في GBDTs الأخرى)
:class:'GradientBoostingClassifier' و: class:'GradientBoostingRegressor')
فرز العينات في كل عقدة (لكل ميزة). الفرز مطلوب حتى يمكن حساب المكاسب المحتملة لنقطة الانقسام بكفاءة. وبالتالي، فإن تقسيم عقدة واحدة له تعقيد
:math:`\mathcal{O}(n_\text {features} \times n \log (n))` حيث :math:`n`
هو عدد العينات في العقدة.

:class:'HistGradientBoostingClassifier' و
:class:'HistGradientBoostingRegressor'، من ناحية أخرى، لا تتطلب فرز قيم الميزة وتستخدم بدلاً من ذلك بنية بيانات تسمى الهستوجرام، حيث يتم ترتيب العينات ضمنيًا.
للبناء الهستوجرام تعقيد :math:`\mathcal{O}(n)`، لذا فإن إجراء تقسيم العقدة له تعقيد
:math:`\mathcal{O}(n_\text{features} \times n)`، وهو أصغر بكثير من السابق. بالإضافة إلى ذلك، بدلاً من النظر في :math:`n`
نقاط الانقسام، فإننا نأخذ في الاعتبار فقط "max_bins" نقاط الانقسام، والتي قد تكون أصغر بكثير.

من أجل بناء الهستوجرامات، يجب تصنيف بيانات الإدخال 'X' في علب ذات قيم صحيحة. يتطلب إجراء التصنيف هذا فرز قيم الميزة، ولكنه يحدث مرة واحدة فقط في بداية عملية التعزيز (وليس في كل عقدة، مثل
:class:'GradientBoostingClassifier' و: class:'GradientBoostingRegressor').

أخيرًا، يتم توازي العديد من أجزاء تنفيذ
:class:'HistGradientBoostingClassifier' و
:class:'HistGradientBoostingRegressor'.

.. rubric:: مراجع

.. [XGBoost] Tianqi Chen, Carlos Guestrin, :arxiv:`"XGBoost: A Scalable Tree
   Boosting System" <1603.02754>`

.. [LightGBM] Ke et. al. `"LightGBM: A Highly Efficient Gradient
   BoostingDecision Tree" <https://papers.nips.cc/paper/
   6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree>`_

.. [Fisher1958] Fisher, W.D. (1958). `"On Grouping for Maximum Homogeneity"
   <http://csiss.ncgia.ucsb.edu/SPACE/workshops/2004/SAC/files/fisher.pdf>`_
   Journal of the American Statistical Association, 53, 789-798.



:class:'GradientBoostingClassifier' و: class:'GradientBoostingRegressor'
فيما يلي وصف لاستخدام ومعلمات :class:`GradientBoostingClassifier` و :class:`GradientBoostingRegressor`. أهم معلمتين في هاتين الأداتين التقديريتين هما `n_estimators` و `learning_rate`.

.. dropdown:: التصنيف

  تدعم :class:`GradientBoostingClassifier` التصنيف الثنائي والمتعدد الفئات.
  يوضح المثال التالي كيفية ملاءمة مصنف التعزيز التدريجي
  باستخدام 100 من أشجار القرار الضعيفة كمتعلمين ضعفاء::

      >>> from sklearn.datasets import make_hastie_10_2
      >>> from sklearn.ensemble import GradientBoostingClassifier

      >>> X, y = make_hastie_10_2(random_state=0)
      >>> X_train, X_test = X[:2000], X[2000:]
      >>> y_train, y_test = y[:2000], y[2000:]

      >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
      ...     max_depth=1, random_state=0).fit(X_train, y_train)
      >>> clf.score(X_test, y_test)
      0.913...

  يتم التحكم في عدد المتعلمين الضعفاء (أي أشجار القرار) بواسطة معلمة
  ``n_estimators``؛ يمكن التحكم في حجم كل شجرة إما عن طريق ضبط عمق الشجرة
  باستخدام ``max_depth`` أو عن طريق ضبط عدد العقد الورقية باستخدام
  ``max_leaf_nodes``. ``learning_rate`` هو معلمة فائقة القيمة في النطاق
  (0.0، 1.0] تتحكم في الإفراط في التكيّف عبر :ref:`shrinkage
  <gradient_boosting_shrinkage>`.

  .. note::

    يتطلب التصنيف بأكثر من فئتين استقراء ``n_classes`` من أشجار القرار في كل تكرار،
    وبالتالي، فإن العدد الإجمالي للأشجار المستقرأة يساوي
    ``n_classes * n_estimators``. بالنسبة لمجموعات البيانات التي تحتوي على عدد كبير
    من الفئات، نوصي بشدة باستخدام
    :class:`HistGradientBoostingClassifier` كبديل لـ
    :class:`GradientBoostingClassifier`.

.. dropdown:: الانحدار

  تدعم :class:`GradientBoostingRegressor` عددًا من
  :ref:`وظائف الخسارة المختلفة <gradient_boosting_loss>`
  للانحدار والتي يمكن تحديدها من خلال حجة
  ``loss``؛ ووظيفة الخسارة الافتراضية للانحدار هي الخطأ التربيعي
  (``'squared_error'``).

  ::

      >>> import numpy as np
      >>> from sklearn.metrics import mean_squared_error
      >>> from sklearn.datasets import make_friedman1
      >>> from sklearn.ensemble import GradientBoostingRegressor

      >>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)
      >>> X_train, X_test = X[:200], X[200:]
      >>> y_train, y_test = y[:200], y[200:]
      >>> est = GradientBoostingRegressor(
      ...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,
      ...     loss='squared_error'
      ... ).fit(X_train, y_train)
      >>> mean_squared_error(y_test, est.predict(X_test))
      5.00...

  يوضح الشكل أدناه نتائج تطبيق :class:`GradientBoostingRegressor`
  مع خسارة المربعات الصغرى و500 متعلم أساسي على مجموعة بيانات مرض السكري
  (:func:`sklearn.datasets.load_diabetes`).
  يوضح الرسم الخطي خطأ التدريب والاختبار في كل تكرار.
  يتم تخزين خطأ التدريب في كل تكرار في الخاصية
  `train_score_` لنموذج التعزيز التدريجي.
  يمكن الحصول على خطأ الاختبار في كل تكرار
  عبر طريقة :meth:`~GradientBoostingRegressor.staged_predict` التي تعيد مولدًا
  يقوم بتقديم التنبؤات في كل مرحلة. يمكن استخدام الرسوم البيانية مثل هذه لتحديد
  العدد الأمثل للأشجار (أي ``n_estimators``) عن طريق التوقف المبكر.

  .. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_regression_001.png
    :target: ../auto_examples/ensemble/plot_gradient_boosting_regression.html
    :align: center
    :scale: 75

.. rubric:: الأمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`
* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_oob.py`

.. _gradient_boosting_warm_start:

ملاءمة متعلمين ضعفاء إضافيين
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

يدعم كل من :class:`GradientBoostingRegressor` و :class:`GradientBoostingClassifier`
خاصية ``warm_start=True`` التي تسمح بإضافة المزيد من المتعلمين إلى نموذج تم ملاءمته بالفعل.
::

تحكم حجم الشجرة
^^^^^^^^^^^^^^^^^^^^^^^^^^

يحدد حجم شجرة الانحدار الأساسي مستوى تفاعلات المتغيرات التي يمكن أن يلتقطها نموذج Gradient Boosting. بشكل عام، يمكن لشجرة ذات عمق "h" أن تلتقط تفاعلات من الرتبة "h".

هناك طريقتان يمكن من خلالهما التحكم في حجم شجرة الانحدار الفردية.

إذا قمت بتحديد "max_depth=h"، فسيتم إنشاء أشجار ثنائية كاملة بعمق "h". سيكون لهذه الأشجار (على الأكثر) "2**h" من العقد الورقية و "2**h - 1" من العقد الانقسامية.

أو يمكنك التحكم في حجم الشجرة عن طريق تحديد عدد العقد الورقية باستخدام المعلمة "max_leaf_nodes". في هذه الحالة، سيتم إنشاء الأشجار باستخدام البحث الأفضل أولاً حيث يتم توسيع العقد التي لها أعلى تحسن في النقاء أولاً.

الشجرة التي تحتوي على "max_leaf_nodes=k" لها "k - 1" من العقد الانقسامية وبالتالي يمكنها نمذجة التفاعلات حتى الرتبة "max_leaf_nodes - 1".

وجدنا أن "max_leaf_nodes=k" يعطي نتائج مماثلة لـ "max_depth=k-1"، ولكنه أسرع بكثير في التدريب على حساب خطأ تدريب أعلى قليلاً.

ترتبط معلمة "max_leaf_nodes" بالمتغير "J" في الفصل الخاص بـ Gradient Boosting في [Friedman2001] _ وهي مرتبطة بمعلمة "interaction.depth" في حزمة gbm في R حيث "max_leaf_nodes == interaction.depth + 1".

الصيغة الرياضية
^^^^^^^^^^^^^^^^^^^^^^^^

نقدم أولاً GBRT للانحدار، ثم نتناول حالة التصنيف بالتفصيل.

انحدار

تعد نماذج GBRT للانحدار نماذجًا إضافية يكون توقعها :math:`\hat{y}_i` لإدخال معين :math:`x_i` على الشكل التالي:

.. math::

  \hat{y}_i = F_M(x_i) = \sum_{m=1}^{M} h_m(x_i)

حيث :math:`h_m` هي تقديرات تسمى *weak learners* في سياق التعزيز. يستخدم Gradient Tree Boosting :ref: `decision tree regressors <tree>` ذات الحجم الثابت كـ weak learners. يرمز الثابت M إلى معلمة `n_estimators`.

مثل خوارزميات التعزيز الأخرى، يتم بناء GBRT بطريقة جشعة:

.. math::

  F_m(x) = F_{m-1}(x) + h_m(x),

حيث يتم تركيب الشجرة المضافة حديثًا :math:`h_m` لتقليل مجموع الخسائر :math:`L_m`، بالنظر إلى المجموعة السابقة :math:`F_{m-1}`:

.. math::

  h_m = \arg\min_{h} L_m = \arg\min_{h} \sum_{i=1}^{n}
  l(y_i, F_{m-1}(x_i) + h(x_i)),

حيث :math:`l(y_i, F(x_i))` تحددها معلمة `loss`، والتي يتم تفصيلها في القسم التالي.

بشكل افتراضي، يتم اختيار النموذج الأولي :math:`F_{0}` على أنه الثابت الذي يقلل الخسارة: بالنسبة لخسارة المربعات الصغرى، يكون هذا هو المتوسط الحسابي لقيم الهدف. يمكن أيضًا تحديد النموذج الأولي عبر حجة "init".

باستخدام تقريب تايلور من الدرجة الأولى، يمكن تقريب قيمة :math:`l` على النحو التالي:

.. math::

  l(y_i, F_{m-1}(x_i) + h_m(x_i)) \approx
  l(y_i, F_{m-1}(x_i))
  + h_m(x_i)
  \left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m - 1}}.

.. note::

  باختصار، يقول تقريب تايلور من الدرجة الأولى أن :math:`l(z) \approx l(a) + (z - a) \frac{\partial l}{\partial z}(a)`. هنا، :math:`z` يقابل :math:`F_{m - 1}(x_i) + h_m(x_i)`، و:math:`a` يقابل :math:`F_{m-1}(x_i)`.

الكمية :math:`\left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m - 1}}` هي مشتقة الخسارة فيما يتعلق بمعلمتها الثانية، المقدرة عند :math:`F_{m-1}(x)`. من السهل حسابها لأي :math:`F_{m - 1}(x_i)` في شكل مغلق منذ قابلية الخسارة للاشتقاق. سنرمزها بـ :math:`g_i`.

بعد إزالة المصطلحات الثابتة، نحصل على:

.. math::

  h_m \approx \arg\min_{h} \sum_{i=1}^{n} h(x_i) g_i

هذا الحد الأدنى إذا تم تركيب :math:`h(x_i)` للتنبؤ بقيمة تتناسب مع التدرج السلبي :math:`-g_i`. لذلك، في كل تكرار، يتم تركيب **المقدر** :math:`h_m` **للتنبؤ بالتدرجات السلبية للعينات**. يتم تحديث التدرجات في كل تكرار. يمكن اعتبار هذا شكلًا من أشكال الانحدار التدريجي في مساحة الدالة.

.. note::

  بالنسبة لبعض الخسائر، على سبيل المثال "absolute_error" حيث تكون التدرجات :math:`\pm 1`، فإن القيم التي يتنبأ بها :math:`h_m` المجهزة ليست دقيقة بدرجة كافية: يمكن للشجرة أن تخرج فقط قيمًا صحيحة. ونتيجة لذلك، يتم تعديل قيم الأوراق للشجرة :math:`h_m` بمجرد تركيب الشجرة، بحيث تقلل قيم الأوراق من الخسارة :math:`L_m`. يعتمد التحديث على الخسارة: بالنسبة لخسارة الخطأ المطلق، يتم تحديث قيمة الورقة إلى الوسيط للعينات في تلك الورقة.

تصنيف

يتشابه التعزيز التدريجي للتصنيف بشكل كبير مع حالة الانحدار. ومع ذلك، فإن مجموع الأشجار :math:`F_M(x_i) = \sum_m h_m(x_i)` ليس متجانسًا مع التنبؤ: لا يمكن أن يكون فئة، نظرًا لأن الأشجار تتنبأ بقيم مستمرة.

يعتمد رسم الخريطة من القيمة :math:`F_M(x_i)` إلى فئة أو احتمال على الخسارة. بالنسبة إلى الخسارة اللوغاريتمية، يتم نمذجة احتمال انتماء :math:`x_i` إلى الفئة الإيجابية على النحو التالي :math:`p(y_i = 1 | x_i) = \sigma(F_M(x_i))` حيث :math:`\sigma` هي دالة التغليف أو دالة التغليف.

بالنسبة للتصنيف متعدد الفئات، يتم بناء K شجرة (لفئات K) في كل واحدة من :math:`M` تكرارات. يتم نمذجة احتمال انتماء :math:`x_i` إلى الفئة k كدالة softmax للقيم :math:`F_{M,k}(x_i)`.

لاحظ أنه حتى لمهمة التصنيف، فإن :math:`h_m` sub-estimator هو لا يزال مقدر الانحدار، وليس مصنف. ويرجع ذلك إلى أن sub-estimators يتم تدريبها للتنبؤ بالتدرجات (السلبية)، والتي تكون دائمًا كميات مستمرة.

وظائف الخسارة
^^^^^^^^^^^^^^

تتم دعم وظائف الخسارة التالية ويمكن تحديدها باستخدام معلمة "loss":

انحدار

* خطأ تربيعي (``'squared_error'``): الخيار الطبيعي للانحدار بسبب خصائصه الحسابية المتفوقة. يتم إعطاء النموذج الأولي بمتوسط قيم الهدف.
* خطأ مطلق (``'absolute_error'``): دالة خسارة قوية للانحدار. يتم إعطاء النموذج الأولي بواسطة الوسيط لقيم الهدف.
* هابر (``'huber'``): دالة خسارة قوية أخرى تجمع بين المربعات الصغرى والانحرافات المطلقة الصغرى؛ استخدم "alpha" للتحكم في الحساسية فيما يتعلق بالقيم الشاذة (راجع [Friedman2001] _ لمزيد من التفاصيل).
* الكمية (``'quantile'``): دالة خسارة للانحدار الكمي. استخدم "0 < alpha < 1" لتحديد الكمية. يمكن استخدام دالة الخسارة هذه لإنشاء فترات تنبؤ (راجع :ref: `sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py`).

تصنيف

* الخسارة اللوغاريتمية الثنائية (``'log-loss'``): دالة الخسارة اللوغاريتمية السالبة للاحتمالية الثنائية للتصنيف. يوفر تقديرات الاحتمالية. يتم إعطاء النموذج الأولي بواسطة نسبة الاحتمالات.
* الخسارة اللوغاريتمية متعددة الفئات (``'log-loss'``): دالة الخسارة اللوغاريتمية السالبة للاحتمالية متعددة الفئات للتصنيف مع ``n_classes`` فئات حصرية. يوفر تقديرات الاحتمالية. يتم إعطاء النموذج الأولي بواسطة الاحتمالية السابقة لكل فئة. في كل تكرار، يجب بناء ``n_classes`` من أشجار الانحدار، مما يجعل GBRT غير فعال إلى حد ما لمجموعات البيانات التي تحتوي على عدد كبير من الفئات.
* الخسارة الأسية (``'exponential'``): نفس دالة الخسارة مثل :class: `AdaBoostClassifier`. أقل قوة من ``'log-loss'``؛ يمكن استخدامه فقط للتصنيف الثنائي.

التقلص عبر معدل التعلم
^^^^^^^^^^^^^^^^^^^^^^^^^^^

اقترح [Friedman2001] _ استراتيجية تنظيم بسيطة تقوم بضبط مساهمة كل weak learner بمعامل ثابت :math:`\nu`:

.. math::

  F_m(x) = F_{m-1}(x) + \nu h_m(x)

يُطلق على المعلمة :math:`\nu` أيضًا اسم **معدل التعلم** لأنها تحدد حجم خطوة إجراء الانحدار التدريجي؛ يمكن تعيينه عبر معلمة "learning_rate".

تتفاعل معلمة "learning_rate" بشكل كبير مع معلمة "n_estimators"، والتي هي عدد weak learners التي سيتم تركيبها. تتطلب قيم "learning_rate" الأصغر أعدادًا أكبر من weak learners للحفاظ على خطأ التدريب الثابت. تشير الأدلة التجريبية إلى أن قيم "learning_rate" الصغيرة تفضل خطأ الاختبار الأفضل. [HTF] _ يوصي بتعيين معدل التعلم إلى ثابت صغير (على سبيل المثال "learning_rate <= 0.1") واختيار "n_estimators" كبير بما يكفي للتوقف المبكر، راجع :ref: `sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py` لمزيد من المناقشة حول التفاعل بين "learning_rate" و "n_estimators" راجع [R2007] _.

الاستعانة بعينة جزئية
^^^^^^^^^^^^

اقترح [Friedman2002] _ التعزيز التدريجي العشوائي، والذي يجمع بين التعزيز التدريجي والمتوسط التجميعي (bagging). في كل تكرار، يتم تدريب المصنف الأساسي على جزء "subsample" من بيانات التدريب المتاحة. يتم رسم العينة العشوائية بدون استبدال.

القيمة النموذجية لـ "subsample" هي 0.5.

يوضح الشكل أدناه تأثير التقلص والاستعانة بعينة جزئية على دقة النموذج. يمكننا أن نرى بوضوح أن التقلص يتفوق على عدم التقلص. يمكن أن تزيد الاستعانة بعينة جزئية مع التقلص من دقة النموذج. من ناحية أخرى، فإن الاستعانة بعينة جزئية بدون تقلص لا تؤدي أداءً جيدًا.

.. figure:: ../auto_examples/ensemble/images/sphx_glr_plot_gradient_boosting_regularization_001.png
   :target: ../auto_examples/ensemble/plot_gradient_boosting_regularization.html
   :align: center
   :scale: 75

تتمثل إحدى الاستراتيجيات الأخرى لتقليل التباين في الاستعانة بعينة جزئية للميزات على غرار الانقسامات العشوائية في :class: `RandomForestClassifier`. يمكن التحكم في عدد الميزات المستعان بعينة جزئية منها من خلال معلمة "max_features".

.. note:: يمكن أن يؤدي استخدام قيمة "max_features" صغيرة إلى تقليل وقت التشغيل بشكل كبير.

يتيح التعزيز التدريجي العشوائي حساب تقديرات خارج الكيس لمقاييس الاختبار عن طريق حساب التحسن في مقياس الاختبار على الأمثلة غير المدرجة في عينة الإقلاع (أي الأمثلة خارج الكيس). يتم تخزين التحسينات في السمة `oob_improvement_`. يحتوي "oob_improvement_[i]" على التحسن من حيث الخسارة على عينات OOB إذا أضفت مرحلة i-th إلى التوقعات الحالية. يمكن استخدام تقديرات خارج الكيس لاختيار النموذج، على سبيل المثال لتحديد العدد الأمثل من التكرارات. تقديرات خارج الكيس متشائمة للغاية، لذلك نوصي باستخدام التحقق من صحة الاستعانة بعينة جزئية بدلاً من ذلك، واستخدام خارج الكيس فقط إذا كان التحقق من صحة الاستعانة بعينة جزئية يستغرق وقتًا طويلاً للغاية.

.. rubric:: أمثلة

* :ref: `sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regularization.py`
* :ref: `sphx_glr_auto_examples_ensemble_plot_gradient_boosting_oob.py`
* :ref: `sphx_glr_auto_examples_ensemble_plot_ensemble_oob.py`

التفسير بأهمية الميزة
^^^^^^^^^^^^^^^^^^^^^^^^^^^

يمكن تفسير شجرة القرار الفردية بسهولة عن طريق تصور بنية الشجرة. ومع ذلك، فإن نماذج التعزيز التدريجي تتكون من مئات أشجار الانحدار، وبالتالي لا يمكن تفسيرها بسهولة عن طريق الفحص البصري للأشجار الفردية. لحسن الحظ، تم اقتراح عدد من التقنيات لتلخيص نماذج التعزيز التدريجي وتفسيرها.
في كثير من الأحيان، لا تساهم الخصائص بشكل متساوٍ في التنبؤ بالاستجابة المستهدفة؛ وفي العديد من الحالات، تكون أغلب الخصائص غير ذات صلة في الواقع.

عند تفسير نموذج ما، يكون السؤال الأول عادةً هو: ما هي تلك الخصائص المهمة، وكيف تساهم في التنبؤ بالاستجابة المستهدفة؟

أشجار القرار الفردية تقوم بشكل جوهري باختيار الخصائص عن طريق اختيار نقاط الانقسام المناسبة. يمكن استخدام هذه المعلومات لقياس أهمية كل خاصية؛ والفكرة الأساسية هي: كلما تم استخدام خاصية ما بشكل متكرر في نقاط انقسام الشجرة، كلما زادت أهمية تلك الخاصية. يمكن توسيع هذا المفهوم للأهمية ليشمل مجموعات أشجار القرار ببساطة عن طريق حساب المتوسط لأهمية كل خاصية بناءً على عدم النقاء (انعدام الصفاء) في كل شجرة (راجع :ref:`random_forest_feature_importance` لمزيد من التفاصيل).

يمكن الوصول إلى درجات أهمية الخصائص لنموذج Gradient Boosting المناسب عن طريق خاصية ``feature_importances_``::

    >>> from sklearn.datasets import make_hastie_10_2
    >>> from sklearn.ensemble import GradientBoostingClassifier

    >>> X, y = make_hastie_10_2(random_state=0)
    >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    ...     max_depth=1, random_state=0).fit(X, y)
    >>> clf.feature_importances_
    array([0.10..., 0.10..., 0.11..., ...

لاحظ أن حساب أهمية الخصائص هذا يعتمد على الإنتروبيا، وهو مختلف عن :func:`sklearn.inspection.permutation_importance` الذي يعتمد على تبديل الخصائص.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`

.. rubric:: مراجع

.. [Friedman2001] Friedman, J.H. (2001). :doi:`Greedy function approximation: A gradient
   boosting machine <10.1214/aos/1013203451>`.
   Annals of Statistics, 29, 1189-1232.

.. [Friedman2002] Friedman, J.H. (2002). `Stochastic gradient boosting.
   <https://statweb.stanford.edu/~jhf/ftp/stobst.pdf>`_.
   Computational Statistics & Data Analysis, 38, 367-378.

.. [R2007] G. Ridgeway (2006). `Generalized Boosted Models: A guide to the gbm
   package <https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf>`_

.. _forest:

Random forests and other randomized tree ensembles
يحتوي نمط sklearn.ensemble على خوارزميتي متوسط بناءً على شجرة القرار المعشاة: خوارزمية RandomForest وطريقة Extra-Trees. كلتا الخوارزميتين هما من تقنيات Perturb-and-Combine المصممة خصيصًا للشجر. وهذا يعني أنه يتم إنشاء مجموعة متنوعة من المصنفات عن طريق إدخال العشوائية في بناء المصنف. يتم إعطاء تنبؤ المجموعة كمتوسط للتنبؤات من المصنفات الفردية.

مثل المصنفات الأخرى، يجب ضبط مصنفات الغابة باستخدام صفيفين: صفيف X متفرق أو كثيف الشكل (n_samples، n_features) يحتوي على عينات التدريب، وصفيف Y من الشكل (n_samples،) يحتوي على قيم الهدف (ملصقات الفئات) لعينات التدريب::

    >>> from sklearn.ensemble import RandomForestClassifier
    >>> X = [[0, 0], [1, 1]]
    >>> Y = [0, 1]
    >>> clf = RandomForestClassifier(n_estimators=10)
    >>> clf = clf.fit(X, Y)

مثل شجرة القرار، تمتد غابات الأشجار أيضًا إلى مشكلات الإخراج المتعدد (إذا كان Y عبارة عن صفيف من الشكل (n_samples، n_outputs)).

غابات عشوائية
----------------

في الغابات العشوائية (راجع فئات RandomForestClassifier وRandomForestRegressor)، يتم بناء كل شجرة في المجموعة من عينة مستخلصة مع الاستبدال (أي عينة الإقلاع) من مجموعة التدريب.

علاوة على ذلك، عند تقسيم كل عقدة أثناء بناء الشجرة، يتم العثور على أفضل تقسيم من خلال البحث المستنفد لقيم ميزات جميع الميزات المدخلة أو مجموعة فرعية عشوائية من حجم "max_features". (راجع إرشادات ضبط المعلمات لمزيد من التفاصيل).

الغرض من هذين المصدرين للعشوائية هو تقليل تغايرية مؤشر الغابة. في الواقع، عادة ما تظهر أشجار القرار الفردية تغايرية عالية وتميل إلى الإفراط في التكيّف. تؤدي العشوائية التي يتم حقنها في الغابات إلى أشجار قرار ذات أخطاء تنبؤ غير مترابطة إلى حد ما. عن طريق حساب متوسط هذه التنبؤات، يمكن أن تلغي بعض الأخطاء. تحقق الغابات العشوائية تقليل التباين عن طريق دمج الأشجار المتنوعة، أحيانًا على حساب زيادة طفيفة في الانحياز. في الممارسة العملية، يكون تقليل التباين كبيرًا في كثير من الأحيان، مما يؤدي إلى نموذج أفضل بشكل عام.

على عكس المنشور الأصلي [B2001]_، يجمع تنفيذ Scikit-learn المصنفات عن طريق حساب المتوسط التنبؤات الاحتمالية الخاصة بها، بدلاً من السماح لكل مصنف بالتصويت لفئة واحدة.

بديل تنافسي للغابات العشوائية هو نموذج Histogram-based Gradient Boosting (HGBT):

- بناء الأشجار: تعتمد الغابات العشوائية عادةً على الأشجار العميقة (التي تفرط في التكيف بشكل فردي) والتي تستخدم الكثير من الموارد الحسابية، حيث تتطلب العديد من الانقسامات وتقييمات الانقسامات المرشحة. تقوم نماذج التعزيز ببناء أشجار ضحلة (التي لا تتناسب بشكل فردي) والتي تكون أسرع في التجهيز والتنبؤ.

- التعزيز التسلسلي: في HGBT، يتم بناء أشجار القرار بشكل تسلسلي، حيث يتم تدريب كل شجرة لتصحيح الأخطاء التي ارتكبتها الأشجار السابقة. يسمح لهم هذا بتحسين أداء النموذج بشكل تكراري باستخدام عدد قليل نسبيًا من الأشجار. على النقيض من ذلك، تستخدم الغابات العشوائية تصويت الأغلبية للتنبؤ بالنتيجة، والذي قد يتطلب عددًا أكبر من الأشجار لتحقيق نفس مستوى الدقة.

- التصنيف الفعال: يستخدم HGBT خوارزمية تصنيف فعالة يمكنها التعامل مع مجموعات البيانات الكبيرة ذات العدد الكبير من الميزات. يمكن لخوارزمية التصنيف معالجة البيانات مسبقًا لتسريع بناء الشجرة لاحقًا (راجع لماذا يكون أسرع للحصول على مزيد من التفاصيل). على النقيض من ذلك، لا يستخدم تنفيذ Scikit-learn للغابات العشوائية التصنيف ويعتمد على الانقسام الدقيق، والذي يمكن أن يكون مكلفًا حسابياً.

بشكل عام، تتوقف التكلفة الحسابية لـ HGBT مقابل RF على الخصائص المحددة لمجموعة البيانات ومهمة النمذجة. من الجيد تجربة كلا النموذجين ومقارنة أدائهما وكفاءتهما الحسابية على مشكلتك المحددة لتحديد النموذج الأنسب.

أمثلة
-----

* sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py

الأشجار العشوائية للغاية
--------------------------

في الأشجار العشوائية للغاية (راجع فئات ExtraTreesClassifier وExtraTreesRegressor)، تذهب العشوائية خطوة أخرى إلى الأمام في طريقة حساب الانقسامات. كما هو الحال في الغابات العشوائية، يتم استخدام مجموعة فرعية عشوائية من الميزات المرشحة، ولكن بدلاً من البحث عن عتبات الأكثر تمييزًا، يتم رسم العتبات بشكل عشوائي لكل ميزة مرشحة ويتم اختيار أفضل هذه العتبات العشوائية كقاعدة للتقسيم. عادة ما يسمح هذا بخفض تغايرية النموذج قليلاً، على حساب زيادة طفيفة في الانحياز::

    >>> from sklearn.model_selection import cross_val_score
    >>> from sklearn.datasets import make_blobs
    >>> from sklearn.ensemble import RandomForestClassifier
    >>> from sklearn.ensemble import ExtraTreesClassifier
    >>> from sklearn.tree import DecisionTreeClassifier

    >>> X, y = make_blobs(n_samples=10000, n_features=10, centers=100,
    ...     random_state=0)

    >>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,
    ...     random_state=0)
    >>> scores = cross_val_score(clf, X, y, cv=5)
    >>> scores.mean()
    0.98...

    >>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
    ...     min_samples_split=2, random_state=0)
    >>> scores = cross_val_score(clf, X, y, cv=5)
    >>> scores.mean()
    0.999...

    >>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
    ...     min_samples_split=2, random_state=0)
    >>> scores = cross_val_score(clf, X, y, cv=5)
    >>> scores.mean() > 0.999
    True

المعلمات
----------

المعلمات الرئيسية التي يجب ضبطها عند استخدام هذه الطرق هي "n_estimators" و"max_features". الأول هو عدد الأشجار في الغابة. كلما كان أكبر، كان ذلك أفضل، ولكن أيضًا سيستغرق وقتًا أطول لحساب. بالإضافة إلى ذلك، لاحظ أن النتائج لن تتحسن بشكل كبير بعد عدد حرج من الأشجار. الأخير هو حجم المجموعات الفرعية العشوائية من الميزات للنظر فيها عند تقسيم عقدة. كلما انخفضت، زاد تقليل التباين، ولكن أيضًا زادت الزيادة في الانحياز. القيم الافتراضية الجيدة هي "max_features=1.0" أو ما يعادلها "max_features=None" (النظر دائمًا في جميع الميزات بدلاً من مجموعة فرعية عشوائية) لمشكلات الانحدار، و"max_features="sqrt" (باستخدام مجموعة فرعية عشوائية من الحجم "sqrt(n_features)") لمهمات التصنيف (حيث "n_features" هو عدد الميزات في البيانات). القيمة الافتراضية "max_features=1.0" تعادل الأشجار المعبأة ويمكن تحقيق المزيد من العشوائية عن طريق تعيين قيم أصغر (على سبيل المثال، 0.3 هو افتراضي نموذجي في الأدبيات). يتم تحقيق نتائج جيدة غالبًا عند تعيين "max_depth=None" بالاشتراك مع "min_samples_split=2" (أي عند تطوير الأشجار بالكامل). ضع في اعتبارك أنه قد لا تكون هذه القيم مثالية، وقد تؤدي إلى نماذج تستهلك الكثير من ذاكرة الوصول العشوائي. يجب دائمًا التحقق من صحة أفضل قيم المعلمات. بالإضافة إلى ذلك، لاحظ أنه في الغابات العشوائية، يتم استخدام عينات الإقلاع بشكل افتراضي (bootstrap=True) في حين أن الاستراتيجية الافتراضية للأشجار الإضافية هي استخدام مجموعة البيانات بأكملها (bootstrap=False). عند استخدام عينات الإقلاع، يمكن تقدير خطأ التعميم على العينات المتروكة أو العينات الخارجة عن الحزمة. يمكن تمكين هذا عن طريق تعيين "oob_score=True".

ملاحظة

حجم النموذج بالمعلمات الافتراضية هو: O (M * N * log (N))، حيث M هو عدد الأشجار وN هو عدد العينات. لتقليل حجم النموذج، يمكنك تغيير هذه المعلمات: "min_samples_split"، "max_leaf_nodes"، "max_depth" و"min_samples_leaf".

التوازي
---------------

أخيرًا، تتميز هذه الوحدة أيضًا بالبناء الموازي للأشجار والحساب الموازي للتنبؤات من خلال معلمة "n_jobs". إذا كان n_jobs=k، فسيتم تقسيم الحسابات إلى k وظائف، ويتم تشغيلها على k نواة من الآلة. إذا كان n_jobs=-1، فسيتم استخدام جميع النوى المتوفرة على الآلة. لاحظ أنه بسبب النفقات العامة للاتصال بين العمليات، فقد لا يكون التسريع خطيًا (أي أن استخدام k وظائف لن يكون للأسف أسرع k مرة). لا يزال من الممكن تحقيق تسريع كبير عند بناء عدد كبير من الأشجار، أو عند بناء شجرة واحدة تتطلب قدرًا عادلًا من الوقت (على سبيل المثال، على مجموعات البيانات الكبيرة).

أمثلة
-----

* sphx_glr_auto_examples_ensemble_plot_forest_iris.py
* sphx_glr_auto_examples_ensemble_plot_forest_importances_faces.py
* sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py

مراجع
----------

.. [B2001] L. Breiman، "Random Forests"، Machine Learning، 45(1)، 5-32، 2001.

.. [B1998] L. Breiman، "Arcing Classifiers"، Annals of Statistics 1998.

* P. Geurts، D. Ernst.، وL. Wehenkel، "الأشجار العشوائية للغاية"، Machine Learning، 63(1)، 3-42، 2006.

تقييم أهمية الميزة
-----------------------------

يمكن استخدام المرتبة النسبية (أي العمق) لميزة مستخدمة كعقدة قرار في شجرة لتقييم الأهمية النسبية لتلك الميزة فيما يتعلق بإمكانية التنبؤ بالمتغير المستهدف. تساهم الميزات المستخدمة في الجزء العلوي من الشجرة في قرار التنبؤ النهائي لجزء أكبر من عينات الإدخال. يمكن استخدام **النسبة المتوقعة من العينات** التي تساهم فيها كتقدير لـ **الأهمية النسبية للميزات**. في Scikit-learn، يتم دمج نسبة العينات التي تساهم بها ميزة مع الانخفاض في النقاء من تقسيمها لإنشاء تقدير معاير لقوة التنبؤ لتلك الميزة.

عن طريق **المتوسط** التقديرات لقدرة التنبؤ عبر عدة أشجار عشوائية يمكن **تقليل التباين** في مثل هذا التقدير واستخدامه لاختيار الميزة. يُعرف هذا باسم الانخفاض المتوسط في النقاء، أو MDI. راجع [L2014]_ لمزيد من المعلومات حول MDI وتقييم أهمية الميزة باستخدام Random Forests.

تحذير

تعاني أهمية الميزة القائمة على النقاء المحسوبة على النماذج القائمة على الأشجار من عيبين يمكن أن يؤديان إلى استنتاجات مضللة. أولاً، يتم حسابها على الإحصاءات المستمدة من مجموعة البيانات التدريبية وبالتالي **لا تخبرنا بالضرورة بالميزات الأكثر أهمية للتنبؤات الدقيقة بمجموعة بيانات محجوزة**. ثانيًا، **يفضلون ميزات التعددية العالية**، أي الميزات ذات القيم الفريدة العديدة. الأهمية حسب الترتيب هي بديل لأهمية النقاء لا يعاني من هذه العيوب. يتم استكشاف هاتين الطريقتين للحصول على أهمية الميزة في: sphx_glr_auto_examples_inspection_plot_permutation_importance.py.

يوضح المثال التالي تمثيلًا ملونًا لأهمية النسبية لكل بكسل فردي لمهمة التعرف على الوجه باستخدام نموذج ExtraTreesClassifier.

في الممارسة العملية، يتم تخزين هذه التقديرات كسمة تسمى "feature_importances_" على النموذج المناسب. هذا هو صفيف الشكل (n_features،) بقيم إيجابية ومجموع 1.0. كلما كانت القيمة أعلى، كلما كانت مساهمة الميزة المطابقة أكثر أهمية في دالة التنبؤ.

أمثلة
-----

* sphx_glr_auto_examples_ensemble_plot_forest_importances_faces.py
* sphx_glr_auto_examples_ensemble_plot_forest_importances.py

مراجع
----------

.. [L2014] G. Louppe،: arXiv: "Understanding Random Forests: From Theory to
   Practice" <1407.7502>،
   أطروحة دكتوراه، جامعة لييج، 2014.

غرس الأشجار العشوائية تمامًا
ينفذ :class:`RandomTreesEmbedding` تحويلًا غير خاضع للإشراف للبيانات. باستخدام غابة من الأشجار العشوائية تمامًا، يشفر :class:`RandomTreesEmbedding` البيانات من خلال مؤشرات الأوراق التي تنتهي فيها نقطة البيانات. بعد ذلك، يتم تشفير هذا المؤشر بطريقة واحدة من K، مما يؤدي إلى ترميز ثنائي عالي الأبعاد ومُنَفَّذ.

يمكن حساب هذا الترميز بكفاءة عالية ويمكن استخدامه بعد ذلك كأساس لمهام التعلم الأخرى.

يمكن التأثير على حجم الترميز وتباعده عن طريق اختيار عدد الأشجار والعمق الأقصى لكل شجرة. لكل شجرة في المجموعة، يحتوي الترميز على إدخال واحد من واحد. ويبلغ حجم الترميز كحد أقصى "n_estimators * 2 ** max_depth"، وهو العدد الأقصى للأوراق في الغابة.

نظرًا لأن نقاط البيانات المجاورة أكثر عرضة للتواجد داخل نفس ورقة الشجرة، فإن التحويل يؤدي إلى تقدير كثافة غير معلم ضمني.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_random_forest_embedding.py`

* :ref:`sphx_glr_auto_examples_manifold_plot_lle_digits.py` يقارن تقنيات تقليل الأبعاد غير الخطية لأرقام مكتوبة بخط اليد.

* :ref:`sphx_glr_auto_examples_ensemble_plot_feature_transformation.py` يقارن بين التحولات المميزة المستندة إلى الإشراف وغير الخاضعة للإشراف والقائمة على الأشجار.

.. seealso::

   يمكن أيضًا أن تكون تقنيات :ref:`manifold` مفيدة لاستنتاج التمثيلات غير الخطية لمساحة الميزة، كما تركز هذه الأساليب على تقليل الأبعاد.

.. _tree_ensemble_warm_start:

تناسب أشجار إضافية
------------------------

تدعم جميع تقديرات RandomForest وExtra-Trees و:class:`RandomTreesEmbedding` ``warm_start=True``، والتي تتيح لك إضافة المزيد من الأشجار إلى نموذج تم تناسبه بالفعل.

::

  >>> from sklearn.datasets import make_classification
  >>> from sklearn.ensemble import RandomForestClassifier

  >>> X, y = make_classification(n_samples=100, random_state=1)
  >>> clf = RandomForestClassifier(n_estimators=10)
  >>> clf = clf.fit(X, y) # تناسب مع 10 أشجار
  >>> len(clf.estimators_)
  10
  >>> # تعيين warm_start وزيادة عدد المؤشرات
  >>> _ = clf.set_params(n_estimators=20, warm_start=True)
  >>> _ = clf.fit(X, y) # تناسب 10 أشجار إضافية
  >>> len(clf.estimators_)
  20

عندما يتم أيضًا تعيين "random_state"، يتم الاحتفاظ بحالة عشوائية داخلية بين مكالمات "fit". وهذا يعني أن تدريب نموذج مرة واحدة مع "n" من المؤشرات هو نفسه بناء النموذج بشكل تكراري عبر مكالمات "fit" متعددة، حيث يكون العدد النهائي للمؤشرات مساويًا لـ "n".

::

  >>> clf = RandomForestClassifier(n_estimators=20) # تعيين `n_estimators` إلى 10 + 10
  >>> _ = clf.fit(X, y) # تناسب `estimators_` ستكون هي نفسها كما `clf` أعلاه

لاحظ أن هذا يختلف عن السلوك المعتاد لـ :term:`random_state` في أنه لا يؤدي إلى نفس النتيجة عبر المكالمات المختلفة.

.. _bagging:

Bagging meta-estimator
في خوارزميات التجميع، تشكل طرق المعايرة فئة من الخوارزميات التي تقوم ببناء عدة مثيلات لمقدّر الصندوق الأسود على مجموعات فرعية عشوائية من مجموعة التدريب الأصلية، ثم تجميع تنبؤاتها الفردية لتشكيل تنبؤ نهائي. وتُستخدم هذه الطرق كوسيلة للحد من تباين مُقدّر أساسي (مثل شجرة القرار)، من خلال إدخال العشوائية في إجراء بنائها ومن ثم تشكيل مجموعة منها. وفي العديد من الحالات، تشكل طرق المعايرة طريقة بسيطة للغاية للتحسين فيما يتعلق بنموذج واحد، دون أن يكون من الضروري تكييف خوارزمية القاعدة الأساسية. وبما أنها توفر طريقة للحد من الإفراط في التخصيص، فإن طرق المعايرة تعمل بشكل أفضل مع النماذج القوية والمعقدة (مثل شجرة القرار الكاملة)، على عكس طرق التعزيز التي تعمل عادة بشكل أفضل مع النماذج الضعيفة (مثل شجرة القرار الضحلة).

تأتي طرق المعايرة بنكهات عديدة ولكنها تختلف في الغالب عن بعضها البعض بطريقة سحبها لمجموعات فرعية عشوائية من مجموعة التدريب:

* عندما يتم سحب المجموعات الفرعية العشوائية من مجموعة البيانات كمجموعات فرعية عشوائية من العينات، يُعرف هذا الخوارزم باسم Pasting [B1999]_.

* عندما يتم سحب العينات مع الاستبدال، تُعرف الطريقة باسم Bagging [B1996]_.

* عندما يتم سحب المجموعات الفرعية العشوائية من مجموعة البيانات كمجموعات فرعية عشوائية من السمات، تُعرف الطريقة باسم Random Subspaces [H1998]_.

* وأخيرًا، عندما يتم بناء المُقدّرات الأساسية على مجموعات فرعية من كل من العينات والسمات، تُعرف الطريقة باسم Random Patches [LG2012]_.

في scikit-learn، تُقدم طرق المعايرة كمُقدّر موحد من فئة BaggingClassifier (أو BaggingRegressor)، والذي يأخذ كمدخلات مُقدّر محدد من قبل المستخدم بالإضافة إلى معلمات تحدد الاستراتيجية المستخدمة لسحب المجموعات الفرعية العشوائية. وعلى وجه التحديد، يتحكم كل من "max_samples" و"max_features" في حجم المجموعات الفرعية (من حيث العينات والسمات)، في حين يتحكم كل من "bootstrap" و"bootstrap_features" في ما إذا كانت العينات والسمات يتم سحبها مع الاستبدال أو بدونه. وعندما يتم استخدام مجموعة فرعية من العينات المتاحة، يمكن تقدير دقة التعميم باستخدام العينات خارج الحقيبة من خلال تعيين "oob_score=True". وكمثال على ذلك، يوضح الجزء التالي من التعليمات البرمجية كيفية إنشاء مجموعة من المعايرة لمقدّرات KNeighborsClassifier، يتم بناء كل منها على مجموعات فرعية عشوائية من 50% من العينات و50% من السمات.

يتمثل المفهوم الأساسي وراء فئة VotingClassifier في دمج مصنفات التعلم الآلي المختلفة مفاهيمياً واستخدام التصويت بالأغلبية أو متوسط الاحتمالات المتوقعة (التصويت اللين) للتنبؤ بعلامات الفئات. ويمكن أن يكون مثل هذا المصنف مفيدًا لمجموعة من النماذج التي لها نفس الأداء الجيد من أجل موازنة نقاط ضعفها الفردية.

في التصويت بالأغلبية، تكون علامة الفئة المتوقعة لعينة معينة هي علامة الفئة التي تمثل أغلبية (الوضع) لعلامات الفئات التي تنبأ بها كل مصنف فردي.

على سبيل المثال، إذا كان التنبؤ لعينة معينة هو:

- المصنف 1 -> الفئة 1
- المصنف 2 -> الفئة 1
- المصنف 3 -> الفئة 2

فإن مصنف التصويت (مع "التصويت='الصعب'") سيصنف العينة على أنها "الفئة 1" بناءً على أغلبية علامات الفئات.

في حالة التعادل، سيختار مصنف التصويت الفئة بناءً على ترتيب الفرز التصاعدي. على سبيل المثال، في السيناريو التالي:

- المصنف 1 -> الفئة 2
- المصنف 2 -> الفئة 1

سيتم تعيين علامة الفئة 1 للعينة.

الاستخدام

يوضح المثال التالي كيفية ملاءمة مصنف قاعدة الأغلبية::

   >>> from sklearn import datasets
   >>> from sklearn.model_selection import cross_val_score
   >>> from sklearn.linear_model import LogisticRegression
   >>> from sklearn.naive_bayes import GaussianNB
   >>> from sklearn.ensemble import RandomForestClassifier
   >>> from sklearn.ensemble import VotingClassifier

   >>> iris = datasets.load_iris()
   >>> X, y = iris.data[:, 1:3], iris.target

   >>> clf1 = LogisticRegression(random_state=1)
   >>> clf2 = RandomForestClassifier(n_estimators=5, random_state=1)
   >>> clf3 = GaussianNB()

   >>> eclf = VotingClassifier(
   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
   ...     voting='hard')

   >>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):
   ...     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)
   ...     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
   Accuracy: 0.95 (+/- 0.04) [Logistic Regression]
   Accuracy: 0.94 (+/- 0.04) [Random Forest]
   Accuracy: 0.91 (+/- 0.04) [naive Bayes]
   Accuracy: 0.95 (+/- 0.04) [Ensemble]

على عكس التصويت بالأغلبية (التصويت الصعب)، يعيد التصويت اللين علامة الفئة كدالة argmax من مجموع الاحتمالات المتوقعة.

يمكن تعيين أوزان محددة لكل مصنف عبر معلمة "الأوزان". عندما يتم توفير الأوزان، يتم جمع الاحتمالات المتوقعة للفئة لكل مصنف، وضربها بوزن المصنف، ثم حساب متوسطها. يتم بعد ذلك اشتقاق علامة الفئة النهائية من علامة الفئة ذات أعلى متوسط احتمال.

ولتوضيح ذلك بمثال بسيط، دعونا نفترض أن لدينا 3 مصنفات ومشكلة تصنيف من 3 فئات نقوم فيها بتعيين أوزان متساوية لجميع المصنفات: w1=1، w2=1، w3=1.

سيتم بعد ذلك حساب متوسطات الأوزان الاحتمالية لعينة ما على النحو التالي:

================  ==========    ==========      ==========
المصنف          الفئة 1       الفئة 2         الفئة 3
================  ==========    ==========      ==========
المصنف 1	 	  w1 * 0.2      w1 * 0.5        w1 * 0.3
المصنف 2	 	  w2 * 0.6      w2 * 0.3        w2 * 0.1
المصنف 3         w3 * 0.3      w3 * 0.4        w3 * 0.3
المتوسط المرجح	 0.37	        0.4             0.23
================  ==========    ==========      ==========

هنا، تكون علامة الفئة المتوقعة هي 2، لأنها تمتلك أعلى متوسط احتمال.

يوضح المثال التالي كيف يمكن أن تتغير مناطق القرار عند استخدام مصنف تصويت لين يعتمد على مصنف Support Vector Machine خطي، وشجرة قرار، ومصنف K-nearest neighbor::

   >>> from sklearn import datasets
   >>> from sklearn.tree import DecisionTreeClassifier
   >>> from sklearn.neighbors import KNeighborsClassifier
   >>> from sklearn.svm import SVC
   >>> from itertools import product
   >>> from sklearn.ensemble import VotingClassifier

   >>> # تحميل بعض بيانات المثال
   >>> iris = datasets.load_iris()
   >>> X = iris.data[:, [0, 2]]
   >>> y = iris.target

   >>> # تدريب المصنفات
   >>> clf1 = DecisionTreeClassifier(max_depth=4)
   >>> clf2 = KNeighborsClassifier(n_neighbors=7)
   >>> clf3 = SVC(kernel='rbf', probability=True)
   >>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],
   ...                         voting='soft', weights=[2, 1, 2])

   >>> clf1 = clf1.fit(X, y)
   >>> clf2 = clf2.fit(X, y)
   >>> clf3 = clf3.fit(X, y)
   >>> eclf = eclf.fit(X, y)

الاستخدام

من أجل التنبؤ بعلامات الفئات بناءً على الاحتمالات المتوقعة للفئات (يجب أن تدعم مصنفات scikit-learn في مصنف التصويت طريقة "predict_proba")::

   >>> eclf = VotingClassifier(
   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
   ...     voting='soft'
   ... )

يمكن أيضًا توفير أوزان للمصنفات الفردية بشكل اختياري::

   >>> eclf = VotingClassifier(
   ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
   ...     voting='soft', weights=[2,5,1]
   ... )

يمكن أيضًا استخدام مصنف التصويت مع GridSearchCV من أجل ضبط فرط معلمات المصنفات الفردية::

    >>> from sklearn.model_selection import GridSearchCV
    >>> clf1 = LogisticRegression(random_state=1)
    >>> clf2 = RandomForestClassifier(random_state=1)
    >>> clf3 = GaussianNB()
    >>> eclf = VotingClassifier(
    ...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
    ...     voting='soft'
    ... )

    >>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}

    >>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
    >>> grid = grid.fit(iris.data, iris.target)

الفكرة الأساسية وراء فئة VotingRegressor هي دمج مراجعات التعلم الآلي المفاهيمية المختلفة وإرجاع المتوسطات المتوقعة. ويمكن أن يكون مثل هذا المرجع مفيدًا لمجموعة من النماذج التي لها نفس الأداء الجيد من أجل موازنة نقاط ضعفها الفردية.

الاستخدام

يوضح المثال التالي كيفية ملاءمة مصنف التصويت::

   >>> from sklearn.datasets import load_diabetes
   >>> from sklearn.ensemble import GradientBoostingRegressor
   >>> from sklearn.ensemble import RandomForestRegressor
   >>> from sklearn.linear_model import LinearRegression
   >>> from sklearn.ensemble import VotingRegressor

   >>> # تحميل بعض بيانات المثال
   >>> X, y = load_diabetes(return_X_y=True)

   >>> # تدريب المراجعات
   >>> reg1 = GradientBoostingRegressor(random_state=1)
   >>> reg2 = RandomForestRegressor(random_state=1)
   >>> reg3 = LinearRegression()
   >>> ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])
   >>> ereg = ereg.fit(X, y)

التعميم المكدس
التعميم المكدس هو طريقة لدمج المُقدّرين لتقليل انحيازاتهم. وبشكل أكثر دقة، يتم تكديس تنبؤات كل مُقدّر فردي واستخدامها كمدخلات لمُقدّر نهائي لحساب التنبؤ. يتم تدريب هذا المُقدّر النهائي من خلال التصديق المتبادل.

توفر كل من class: 'StackingClassifier' و class: 'StackingRegressor' مثل هذه الاستراتيجيات التي يمكن تطبيقها على مشكلات التصنيف والانحدار.

يرتبط معامل 'estimators' بقائمة المُقدّرين الذين يتم تكديسهم معًا بالتوازي على بيانات المدخلات. يجب إعطاؤه على شكل قائمة من الأسماء والمُقدّرين.

يستخدم المُقدّر النهائي 'final_estimator' تنبؤات 'estimators' كمدخلات. يجب أن يكون مصنفًا أو مُقدّر انحدار عند استخدام class: 'StackingClassifier' أو class: 'StackingRegressor'، على التوالي.

لتدريب المُقدّرين 'estimators' والمُقدّر النهائي 'final_estimator'، يجب استدعاء طريقة 'fit' على بيانات التدريب.

خلال التدريب، يتم ضبط المُقدّرين 'estimators' على كامل بيانات التدريب 'X_train'. سيتم استخدامها عند استدعاء طريقة 'predict' أو 'predict_proba'. ولتعميم وتجنب الإفراط في الضبط، يتم تدريب المُقدّر النهائي 'final_estimator' على عينات خارجية باستخدام طريقة 'sklearn.model_selection.cross_val_predict' داخليًا.

بالنسبة لـ class: 'StackingClassifier'، لاحظ أن إخراج المُقدّرين 'estimators' يتحكم فيه معامل 'stack_method' ويتم استدعاؤه بواسطة كل مُقدّر. هذا المعامل إما أن يكون سلسلة، أو أسماء طرق المُقدّر، أو 'auto' الذي سيحدد تلقائيًا طريقة متوفرة اعتمادًا على التوفر، ويتم اختباره حسب تفضيل: 'predict_proba'، أو 'decision_function'، أو 'predict'.

يمكن استخدام class: 'StackingRegressor' و class: 'StackingClassifier' كأي مُقدّر انحدار أو تصنيف آخر، مع عرض طريقة 'predict'، أو 'predict_proba'، أو 'decision_function'، على سبيل المثال.

لاحظ أنه من الممكن أيضًا الحصول على إخراج المُقدّرين المكدسين 'estimators' باستخدام طريقة 'transform'.

في الممارسة العملية، يتنبأ مُقدّر التكديس بنفس جودة أفضل مُقدّر في الطبقة الأساسية، بل ويتفوق عليه أحيانًا من خلال الجمع بين نقاط القوة المختلفة لهذه المُقدّرات. ومع ذلك، فإن تدريب مُقدّر التكديس مكلف من الناحية الحسابية.

بالنسبة لـ class: 'StackingClassifier'، عند استخدام 'stack_method_='predict_proba'، يتم إسقاط العمود الأول عندما تكون المشكلة هي مشكلة تصنيف ثنائي. في الواقع، كلا عمودي الاحتمالية التي يتنبأ بها كل مُقدّر متطابقان تمامًا.

يمكن تحقيق طبقات التكديس المتعددة من خلال تعيين 'final_estimator' إلى class: 'StackingClassifier' أو class: 'StackingRegressor'.

يحتوي نموذج sklearn.ensemble على خوارزمية التعزيز AdaBoost الشائعة، التي قدمها Freund و Schapire في عام 1995.

المبدأ الأساسي لـ AdaBoost هو ضبط تسلسل من المتعلمين الضعفاء (أي النماذج التي تكون أفضل بقليل من التخمين العشوائي، مثل أشجار القرار الصغيرة) على الإصدارات المعدلة بشكل متكرر من البيانات. ثم يتم الجمع بين التنبؤات من كل منهم من خلال تصويت الأغلبية المرجح (أو المجموع) لإنتاج التنبؤ النهائي. تتكون تعديلات البيانات في كل ما يسمى بتكرار التعزيز من تطبيق الأوزان :math: 'w_1'، :math: 'w_2'، ...، :math: 'w_N' على كل من عينات التدريب. في البداية، يتم تعيين جميع هذه الأوزان إلى :math: 'w_i = 1/N'، بحيث تقتصر الخطوة الأولى ببساطة على تدريب متعلم ضعيف على البيانات الأصلية. بالنسبة لكل تكرار لاحق، يتم تعديل الأوزان الفردية للنماذج ويتم إعادة تطبيق خوارزمية التعلم على البيانات المعاد ترجيحها. في خطوة معينة، يتم زيادة أوزان نماذج التدريب التي تم التنبؤ بها بشكل غير صحيح بواسطة النموذج المدعوم المستحث في الخطوة السابقة، في حين يتم تقليل الأوزان للنماذج التي تم التنبؤ بها بشكل صحيح. مع تقدم التكرارات، تتلقى الأمثلة التي يصعب التنبؤ بها تأثيرًا متزايدًا باستمرار. يتم إجبار كل متعلم ضعيف لاحق على التركيز على الأمثلة التي فاتته من قبل المتعلمين السابقين في التسلسل.

يمكن استخدام AdaBoost لكل من مشكلات التصنيف والانحدار:

- بالنسبة للتصنيف متعدد الفئات، ينفذ class: 'AdaBoostClassifier' خوارزمية AdaBoost.SAMME.

- للانحدار، ينفذ class: 'AdaBoostRegressor' خوارزمية AdaBoost.R2.

الاستخدام
-----

يُظهر المثال التالي كيفية ضبط مصنف AdaBoost مع 100 متعلم ضعيف::

من sklearn.model_selection استورد cross_val_score

من sklearn.datasets استورد load_iris

من sklearn.ensemble استورد AdaBoostClassifier

X، مجموعة بيانات load_iris (return_X_y = True)

clf = AdaBoostClassifier (n_estimators = 100، algorithm = "SAMME")

التدرجات = cross_val_score (clf، X، y، cv = 5)

التدرجات. mean ()

0.9...

يتم التحكم في عدد المتعلمين الضعفاء بواسطة معامل "n_estimators". يتحكم معامل "learning_rate" في مساهمة المتعلمين الضعفاء في المزيج النهائي. بشكل افتراضي، تكون المتعلمات الضعيفة هي جذوع القرار. يمكن تحديد متعلمين ضعفاء مختلفين من خلال معامل "estimator".

المعلمات الرئيسية التي يجب ضبطها للحصول على نتائج جيدة هي "n_estimators" ومدى تعقيد المُقدّرات الأساسية (على سبيل المثال، عمقها "max_depth" أو الحد الأدنى المطلوب من العينات للنظر في الانقسام "min_samples_split").