استخراج الخصائص
==================

يمكن استخدام وحدة sklearn.feature_extraction لاستخراج الخصائص بتنسيق مدعوم من خوارزميات التعلم الآلي من مجموعات البيانات المكونة من تنسيقات مثل النص والصورة.

.. note::

   يختلف استخراج الميزات اختلافًا كبيرًا عن اختيار الميزة: يتكون السابق من تحويل البيانات التعسفية، مثل النص أو الصور، إلى ميزات رقمية يمكن استخدامها للتعلم الآلي. هذا الأخير هو تقنية تعلم آلي يتم تطبيقها على هذه الميزات.

تحميل الميزات من القواميس
===========================

يمكن استخدام فئة DictVectorizer لتحويل صفائف الميزات التي يتم تمثيلها على أنها قوائم من كائنات القاموس Python القياسية إلى تمثيل NumPy/SciPy الذي تستخدمه خوارزميات التعلم الآلي.

في حين أن معالجة Python ليست سريعة بشكل خاص، فإن لـ "dict" ميزة كونها مريحة للاستخدام، ومتفرقة (لا يلزم تخزين الميزات الغائبة) وتخزين أسماء الميزات بالإضافة إلى القيم.

تنفذ DictVectorizer ما يسمى برمز "one-of-K" أو "one-hot" للميزات الفئوية (المعروفة أيضًا باسم الاسمية أو المنفصلة). الميزات الفئوية هي أزواج "attribute-value" حيث تكون القيمة مقيدة بقائمة من الإمكانات المنفصلة دون ترتيب (على سبيل المثال، معرفات الموضوع، وأنواع الكائنات، والعلامات، والأسماء...).

في ما يلي، "المدينة" هي سمة فئوية في حين أن "درجة الحرارة" هي ميزة رقمية تقليدية::

  >>> القياسات = [
  ...     {'city': 'Dubai', 'temperature': 33.}،
  ...     {'city': 'London', 'temperature': 12.}،
  ...     {'city': 'سان فرانسيسكو'، 'درجة الحرارة': 18.}،
  ... ]

  >>> من sklearn.feature_extraction import DictVectorizer
  >>> فيك = DictVectorizer ()

  >>> vec.fit_transform (القياسات). toarray ()
  الصفيف ([[1.، 0.، 0.، 33.]،
          [0.، 1.، 0.، 12.]،
          [0.، 0.، 1.، 18.]])

  >>> vec.get_feature_names_out ()
  الصفيف (['city = دبي'، 'city = لندن'، 'city = سان فرانسيسكو'، 'درجة الحرارة']، ...)

تقبل DictVectorizer قيم سلاسل متعددة لميزة واحدة، مثل فئات متعددة لفيلم.

نفترض أن قاعدة بيانات تصنف كل فيلم باستخدام بعض الفئات (غير إلزامية) وسنة إصداره.

    >>> movie_entry = [{'category': ['thriller', 'drama'], 'year': 2003}،
    ...                {'category': ['animation', 'family'], 'year': 2011}،
    ...                {'year': 1974}]
    >>> vec.fit_transform (movie_entry). toarray ()
    الصفيف ([[0.000e+00، 1.000e+00، 0.000e+00، 1.000e+00، 2.003e+03]،
           [1.000e+00، 0.000e+00، 1.000e+00، 0.000e+00، 2.011e+03]،
           [0.000e+00، 0.000e+00، 0.000e+00، 0.000e+00، 1.974e+03]])
    >>> vec.get_feature_names_out ()
    الصفيف (['category = animation'، 'category = drama'، 'category = family'،
           'category = thriller'، 'year']، ...)
    >>> vec.transform ({'category': ['thriller']،
    ...                'unseen_feature': '3'}). toarray ()
    الصفيف ([[0.، 0.، 0.، 1.، 0.]])

DictVectorizer هو أيضًا تحويل تمثيل مفيد
لتدريب المصنفات التسلسلية في نماذج معالجة اللغة الطبيعية
التي تعمل عادة عن طريق استخراج نوافذ الميزات حول كلمة معينة ذات أهمية.

على سبيل المثال، افترض أن لدينا خوارزمية أولى تستخرج علامات جزء من الكلام التي نريد استخدامها كعلامات تكميلية لتدريب مصنف تسلسلي (مثل chunker). يمكن أن يكون القاموس التالي نافذة من الميزات المستخرجة حول كلمة "sat" في الجملة "The cat sat on the mat."::

  >>> pos_window = [
  ...     {
  ...         'word-2': 'the'،
  ...         'pos-2': 'DT'،
  ...         'word-1': 'cat'،
  ...         'pos-1': 'NN'،
  ...         'word+1': 'on'،
  ...         'pos+1': 'PP'،
  ...     }،
  ...     # في تطبيق حقيقي سيتم استخراج العديد من هذه القواميس
  ... ]

يمكن تحويل هذا الوصف إلى مصفوفة ثنائية الأبعاد متفرقة مناسبة للتغذية في مصنف (ربما بعد أن يتم تمريرها عبر TfidfTransformer للتوحيد)::

  >>> vec = DictVectorizer ()
  >>> pos_vectorized = vec.fit_transform (pos_window)
  >>> pos_vectorized
  <Compressed Sparse ... dtype = 'float64'
    مع 6 عناصر مخزنة وشكل (1، 6)>
  >>> pos_vectorized. toarray ()
  الصفيف ([[1.، 1.، 1.، 1.، 1.، 1.]])
  >>> vec.get_feature_names_out ()
  الصفيف (['pos+1 = PP'، 'pos-1 = NN'، 'pos-2 = DT'، 'word+1 = on'، 'word-1 = cat'،
         'word-2 = the']، ...)

كما يمكنك أن تتخيل، إذا قمت باستخراج مثل هذا السياق حول كل كلمة فردية
في مجموعة من الوثائق، ستكون المصفوفة الناتجة عريضة جدًا
(الكثير من الميزات ذات الرمز الواحد) مع معظمها بقيمة صفر في معظم الوقت.
لذلك، لكي يتمكن هيكل البيانات الناتج من الاحتفاظ بالذاكرة،
تستخدم فئة DictVectorizer مصفوفة "scipy.sparse" بشكل افتراضي بدلاً من "numpy.ndarray".


تجزئة الميزات
===============

.. currentmodule:: sklearn.feature_extraction

فئة FeatureHasher هي أداة لتجزئة الميزات عالية السرعة ومنخفضة الذاكرة
تستخدم تقنية تسمى تجزئة الميزات، أو "حيلة التجزئة".
بدلاً من بناء جدول تجزئة للميزات التي تمت مواجهتها في التدريب،
كما تفعل أدوات تجزئة الميزات،
تطبق مثيلات FeatureHasher دالة تجزئة على الميزات
لتحديد مؤشر عمودها في مصفوفات العينات مباشرة.
والنتيجة هي زيادة السرعة وانخفاض استخدام الذاكرة،
على حساب قابلية الفحص؛
لا يتذكر المجزئ شكل الميزات المدخلة
وليس لديه طريقة "inverse_transform".

نظرًا لأن دالة التجزئة قد تتسبب في حدوث تصادمات بين الميزات (غير ذات الصلة)،
يتم استخدام دالة تجزئة موقعة والقيمة الموقعة لدالة التجزئة
تحديد علامة القيمة المخزنة في المصفوفة الإخراجية لميزة.
بهذه الطريقة، من المحتمل أن تلغي الاصطدامات بعضها البعض بدلاً من تراكم الأخطاء،
ومتوسط ​​أي قيمة ميزة إخراجية متوقع هو صفر. يتم تمكين هذه الآلية بشكل افتراضي مع "alternate_sign=True" وهي مفيدة بشكل خاص
لحجم جدول التجزئة الصغير ("n_features <10000"). بالنسبة لأحجام جداول التجزئة الكبيرة،
يمكن تعطيله، للسماح بالإخراج ليتم تمريره إلى خوارزميات مثل
MultinomialNB أو
خوارزميات اختيار الميزات التي تتوقع مدخلات غير سالبة.

تقبل FeatureHasher إما الخرائط
(مثل "dict" في Python ومتغيراته في وحدة "collections")،
أزواج "(الميزة، القيمة)"، أو السلاسل،
اعتمادًا على معلمة "input_type" في الباني.
يتم التعامل مع الخرائط على أنها قوائم من أزواج "(الميزة، القيمة)"،
في حين أن السلاسل الفردية لها قيمة ضمنية تبلغ 1،
لذلك يتم تفسير "['feat1'، 'feat2'، 'feat3']" على أنها
"[(feat1، 1)، (feat2، 1)، (feat3، 1)]".
إذا حدثت ميزة واحدة متعددة المرات في عينة،
سيتم جمع القيم المرتبطة
(لذلك تصبح ("feat"، 2) و ("feat"، 3.5) "feat"، 5.5).
الإخراج من FeatureHasher هو دائمًا مصفوفة "scipy.sparse"
في تنسيق CSR.

يمكن استخدام تجزئة الميزات في تصنيف المستندات،
ولكن على عكس CountVectorizer،
لا يقوم FeatureHasher بتجزئة الكلمات
أو أي معالجة مسبقة أخرى باستثناء الترميز Unicode-to-UTF-8؛
راجع hashing_vectorizer أدناه، لمجزئ/مجزئ مجمع.

على سبيل المثال، ضع في اعتبارك مهمة معالجة اللغة الطبيعية على مستوى الكلمات
التي تحتاج إلى استخراج ميزات من أزواج "(token، part_of_speech)".
يمكن استخدام دالة مولد Python لاستخراج الميزات::

  def token_features(token، part_of_speech):
      if token.isdigit():
          yield "numeric"
      else:
          yield "token={}".format(token.lower())
          yield "token,pos={},{}".format(token، part_of_speech)
      if token[0].isupper():
          yield "uppercase_initial"
      if token.isupper():
          yield "all_uppercase"
      yield "pos={}".format(part_of_speech)

بعد ذلك، يمكن بناء "raw_X" ليتم تغذيته في "FeatureHasher.transform"
يمكن بناؤه باستخدام::

  raw_X = (token_features (tok، pos_tagger (tok)) لـ tok في corpus)

وإطعامها إلى مجزئ باستخدام::

  hasher = FeatureHasher(input_type='string')
  X = hasher.transform(raw_X)

للحصول على مصفوفة "scipy.sparse" "X".

لاحظ استخدام تعبير المولد،
الذي يقدم الكسل في استخراج الميزات:
يتم معالجة الرموز فقط عند الطلب من المجزئ.

.. dropdown:: تفاصيل التنفيذ

  تستخدم FeatureHasher متغير MurmurHash3 المكون من 32 بت.
  ونتيجة لذلك (وبسبب القيود في "scipy.sparse")،
  فإن الحد الأقصى لعدد الميزات المدعومة حاليًا هو: 2 ^ 31 - 1.

  استخدمت الصيغة الأصلية لحيلة التجزئة بواسطة Weinberger et al.
  استخدم دالتين منفصلتين للتجزئة: h و xi
  لتحديد مؤشر العمود وعلامة ميزة، على التوالي.
  يفترض التنفيذ الحالي
  أن بت العلامة في MurmurHash3 مستقل عن بتاته الأخرى.

  نظرًا لأنه يتم استخدام الباقي البسيط لتحويل دالة التجزئة إلى مؤشر عمود،
  من المستحسن استخدام قوة العدد اثنين كمعلمة "n_features"؛
  وإلا فلن يتم تعيين الميزات بالتساوي إلى الأعمدة.

  .. rubric:: المراجع

  * `MurmurHash3 <https://github.com/aappleby/smhasher>`_.


.. rubric:: المراجع

* كيليان وينبرجر، أنيربان داسجوبتا، جون لانجفورد، أليكس سمولا وجوش أتينبيرج (2009). `تجزئة الميزات للتعلم متعدد المهام على نطاق واسع
  <https://alex.smola.org/papers/2009/Weinbergeretal09.pdf>`_. Proc. ICML.

استخراج ميزات النص
تمثيل "كيس من الكلمات"
------------------------------------

يعد تحليل النص مجال تطبيق رئيسيًا لخوارزميات التعلم الآلي. ومع ذلك، لا يمكن إدخال البيانات الخام، وهي تسلسل من الرموز، مباشرة إلى الخوارزميات نفسها حيث أن معظمها يتوقع متجهات ميزات رقمية ذات حجم ثابت بدلاً من وثائق النص الخام ذات الطول المتغير.

ولمعالجة هذا الأمر، يوفر scikit-learn وظائف مساعدة لأكثر الطرق شيوعًا لاستخراج الميزات الرقمية من المحتوى النصي، وهي:

- **تجزئة** السلاسل النصية وإعطاء معرف رقمي لكل رمز محتمل، على سبيل المثال باستخدام المسافات البيضاء وعلامات الترقيم كفاصلات للرموز.

- **حساب** عدد مرات ظهور الرموز في كل وثيقة.

- **تطبيع** ووزن الرموز التي تظهر في غالبية العينات/الوثائق وتقليل أهميتها.

في هذا المخطط، يتم تعريف الميزات والعينات على النحو التالي:

- يتم التعامل مع كل **تكرار رمز فردي** (سواء كان مطبعيًا أم لا) كميزة **مميزة**.

- يتم اعتبار متجه جميع تكرارات الرموز لوثيقة معينة كعينة **متعددة المتغيرات**.

وبالتالي، يمكن تمثيل مجموعة من الوثائق بمصفوفة تحتوي على صف واحد لكل وثيقة وعمود واحد لكل رمز (مثل الكلمة) يحدث في المجموعة.

نطلق على **التمثيل الرقمي** العملية العامة لتحويل مجموعة من وثائق النص إلى متجهات ميزات رقمية. وتسمى هذه الاستراتيجية المحددة (تجزئة الكلمات وحسابها وتطبيعها) باسم **كيس من الكلمات** أو تمثيل "كيس من n-grams". يتم وصف الوثائق من خلال تكرار الكلمات مع تجاهل معلومات الموضع النسبي للكلمات في الوثيقة تمامًا.

نسبة التخلخل
--------

نظرًا لأن معظم الوثائق تستخدم عادةً مجموعة فرعية صغيرة جدًا من الكلمات المستخدمة في المجموعة، فستكون للمصفوفة الناتجة العديد من قيم الميزات التي تساوي الصفر (عادة أكثر من 99% منها).

على سبيل المثال، ستستخدم مجموعة من 10000 وثيقة نصية قصيرة (مثل رسائل البريد الإلكتروني) مفردات بحجم 100000 كلمة فريدة من نوعها في المجموع، بينما ستستخدم كل وثيقة من 100 إلى 1000 كلمة فريدة من نوعها بشكل فردي.

ولكي يكون من الممكن تخزين مثل هذه المصفوفة في الذاكرة ولكن أيضًا لتسريع العمليات الجبرية للمصفوفة/المتجه، عادة ما تستخدم التطبيقات تمثيلًا متفرقًا مثل التطبيقات المتوفرة في حزمة ``scipy.sparse``.

الاستخدام الشائع للتمثيل الرقمي
-----------------------

تنفذ :class:`CountVectorizer` كل من تجزئة الكلمات وحساب التكرارات في فئة واحدة::

  >>> from sklearn.feature_extraction.text import CountVectorizer

يحتوي هذا النموذج على العديد من المعلمات، ولكن القيم الافتراضية معقولة جدًا (يرجى الاطلاع على الوثائق المرجعية <feature_extraction_ref-from-text> للحصول على التفاصيل)::

  >>> vectorizer = CountVectorizer()
  >>> vectorizer
  CountVectorizer()

دعنا نستخدمه لتجزئة مجموعة من وثائق النص وتحديد عدد مرات ظهور الكلمات::

  >>> corpus = [
  ...     'This is the first document.',
  ...     'This is the second second document.',
  ...     'And the third one.',
  ...     'Is this the first document?',
  ... ]
  >>> X = vectorizer.fit_transform(corpus)
  >>> X
  <1x9 sparse matrix of type ''
    with 19 stored elements at ...>

يقوم التكوين الافتراضي بتجزئة السلسلة النصية عن طريق استخراج الكلمات التي تتكون من حرفين على الأقل. يمكن طلب الدالة المحددة التي تقوم بهذه الخطوة بشكل صريح::

  >>> analyze = vectorizer.build_analyzer()
  >>> analyze("This is a text document to analyze.") == (
  ...     ['this', 'is', 'text', 'document', 'to', 'analyze'])
  True

يتم تعيين كل مصطلح يعثر عليه المحلل أثناء التثبيت إلى فهرس رقمي فريد مطابق لعمود في المصفوفة الناتجة. يمكن استرداد هذا التفسير للأعمدة على النحو التالي::

  >>> vectorizer.get_feature_names_out()
  array(['and', 'document', 'first', 'is', 'one', 'second', 'the',
         'third', 'this'], ...)

  >>> X.toarray()
  array([[0, 1, 1, 1, 0, 0, 1, 0, 1],
         [0, 1, 0, 1, 0, 2, 1, 0, 1],
         [1, 0, 0, 0, 1, 0, 1, 1, 0],
         [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)

يتم تخزين الخريطة العكسية من اسم الميزة إلى فهرس العمود في سمة ``vocabulary_`` للتمثيل الرقمي::

  >>> vectorizer.vocabulary_.get('document')
  1

وبالتالي، يتم تجاهل الكلمات التي لم يتم رؤيتها في مجموعة التدريب تمامًا في الاستدعاءات المستقبلية لأسلوب التحويل::

  >>> vectorizer.transform(['Something completely new.']).toarray()
  array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)

لاحظ أنه في المجموعة السابقة، تحتوي الوثيقتان الأولى والأخيرة على نفس الكلمات تمامًا، وبالتالي يتم تشفيرهما في متجهات متطابقة. على وجه الخصوص، نفقد المعلومات التي تشير إلى أن الوثيقة الأخيرة هي صيغة استفهام. للحفاظ على بعض معلومات ترتيب الكلمات المحلية، يمكننا استخراج 2-grams من الكلمات بالإضافة إلى 1-grams (كلمات فردية)::

  >>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),
  ...                                     token_pattern=r'\b\w+\b', min_df=1)
  >>> analyze = bigram_vectorizer.build_analyzer()
  >>> analyze('Bi-grams are cool!') == (
  ...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])
  True

وبالتالي، فإن المفردات التي يستخرجها هذا التمثيل الرقمي أكبر بكثير ويمكنها الآن حل الغموض المشفر في أنماط الموضع المحلي::

  >>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()
  >>> X_2
  array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],
         [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],
         [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],
         [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)


على وجه الخصوص، فإن الصيغة الاستفهامية "Is this" موجودة فقط في الوثيقة الأخيرة::

  >>> feature_index = bigram_vectorizer.vocabulary_.get('is this')
  >>> X_2[:, feature_index]
  array([0, 0, 0, 1]...)

.. _stop_words:

استخدام كلمات التوقف
----------------

كلمات التوقف هي كلمات مثل "and" و "the" و "him"، والتي يفترض أنها غير مفيدة في تمثيل محتوى النص، والتي يمكن إزالتها لتجنب اعتبارها إشارة للتنبؤ. ومع ذلك، في بعض الأحيان، تكون الكلمات المماثلة مفيدة للتنبؤ، مثل تصنيف أسلوب الكتابة أو الشخصية.

هناك العديد من المشكلات المعروفة في قائمة كلمات التوقف باللغة الإنجليزية التي نوفرها. لا تهدف إلى أن تكون حلًا عامًا "واحدا يناسب الجميع"، حيث قد تتطلب بعض المهام حلاً مخصصًا أكثر. راجع [NQY18]_ لمزيد من التفاصيل.

يرجى توخي الحذر عند اختيار قائمة كلمات التوقف. قد تتضمن قوائم كلمات التوقف الشائعة كلمات تكون مفيدة جدًا لبعض المهام، مثل *الكمبيوتر*.

يجب أيضًا التأكد من أن قائمة كلمات التوقف قد خضعت لنفس المعالجة والتجزئة المطبقة في التمثيل الرقمي. يتم تقسيم كلمة *we've* إلى *we* و *ve* بواسطة محلل التجزئة الافتراضي لـ CountVectorizer، لذا إذا كانت *we've* موجودة في ``stop_words``، ولكن *ve* غير موجودة، فسيتم الاحتفاظ بـ *ve* من *we've* في النص المحول. ستحاول تمثيلنا الرقمي تحديد بعض أنواع عدم الاتساق والتحذير منها.

.. rubric:: المراجع

.. [NQY18] J. Nothman، H. Qin و R. Yurchak (2018).
   `"Stop Word Lists in Free Open-source Software Packages"
   <https://aclweb.org/anthology/W18-2502>`__.
   في *وقائع ورشة عمل البرمجيات مفتوحة المصدر لمعالجة اللغات الطبيعية*.


.. _tfidf:

ترجيح المصطلحات Tf-idf
في مجموعة كبيرة من النصوص، ستكون بعض الكلمات موجودة بكثرة (على سبيل المثال: "the"، "a"، "is" في اللغة الإنجليزية)، وبالتالي لن تحمل الكثير من المعلومات المفيدة حول المحتوى الفعلي للوثيقة. إذا قمنا بتغذية بيانات العد المباشر مباشرة إلى مصنف، فستحجب هذه المصطلحات الشائعة جداً تكرارات المصطلحات الأندر والأكثر أهمية.

ولإعادة وزن ميزات العد إلى قيم ذات نقطة عائمة مناسبة للاستخدام بواسطة مصنف، من الشائع جدًا استخدام تحويل التردد-معكوس للوثيقة (tf-idf).

يشير "tf" إلى تردد المصطلح بينما يشير "tf-idf" إلى تردد المصطلح مضروبًا في معكوس تكرار الوثيقة:

:math: 'text{tf-idf(t,d)}=text{tf(t,d)} \times text{idf(t)}'.

باستخدام الإعدادات الافتراضية لـ "TfidfTransformer"، "TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)"، يتم ضرب تردد المصطلح، وهو عدد المرات التي يظهر فيها المصطلح في وثيقة معينة، في المكون "idf"، والذي يتم حسابه على النحو التالي:

:math: 'text{idf} (t) = log {\frac {1 + n} {1+text{df} (t)}} + 1'

حيث :math: 'n' هو العدد الإجمالي للوثائق في مجموعة الوثائق، و:math: 'text{df} (t)' هو عدد الوثائق في مجموعة الوثائق التي تحتوي على المصطلح :math: 't'. يتم بعد ذلك تطبيع المتجهات الناتجة عن طريق القيمة الخاصة بالقاعدة:

:math: 'v_ {norm} = \frac {v} {|| v || _2} = \frac {v} {\sqrt {v_ {1} ^ 2 + v_ {2} ^ 2 + \ dots + v_ {n} ^ 2}}'.

كانت هذه في الأصل خطة ترجيح المصطلحات التي تم تطويرها لاسترجاع المعلومات (كدالة ترتيب لنتائج محركات البحث) والتي وجدت أيضًا استخدامًا جيدًا في تصنيف الوثائق وتجميعها.

تحتوي الأقسام التالية على مزيد من التوضيحات والأمثلة التي توضح كيفية حساب قيم "tf-idf" بالضبط، وكيف تختلف قيم "tf-idf" المحسوبة في "scikit-learn's TfidfTransformer" و "TfidfVectorizer" اختلافًا طفيفًا عن الترميز القياسي في الكتب المدرسية الذي يُعرِّف "idf" على النحو التالي:

:math: 'text{idf} (t) = log {\frac {n} {1+text{df} (t)}}'.


في "TfidfTransformer" و "TfidfVectorizer" مع "smooth_idf=False"، يتم إضافة العدد "1" إلى "idf" بدلاً من مقام "idf":

:math: 'text{idf} (t) = log {\frac {n} {text{df} (t)}} + 1'

يتم تنفيذ هذا التطبيع بواسطة فئة "TfidfTransformer":

>>> from sklearn.feature_extraction.text import TfidfTransformer
>>> transformer = TfidfTransformer(smooth_idf=False)
>>> transformer
TfidfTransformer(smooth_idf=False)

يرجى الرجوع إلى وثائق المرجع <feature_extraction_ref-from-text> للحصول على التفاصيل حول جميع المعلمات.

مثال رقمي لمصفوفة "tf-idf"

لنأخذ مثالًا باستخدام العد التالي. المصطلح الأول موجود بنسبة 100% من الوقت وبالتالي فهو غير مثير للاهتمام. الميزتان الأخريان موجودتان في أقل من 50% من الوقت وبالتالي من المحتمل أن تكونا أكثر تمثيلاً لمحتوى الوثائق:

>>> counts = [[3, 0, 1],
... [2, 0, 0],
... [3, 0, 0],
... [4, 0, 0],
... [3, 2, 0],
... [3, 0, 2]]
...
>>> tfidf = transformer.fit_transform(counts)
>>> tfidf
<Compressed Sparse...dtype 'float64'
  with 9 stored elements and shape (6, 3)>

>>> tfidf.toarray()
array([[0.81940995, 0.، 0.57320793]،
 [1.، 0.، 0.]،
 [1.، 0.، 0.]،
 [1.، 0.، 0.]،
 [0.47330339، 0.88089948، 0.]،
 [0.58149261، 0.، 0.81355169]])

يتم تطبيع كل صف ليصبح له قاعدة القيمة الخاصة به:

:math: 'v_ {norm} = \frac {v} {|| v || _2} = \frac {v} {\sqrt {v_ {1} ^ 2 + v_ {2} ^ 2 + \ dots + v_ {n} ^ 2}}'

على سبيل المثال، يمكننا حساب "tf-idf" للمصطلح الأول في الوثيقة الأولى في مصفوفة "counts" كما يلي:

:math: 'n = 6'

:math: 'text{df} (t) _ {text {term1}} = 6'

:math: 'text{idf} (t) _ {text {term1}} = log \ frac {n} {text {df} (t)} + 1 = log (1) + 1 = 1'

:math: 'text{tf-idf} _ {text {term1}} = text {tf} \ times text {idf} = 3 \ times 1 = 3'

الآن، إذا كررنا هذا الحساب للمصطلحين المتبقيين في الوثيقة، نحصل على ما يلي:

:math: 'text{tf-idf} _ {text {term2}} = 0 \ times (\ log (6/1) + 1) = 0'

:math: 'text{tf-idf} _ {text {term3}} = 1 \ times (\ log (6/2) + 1) \ approx 2.0986'

ومتجه "tf-idf" الخام:

:math: 'text{tf-idf} _ {text {raw}} = [3، 0، 2.0986].'


بعد ذلك، من خلال تطبيق قاعدة القيمة (L2)، نحصل على قيم "tf-idf" التالية للوثيقة 1:

:math: '\frac { [3، 0، 2.0986]} {\ sqrt {\ big (3 ^ 2 + 0 ^ 2 + 2.0986 ^ 2 \ big)}} = [0.819، 0، 0.573].'

علاوة على ذلك، تضيف المعلمة الافتراضية "smooth_idf=True" العدد "1" إلى البسط والمقام كما لو كانت هناك وثيقة إضافية تحتوي على كل مصطلح في المجموعة مرة واحدة بالضبط، مما يمنع حدوث انقسامات صفرية:

:math: 'text{idf} (t) = log {\frac {1 + n} {1+text{df} (t)}} + 1'

باستخدام هذا التعديل، يتغير "tf-idf" للمصطلح الثالث في الوثيقة 1 إلى 1.8473:

:math: 'text{tf-idf} _ {text {term3}} = 1 \ times log (7/3) + 1 \ approx 1.8473'

وتتغير قيمة "tf-idf" بعد تطبيق قاعدة القيمة (L2) إلى:

:math: '\frac { [3، 0، 1.8473]} {\ sqrt {\ big (3 ^ 2 + 0 ^ 2 + 1.8473 ^ 2 \ big)}} = [0.8515، 0، 0.5243]'

>>> transformer = TfidfTransformer()
>>> transformer.fit_transform(counts).toarray()
array([[0.85151335، 0.، 0.52433293]،
 [1.، 0.، 0.]،
 [1.، 0.، 0.]،
 [1.، 0.، 0.]،
 [0.55422893، 0.83236428، 0.]،
 [0.63035731، 0.، 0.77630514]])

يتم تخزين أوزان كل ميزة محسوبة بواسطة طريقة "fit" في خاصية نموذج:

>>> transformer.idf_
array ([1. ...، 2.25 ...، 1.84 ...])

نظرًا لأن "tf-idf" يتم استخدامه غالبًا لميزات النص، هناك أيضًا فئة أخرى تسمى "TfidfVectorizer" تجمع بين جميع خيارات "CountVectorizer" و "TfidfTransformer" في نموذج واحد:

>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> vectorizer = TfidfVectorizer()
>>> vectorizer.fit_transform(corpus)
<Compressed Sparse...dtype 'float64'
  with 19 stored elements and shape (4, 9)>

في حين أن تطبيع "tf-idf" مفيد جدًا في كثير من الأحيان، فقد تكون هناك حالات تكون فيها مؤشرات التكرار الثنائية أفضل. يمكن تحقيق ذلك باستخدام معلمة "binary" في فئة "CountVectorizer". وعلى وجه الخصوص، تقوم بعض الخوارزميات مثل "bernoulli_naive_bayes" بوضع نماذج للمتغيرات العشوائية الثنائية المنفصلة. أيضًا، من المحتمل أن تحتوي النصوص القصيرة جدًا على قيم "tf-idf" ضوضائية بينما تكون معلومات التكرار الثنائية أكثر استقرارًا.

وكالعادة، فإن أفضل طريقة لتعديل معلمات استخراج الميزات هي استخدام البحث الشبكي المعبر عن طريق أنابيب ميزة الاستخراج مع مصنف:

* :ref: 'sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py'


فك تشفير ملفات النص
-------------------
يتكون النص من أحرف، ولكن تتكون الملفات من بايتات. تمثل هذه البايتات الأحرف وفقًا لبعض الترميزات. للعمل مع ملفات النص في بايثون، يجب "فك تشفير" بايتاتها إلى مجموعة أحرف تسمى يونيكود (Unicode).

تشمل الترميزات الشائعة ASCII، وLatin-1 (أوروبا الغربية)، وKOI8-R (الروسية)، والترميزات العالمية UTF-8 وUTF-16. وهناك العديد من الترميزات الأخرى.

.. note::
يمكن أيضًا أن يُطلق على الترميز اسم "مجموعة الأحرف"، ولكن هذا المصطلح أقل دقة: فقد توجد عدة ترميزات لمجموعة أحرف واحدة.

تعرف فئات استخراج ميزات النص في "scikit-learn" كيفية فك تشفير ملفات النص، ولكن فقط إذا أخبرتها بترميز الملفات. تأخذ فئة "CountVectorizer" معلمة "encoding" لهذا الغرض. وبالنسبة لملفات النص الحديثة، يكون الترميز الصحيح على الأرجح هو UTF-8، وهو الترميز الافتراضي (''encoding="utf-8"'').

ومع ذلك، إذا لم يكن النص الذي تقوم بتحميله مشفرًا بـ UTF-8، فستحصل على خطأ "UnicodeDecodeError". يمكن إخبار الفئات المُنشئة بتجاهل أخطاء فك التشفير من خلال تعيين معلمة "decode_error" إما إلى "ignore" أو "replace". راجع وثائق دالة بايثون "bytes.decode" للحصول على مزيد من التفاصيل (اكتب "help(bytes.decode)" في موجه بايثون).

.. dropdown:: استكشاف أخطاء فك تشفير النص وإصلاحها

إذا كنت تواجه مشكلة في فك تشفير النص، فجرّب ما يلي:

- اكتشف الترميز الفعلي للنص. قد يأتي الملف برأس أو ملف "README" يخبرك بالترميز، أو قد يكون هناك ترميز قياسي يمكنك افتراضه بناءً على مصدر النص.

- قد تتمكن من معرفة نوع الترميز بشكل عام باستخدام أمر UNIX "file". تأتي وحدة بايثون "chardet" مع برنامج نصي يسمى "chardetect.py" يمكنه تخمين الترميز المحدد، على الرغم من أنه لا يمكن الاعتماد على تخمينه الصحيح.

- يمكنك تجربة UTF-8 وتجاهل الأخطاء. يمكنك فك تشفير سلاسل البايتات باستخدام "bytes.decode(errors='replace')" لاستبدال جميع أخطاء فك التشفير بحرف غير ذي معنى، أو يمكنك تعيين "decode_error='replace'" في الفئات المُنشئة. قد يتسبب هذا في تلف ميزاتك.

- قد يأتي النص الفعلي من مجموعة متنوعة من المصادر التي قد تكون قد استخدمت ترميزات مختلفة، أو حتى تم فك تشفيرها بشكل غير صحيح بترميز مختلف عن الترميز الذي تم تشفيره به. وهذا أمر شائع في النص المسترد من الويب. يمكن لحزمة بايثون "ftfy" <https://github.com/LuminosoInsight/python-ftfy> تلقائيًا فرز بعض فئات أخطاء فك التشفير، لذا يمكنك تجربة فك تشفير النص غير المعروف كـ "latin-1" ثم استخدام "ftfy" لإصلاح الأخطاء.

- إذا كان النص مزيجًا من الترميزات يصعب فرزه (كما هو الحال في مجموعة بيانات "20 Newsgroups")، فيمكنك استخدام ترميز أحادي البايت مثل "latin-1" كملاذ أخير. قد يتم عرض بعض النصوص بشكل غير صحيح، ولكن سيمثل تسلسل البايتات نفسه دائمًا نفس الميزة.

على سبيل المثال، يستخدم المقتطف التالي وحدة "chardet" (غير مضمنة مع "scikit-learn"، يجب تثبيتها بشكل منفصل) لمعرفة ترميز ثلاثة نصوص. ثم تقوم بتمثيل النصوص بشكل شعاعي وطباعة المفردات المكتسبة. لم يتم إظهار الإخراج هنا.

>>> import chardet    # doctest: +SKIP
>>> text1 = b"Sei mir gegr\xc3\xbc\xc3\x9ft mein Sauerkraut"
>>> text2 = b"holdselig sind deine Ger\xfcche"
>>> text3 = b"\xff\xfeA\x00u\x00f\x00 \x00F\x00l\x00\xfc\x00g\x00e\x00l\x00n\x00 \x00d\x00e\x00s\x00 \x00G\x00e\x00s\x00a\x00n\x00g\x00e\x00s\x00,\x00 \x00H\x00e\x00r\x00z\x00l\x00i\x00e\x00b\x00c\x00h\x00e\x00n\x00,\x00 \x00t\x00r\x00a\x00g\x00 \x00i\x00c\x00h\x00 \x00d\x00i\x00c\x00h\x00 \x00f\x00o\x00r\x00t\x00"
>>> decoded = [x.decode(chardet.detect(x)['encoding'])
... for x in (text1, text2, text3)]        # doctest: +SKIP
>>> v = CountVectorizer().fit(decoded).vocabulary_    # doctest: +SKIP
>>> for term in v: print(v)                           # doctest: +SKIP

(بناءً على إصدار "chardet"، فقد تخطئ في الأول.)

للحصول على مقدمة حول يونيكود وترميزات الأحرف بشكل عام، راجع "Absolute Minimum Every Software Developer Must Know About Unicode" لجويل سبولسكي <https://www.joelonsoftware.com/articles/Unicode.html>.


التطبيقات والأمثلة
-------------------------

تمثيل "Bag of Words" بسيط للغاية ولكنه مفيد جدًا في الممارسة العملية.

على وجه الخصوص، في سياق **الإشراف**، يمكن دمجه بنجاح مع النماذج الخطية السريعة والقابلة للتطوير لتدريب **مصنفات الوثائق**، على سبيل المثال:

* :ref
مجموعة من الوحدات (ما هي حقيبة الكلمات) لا يمكن أن تلتقط العبارات والتعبيرات متعددة الكلمات، وتتجاهل فعليًا أي اعتماد على ترتيب الكلمات. بالإضافة إلى ذلك، لا يراعي نموذج حقيبة الكلمات الأخطاء الإملائية المحتملة أو اشتقاقات الكلمات.

n-grams إلى الإنقاذ! بدلاً من بناء مجموعة بسيطة من الوحدات (n=1)، قد يفضل المرء مجموعة من bigrams (n=2)، حيث يتم حساب تكرار ظهور أزواج من الكلمات المتتالية.

قد يفضل المرء بدلاً من ذلك مجموعة من n-grams المحارف، وهو تمثيل مرن ضد الأخطاء الإملائية والاشتقاقات.

على سبيل المثال، دعنا نقول أننا نتعامل مع مجموعة من الوثائق: "['words'، 'wprds']". تحتوي الوثيقة الثانية على خطأ إملائي في كلمة "words". سيعتبر التمثيل البسيط لحقيبة الكلمات هاتين الوثيقتين مختلفتين تمامًا، حيث تختلفان في السمتين المحتملتين. ومع ذلك، فإن تمثيل n-gram المحارف، سيجد أن الوثيقتين متطابقتان في 4 من 8 سمات، مما قد يساعد المصنف المفضل في اتخاذ قرار أفضل:

في المثال أعلاه، يتم استخدام محلل "char_wb"، والذي يقوم بإنشاء n-grams فقط من المحارف داخل حدود الكلمات (محددة بمسافة على كل جانب). من ناحية أخرى، يقوم المحلل "char" بإنشاء n-grams التي تمتد عبر الكلمات:

يتم استخدام متغير "char_wb" المتوافق مع حدود الكلمات بشكل خاص للغات التي تستخدم المسافات البيضاء لفصل الكلمات حيث يقوم بتوليد سمات أقل تشويشًا بشكل ملحوظ من المتغير "char" الخام في هذه الحالة. بالنسبة لهذه اللغات، يمكن أن يزيد من كل من الدقة التنبؤية وسرعة التقارب للمصنفات المدربة باستخدام هذه الميزات مع الحفاظ على المتانة فيما يتعلق بالأخطاء الإملائية واشتقاقات الكلمات.

في حين يمكن الحفاظ على بعض معلومات الموضع المحلي عن طريق استخراج n-grams بدلاً من الكلمات الفردية، فإن حقيبة الكلمات وحقيبة n-grams تدمر معظم البنية الداخلية للوثيقة وبالتالي معظم المعنى الذي تحمله تلك البنية الداخلية.

من أجل معالجة المهمة الأوسع لفهم اللغة الطبيعية، يجب مراعاة البنية المحلية للجمل والفقرات. وبالتالي، سيتم صياغة العديد من هذه النماذج على أنها مشكلات "إخراج منظم" تقع حاليًا خارج نطاق scikit-learn.

ناقلات تحويل نص كبير باستخدام خدعة التجزئة
------------------------------------------

مخطط التحويل أعلاه بسيط، ولكن نظرًا لأنه يحتفظ بخريطة ذاكرة **من الرموز التسلسلية إلى مؤشرات ميزات الأعداد الصحيحة** (سمة "vocabulary_")، فإنه يتسبب في حدوث عدة **مشكلات عند التعامل مع مجموعات البيانات الكبيرة**:

- كلما زاد حجم المجموعة، زاد حجم المفردات وبالتالي استخدام الذاكرة.

- يتطلب التجهيز تخصيص هياكل بيانات وسيطة بحجم يتناسب مع حجم المجموعة الأصلية.

- يتطلب بناء خريطة الكلمات تمريرة كاملة عبر المجموعة، وبالتالي لا يمكن ملاءمة المصنفات النصية بطريقة عبر الإنترنت تمامًا.

- قد يكون التخليل والتخليل لمجهزات التحويل التي تحتوي على "vocabulary_" كبيرًا جدًا بطيئًا جدًا (عادةً ما يكون أبطأ بكثير من التخليل / التخليل لهياكل البيانات المسطحة مثل مصفوفة NumPy بنفس الحجم)،

- من الممكن تقسيم عمل التحويل إلى مهام فرعية متزامنة حيث يجب أن تكون سمة "vocabulary_" حالة مشتركة مع حاجز تزامن دقيق جدًا: تعتمد خريطة الرمز التسلسلي إلى مؤشر الميزة على ترتيب أول حدوث لكل رمز، وبالتالي يجب أن تكون مشتركة، مما قد يضر بأداء العمال المتزامنين لدرجة تجعلهم أبطأ من المتغير التسلسلي.

من الممكن التغلب على هذه القيود من خلال الجمع بين "خدعة التجزئة" (Feature_hashing) التي ينفذها :class: sklearn.feature_extraction.FeatureHasher والتحضير المسبق للنص وميزات التمييز في :class: CountVectorizer.

ينفذ هذا المزيج في :class: HashingVectorizer، وهي فئة محول متوافقة إلى حد كبير مع واجهة برمجة التطبيقات :class: CountVectorizer. :class: HashingVectorizer لا تحتوي على حالة، مما يعني أنه لا يلزم استدعاء "fit" عليها:

يمكنك أن ترى أنه تم استخراج 16 رمزًا مميزًا غير صفري في ناقل الإخراج: وهذا أقل من 19 رمزًا غير صفري تم استخراجها سابقًا بواسطة :class: CountVectorizer على نفس مجموعة البيانات التجريبية. يأتي التناقض من اصطدامات دالة التجزئة بسبب القيمة المنخفضة لبارامتر "n_features".

في إعداد العالم الحقيقي، يمكن ترك معلمة "n_features" بقيمتها الافتراضية "2 ** 20" (حوالي مليون ميزة ممكنة). إذا كانت الذاكرة أو حجم النماذج اللاحقة تمثل مشكلة، فيمكن اختيار قيمة أقل مثل "2 ** 18" دون تقديم الكثير من الاصطدامات الإضافية في مهام تصنيف النص النموذجية.

لاحظ أن البعد لا يؤثر على وقت التدريب على وحدة المعالجة المركزية للخوارزميات التي تعمل على مصفوفات CSR (LinearSVC (dual = True))، Perceptron، SGDClassifier، PassiveAggressive)، ولكنه يفعل ذلك للخوارزميات التي تعمل مع مصفوفات CSC (LinearSVC (dual = False)، Lasso ()، إلخ).

دعنا نحاول مرة أخرى مع الإعداد الافتراضي:

لم نعد نحصل على الاصطدامات، ولكن هذا يأتي على حساب زيادة كبيرة في أبعاد مساحة الإخراج.

بالطبع، قد تصطدم مصطلحات أخرى غير المصطلحات التسعة عشر المستخدمة هنا.

تأتي :class: HashingVectorizer أيضًا مع القيود التالية:

- لا يمكن عكس النموذج (لا توجد طريقة "inverse_transform")، ولا يمكن الوصول إلى التمثيل السلسلة الأصلي للميزات، بسبب الطبيعة أحادية الاتجاه لدالة التجزئة التي تقوم بالتعيين.

- لا يوفر ترجيح IDF حيث من شأن ذلك أن يقدم حالة في النموذج. يمكن إضافة :class: TfidfTransformer إلى خط أنابيب إذا لزم الأمر.

يمكن أن يكون تطوير استخدام :class: HashingVectorizer مثيرًا للاهتمام لأنه يتيح إمكانية إجراء التوسع "خارج النواة". وهذا يعني أنه يمكننا التعلم من البيانات التي لا تناسب ذاكرة الكمبيوتر الرئيسية.

تتمثل إحدى الاستراتيجيات لتنفيذ التوسع خارج النواة في بث البيانات إلى المثمن في دفعات صغيرة. يتم تحويل كل دفعة صغيرة باستخدام :class: HashingVectorizer لضمان أن مساحة الإدخال للمصنف لها نفس الأبعاد دائمًا. يتم تحديد مقدار الذاكرة المستخدمة في أي وقت بحجم الدفعة الصغيرة. على الرغم من عدم وجود حد لكمية البيانات التي يمكن تناولها باستخدام هذا النهج، إلا أن وقت التعلم محدود عمليًا بوقت وحدة المعالجة المركزية الذي يرغب المرء في إنفاقه على المهمة.

لمثال كامل على التوسع خارج النواة في مهمة تصنيف النص، راجع :ref: sphx_glr_auto_examples_applications_plot_out_of_core_classification.py.

تخصيص فئات المحول
---------------------

يمكن تخصيص السلوك عن طريق تمرير دالة قابلة للاستدعاء إلى منشئ المحول::

فيما يلي نسميها على وجه التحديد:

* "preprocessor": دالة قابلة للاستدعاء تأخذ وثيقة كاملة كإدخال (كسلسلة واحدة)، وتعيد إصدارًا محولًا محتملًا للوثيقة، لا يزال كسلسلة واحدة. يمكن استخدام هذا لإزالة علامات HTML، أو تحويل الوثيقة بالكامل إلى أحرف صغيرة، وما إلى ذلك.

* "tokenizer": دالة قابلة للاستدعاء تأخذ الإخراج من المعالج المسبق وتقسمه إلى رموز، ثم تعيد قائمة بهذه الرموز.

* "analyzer": دالة قابلة للاستدعاء تحل محل المعالج المسبق والمعالج. تُطلق برامج التحليل الافتراضية جميع المعالجات المسبقة والمعالجات، ولكن قد يتعين على برامج التحليل المخصصة إعادة إنتاج هذه الخطوات. يتم تنفيذ استخراج n-gram وتصفية الكلمات غير الضرورية على مستوى المحلل.

(قد يتعرف مستخدمو Lucene على هذه الأسماء، ولكن كن على دراية بأن مفاهيم scikit-learn قد لا تتطابق مع مفاهيم Lucene واحدًا لواحد.)

لجعل المعالج المسبق والمعالج وبرامج التحليل على دراية بمعلمات النموذج، يمكن اشتقاقها من الفئة وإعادة كتابة أساليب المصنع "build_preprocessor" و "build_tokenizer" و "build_analyzer" بدلاً من تمرير وظائف مخصصة.

نصائح وحيل

* إذا كانت الوثائق مفهرسة مسبقًا بواسطة حزمة خارجية، فاحفظها في ملفات (أو سلاسل) مع الرموز مفصولة بمسافة بيضاء ومرر "analyzer=str.split".

* التحليل المتقدم على مستوى الرمز، مثل التصريف، والتصريف، والتقسيم المركب، والتصفية بناءً على الجزء من الكلام، وما إلى ذلك، غير مدرج في قاعدة تعليمات برمجة scikit، ولكنه يمكن إضافته عن طريق تخصيص المعالج أو المحلل.

  فيما يلي "CountVectorizer" بمعالج ومشتق باستخدام NLTK::

    >>> from nltk import word_tokenize          # doctest: +SKIP
    >>> from nltk.stem import WordNetLemmatizer # doctest: +SKIP
    >>> class LemmaTokenizer:
    ...     def __init__(self):
    ...         self.wnl = WordNetLemmatizer()
    ...     def __call__(self، doc):
    ...         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]
    ...
    >>> vect = CountVectorizer(tokenizer=LemmaTokenizer())  # doctest: +SKIP

  (لاحظ أن هذا لن يقوم بتصفية علامات الترقيم.)

  سيقوم المثال التالي، على سبيل المثال، بتحويل بعض التهجئة البريطانية إلى التهجئة الأمريكية::

    >>> import re
    >>> def to_british(tokens):
    ...     for t in tokens:
    ...         t = re.sub(r"(...)our$", r"\1or"، t)
    ...         t = re.sub(r"([bt])re$", r"\1er"، t)
    ...         t = re.sub(r"([iy])s(e$|ing|ation)", r"\1z\2"، t)
    ...         t = re.sub(r"ogue$", "og"، t)
    ...         yield t
    ...
    >>> class CustomVectorizer(CountVectorizer):
    ...     def build_tokenizer(self):
    ...         tokenize = super().build_tokenizer()
    ...         return lambda doc: list(to_british(tokenize(doc)))
    ...
    >>> print(CustomVectorizer().build_analyzer()(u"color colour"))
    [... 'color'، ... 'color']

  فيما يلي بعض أساليب المعالجة المسبقة الأخرى: التصريف، والتصريف، أو توحيد الرموز العددية، مع توضيح الأخير في:

  * :ref: sphx_glr_auto_examples_bicluster_plot_bicluster_newsgroups.py

يمكن أن يكون تخصيص المحول مفيدًا أيضًا عند التعامل مع اللغات الآسيوية التي لا تستخدم فاصل كلمات صريح مثل المسافة البيضاء.

استخراج ميزات الصورة
استخراج التصحيح
----------------

تقوم دالة :func:`extract_patches_2d` باستخراج التصحيحات من صورة مخزنة
كمصفوفة ثنائية الأبعاد، أو ثلاثية الأبعاد مع معلومات اللون على طول المحور
الثالث. ولإعادة بناء صورة من جميع تصحيحاتها، استخدم الدالة
:func:`reconstruct_from_patches_2d`. على سبيل المثال، دعنا نقوم بتوليد صورة
بكسل 4x4 مع 3 قنوات لونية (على سبيل المثال بتنسيق RGB)::

    >>> import numpy as np
    >>> from sklearn.feature_extraction import image

    >>> one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))
    >>> one_image[:, :, 0]  # R channel of a fake RGB picture
    array([[ 0,  3,  6,  9],
           [12, 15, 18, 21],
           [24, 27, 30, 33],
           [36, 39, 42, 45]])

    >>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,
    ...     random_state=0)
    >>> patches.shape
    (2, 2, 2, 3)
    >>> patches[:, :, :, 0]
    array([[[ 0,  3],
            [12, 15]],
    <BLANKLINE>
           [[15, 18],
            [27, 30]]])
    >>> patches = image.extract_patches_2d(one_image, (2, 2))
    >>> patches.shape
    (9, 2, 2, 3)
    >>> patches[4, :, :, 0]
    array([[15, 18],
           [27, 30]])

دعنا الآن نحاول إعادة بناء الصورة الأصلية من التصحيحات عن طريق حساب المتوسط
على المناطق المتداخلة::

    >>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))
    >>> np.testing.assert_array_equal(one_image, reconstructed)

تعمل فئة :class:`PatchExtractor` بنفس طريقة الدالة :func:`extract_patches_2d`،
ولكنها تدعم عدة صور كمدخلات. وهي منفذة كمحول scikit-learn، لذلك يمكن استخدامها
في الأنابيب. انظر::

    >>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)
    >>> patches = image.PatchExtractor(patch_size=(2, 2)).transform(five_images)
    >>> patches.shape
    (45, 2, 2, 3)

مخطط الاتصال بصورة
---------------------

يمكن لبعض التقديرات في scikit-learn استخدام معلومات الاتصال بين الميزات أو
العينات. على سبيل المثال، يمكن لتجميع Ward (:ref:`hierarchical_clustering`)
تجميع بكسلات صورة متجاورة فقط، وبالتالي تشكيل تصحيحات متجاورة:

.. figure:: ../auto_examples/cluster/images/sphx_glr_plot_coin_ward_segmentation_001.png
   :target: ../auto_examples/cluster/plot_coin_ward_segmentation.html
   :align: center
   :scale: 40

ولهذا الغرض، تستخدم التقديرات مصفوفة "اتصال" تحدد العينات المتصلة.

تُعيد دالة :func:`img_to_graph` مثل هذه المصفوفة من صورة ثنائية أو ثلاثية
الأبعاد. وبالمثل، تقوم دالة :func:`grid_to_graph` ببناء مصفوفة اتصال للصور
معرفة شكل هذه الصورة.

يمكن استخدام هذه المصفوفات لفرض الاتصال في التقديرات التي تستخدم معلومات
الاتصال، مثل تجميع Ward (:ref:`hierarchical_clustering`)، ولكن أيضًا لبناء
نوى محسوبة مسبقًا، أو مصفوفات تشابه.

.. note:: **أمثلة**

   * :ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`

   * :ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`

   * :ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`