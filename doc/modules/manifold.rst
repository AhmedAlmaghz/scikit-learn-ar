تعلم المنوال
=================

"ابحث عن الضروريات الأساسية
الضروريات البسيطة
انسَ همومك وصراعك
أعني الضروريات الأساسية
وصفات الطبيعة الأم
التي تجلب ضروريات الحياة الأساسية

- من أغنية بالو [كتاب الأدغال]

تعلم المنوال هو نهج لخفض الأبعاد غير الخطية.
تستند الخوارزميات الخاصة بهذه المهمة إلى فكرة أن البعدية للعديد من مجموعات البيانات مرتفع بشكل مصطنع فقط.

مقدمة
============

يمكن أن تكون مجموعات البيانات عالية الأبعاد صعبة للغاية في تصورها. في حين يمكن رسم البيانات
في بُعدين أو ثلاثة أبعاد لإظهار البنية الأساسية للبيانات، فإن الرسوم البيانية المكافئة عالية الأبعاد
أقل بديهية. للمساعدة في تصور بنية مجموعة البيانات، يجب تقليل البعد بطريقة ما.

أبسط طريقة لتحقيق هذا التخفيض الأبعاد هي عن طريق أخذ
المشروع العشوائي للبيانات. على الرغم من أن هذا يسمح ببعض درجات
تصور بنية البيانات، إلا أن عشوائية الاختيار تترك الكثير مما هو مرغوب فيه. في عرض عشوائي،
من المحتمل أن تضيع البنية الأكثر إثارة للاهتمام داخل البيانات.

للتخفيف من هذا القلق، تم تصميم عدد من الأطر الخطية الخاضعة للإشراف وغير الخاضعة للإشراف
خفض الأبعاد، مثل التحليل الرئيسي للمكونات (PCA)، وتحليل المكون المستقل،
التحليل التمييزي الخطي، وغيرها. تحدد هذه الخوارزميات أدلة محددة
لاختيار عرض "مثير للاهتمام" خطي للبيانات.
يمكن أن تكون هذه الأساليب قوية، ولكنها غالبًا ما تفقد بنية غير خطية مهمة في البيانات.

يمكن اعتبار تعلم المنوال محاولة لتعميم الأطر الخطية
مثل PCA ليكون حساسًا للبنية غير الخطية في البيانات. على الرغم من
توجد متغيرات خاضعة للإشراف، فإن مشكلة تعلم المنوال النموذجية
غير خاضع للإشراف: فهو يتعلم البنية عالية الأبعاد للبيانات
من البيانات نفسها، دون استخدام التصنيفات المحددة مسبقًا.

أمثلة عملية
----------------

* راجع :ref: `sphx_glr_auto_examples_manifold_plot_lle_digits.py <sphx_glr_auto_examples_manifold_plot_lle_digits.py>` ل
  مثال على تقليل الأبعاد على الأرقام المكتوبة بخط اليد.

* راجع :ref: `sphx_glr_auto_examples_manifold_plot_compare_methods.py <sphx_glr_auto_examples_manifold_plot_compare_methods.py>` ل
  مثال على تقليل الأبعاد على مجموعة بيانات "S-curve" اللعبة.

تنفيذ تعلم المنوال المتاحة في scikit-learn
ملخصة أدناه

Isomap
======

أحد أقدم النهج لتعلم المنوال هو خوارزمية Isomap
اختصار لـ Isometric Mapping. يمكن اعتبار Isomap امتدادًا لـ
تعدد الأبعاد (MDS) أو Kernel PCA.
يسعى Isomap إلى الحصول على تضمين منخفض الأبعاد يحافظ على المسافات الجيوديسية
بين جميع النقاط. يمكن إجراء Isomap باستخدام الكائن
:class: `Isomap <isomap>`.

تعقيد
------

تتكون خوارزمية Isomap من ثلاث مراحل:

1. **بحث أقرب جار**. يستخدم Isomap
:class: `BallTree <sklearn.neighbors.BallTree>` للبحث عن الجيران بكفاءة.
التكلفة تقريبية :math: `O [D log (k) N log (N)] <isomap_complexity_1>`، لـ :math: `k`
أقرب جيران :math: `N` النقاط في :math: `D` الأبعاد.

2. **بحث الرسم البياني أقصر مسار**. أكثر الخوارزميات المعروفة كفاءة
لهذه هي *خوارزمية ديكسترا*، والتي تبلغ حوالي
:math: `O [N^2 (k + log (N))] <isomap_complexity_2>`، أو *خوارزمية فلويد-وارشال*، والتي
:math: `O [N^3] <isomap_complexity_3>`. يمكن للمستخدم تحديد الخوارزمية
بـ ``path_method`` الكلمة الرئيسية لـ ``Isomap``. إذا لم يتم تحديده،
تحاول التعليمات البرمجية اختيار أفضل خوارزمية لبيانات الإدخال.

3. **التحلل القيمي الجزئي**. التضمين مشفر في
متجهات القيمة الذاتية المقابلة لـ :math: `d` أكبر القيم الذاتية
من :math: `N \ times N` نواة isomap. بالنسبة لحل دقيق،
التكلفة تقريبية :math: `O [d N^2] <isomap_complexity_4>`. يمكن غالبًا تحسين هذه التكلفة
باستخدام محلل ``ARPACK``. يمكن للمستخدم تحديد محلل القيمة الذاتية
بـ ``eigen_solver`` الكلمة الرئيسية لـ ``Isomap``. إذا لم يتم تحديده،
تحاول التعليمات البرمجية اختيار أفضل خوارزمية لبيانات الإدخال.

يبلغ التعقيد الإجمالي لـ Isomap
:math: `O [D log (k) N log (N)] + O [N^2 (k + log (N))] + O [d N^2] <isomap_complexity>`.

* :math: `N` : عدد نقاط التدريب
* :math: `D` : البعد المدخال
* :math: `k` : عدد الجيران الأقرب
* :math: `d` : البعد الإخراج

مراجع
----------

* "إطار هندسي عالمي لخفض الأبعاد غير الخطية"
  `<http://science.sciencemag.org/content/290/5500/2319.full>`_
  تينينباوم، جيه بي؛ دي سيلفا، الخامس؛ و Langford، جيه سي العلوم 290 (5500)

التضمين الخطي المحلي
========================

يسعى التضمين الخطي المحلي (LLE) إلى الحصول على عرض منخفض الأبعاد للبيانات
التي تحافظ على المسافات داخل الأحياء المحلية. يمكن التفكير فيه
كسلسلة من التحليلات الرئيسية للمكونات المحلية التي تتم مقارنتها عالميًا
لإيجد أفضل تضمين غير خطي.

يمكن إجراء التضمين الخطي المحلي باستخدام الدالة
:func: `locally_linear_embedding <locally_linear_embedding>` أو نظيرتها الموجهة نحو الكائنات
:class: `LocallyLinearEmbedding <locally_linear_embedding>`.

تعقيد
------

تتكون خوارزمية LLE القياسية من ثلاث مراحل:

1. **بحث أقرب جار**. راجع المناقشة تحت Isomap أعلاه.

2. **بناء مصفوفة الأوزان**. :math: `O [D N k^3] <lle_complexity_2>`.
   ينطوي بناء مصفوفة الأوزان LLE على حل معادلة خطية
:math: `k \ times k` لكل من :math: `N` الأحياء المحلية.

3. **التحلل القيمي الجزئي**. راجع المناقشة تحت Isomap أعلاه.

يبلغ التعقيد الإجمالي لـ LLE القياسي
:math: `O [D log (k) N log (N)] + O [D N k^3] + O [d N^2] <lle_complexity>`.

* :math: `N` : عدد نقاط التدريب
* :math: `D` : البعد المدخال
* :math: `k` : عدد الجيران الأقرب
* :math: `d` : البعد الإخراج

مراجع
----------

* "خفض الأبعاد غير الخطية بواسطة التضمين الخطي المحلي"
  `<http://www.sciencemag.org/content/290/5500/2323.full>`_
  رويس، إس و شاول، لام العلوم 290:2323 (2000)

التضمين الخطي المحلي المعدل
=================================

إحدى المشكلات المعروفة جيدًا في LLE هي مشكلة التنظيم. عندما يكون عدد
الجيران أكبر من عدد الأبعاد المدخلة، فإن المصفوفة
تحديد كل حي محلي ناقص الرتبة. لمعالجة هذا، يطبق LLE القياسي
معامل تنظيم عشوائي :math: `r`، والذي يتم اختياره
نسبيًا إلى أثر المصفوفة المحلية للأوزان. على الرغم من أنه يمكن إظهار ذلك رسميًا
بما أن :math: `r \ to 0`، يتقارب الحل مع التضمين المطلوب، لا يوجد ضمان بأن
سيتم العثور على الحل الأمثل لـ :math: `r> 0`. تتجلى هذه المشكلة في التضمينات التي
تشوه الهندسة الأساسية للمنوال.

إحدى الطرق لمعالجة مشكلة التنظيم هي استخدام أوزان متعددة
متجهات في كل حي. هذه هي جوهر *التضمين الخطي المحلي المعدل*
(MLLE). يمكن إجراء MLLE باستخدام الدالة
:func: `locally_linear_embedding <locally_linear_embedding>` أو نظيرتها الموجهة نحو الكائنات
:class: `LocallyLinearEmbedding <locally_linear_embedding>`، مع
``method = 'modified'``. إنه يتطلب ``n_neighbors> n_components``.

تعقيد
------

تتكون خوارزمية MLLE من ثلاث مراحل:

1. **بحث أقرب جار**. نفس LLE القياسية

2. **بناء مصفوفة الأوزان**. حوالي
   :math: `O [D N k^3] + O [N (k-D) k^2] <mlle_complexity_2>`.
   المصطلح الأول مكافئ تمامًا لذلك الموجود في LLE القياسي.
   يتعلق المصطلح الثاني ببناء مصفوفة الأوزان من أوزان متعددة. في الممارسة العملية،
   التكلفة الإضافية لبناء مصفوفة أوزان MLLE صغيرة نسبيًا مقارنة بالتكلفة
   من المرحلتين 1 و 3.

3. **التحلل القيمي الجزئي**. نفس LLE القياسية

يبلغ التعقيد الإجمالي لـ MLLE
:math: `O [D log (k) N log (N)] + O [D N k^3] + O [N (k-D) k^2] + O [d N^2] <mlle_complexity>`.

* :math: `N` : عدد نقاط التدريب
* :math: `D` : البعد المدخال
* :math: `k` : عدد الجيران الأقرب
* :math: `d` : البعد الإخراج

مراجع
----------

* "MLLE: التضمين الخطي المحلي المعدل باستخدام أوزان متعددة"
  `<https://citeseerx.ist.psu.edu/doc_view/pid/0b060fdbd92cbcc66b383bcaa9ba5e5e624d7ee3>`_
  تشانغ، ز. ووانغ، ج.

خريطة القيمة الذاتية للهيسية
طريقة "هيسيان إيغن مابينغ" (المعروفة أيضًا باسم "هيسيان-بيسد إل إل إي": HLLE) هي طريقة أخرى لحل مشكلة التنظيم في طريقة "إل إل إي". تدور هذه الطريقة حول شكل رباعي هيسيان-بيسد في كل حي يتم استخدامه لاستعادة البنية الخطية المحلية. على الرغم من أن التنفيذ الآخر يشير إلى ضعف التوسع مع حجم البيانات، فإن مكتبة "سكيكيت-ليرن" تنفذ بعض التحسينات الخوارزمية التي تجعل تكلفتها مماثلة لتكلفة المتغيرات الأخرى لطريقة "إل إل إي" بالنسبة لأبعاد الإخراج الصغيرة. يمكن تنفيذ طريقة HLLE باستخدام دالة "لوكالي_لاينر_إمبيدينغ" أو نظيرها الموجه نحو الكائنات "لوكالي_لاينر_إمبيدينغ"، مع الكلمة الأساسية "ميثود = 'هيسيان'". تتطلب هذه الطريقة أن تكون "إن_نيجيبرز > إن_كومبونينتس * (إن_كومبونينتس + 3) / 2".

تتكون خوارزمية HLLE من ثلاث مراحل:

1. **بحث أقرب الجيران**: نفس طريقة "إل إل إي" القياسية.
2. **بناء مصفوفة الأوزان**: تقريبًا O[D N k^3] + O[N d^6]. يعكس الحد الأول تكلفة مماثلة لتكلفة طريقة "إل إل إي" القياسية. يأتي الحد الثاني من تحليل QR لمقدّر هيسيان المحلي.
3. **التحليل الجزئي للقيمة الذاتية**: نفس طريقة "إل إل إي" القياسية.

تبلغ التعقيد الكلي لطريقة HLLE القياسية: O[D log(k) N log(N)] + O[D N k^3] + O[N d^6] + O[d N^2].

- :math:`N` : عدد نقاط التدريب
- :math:`D` : البعد المدخَل
- :math:`k` : عدد أقرب الجيران
- :math:`d` : بعد الإخراج

**المراجع**:

* "هيسيان إيغن مابس: تقنيات التضمين الخطي المحلي للبيانات عالية الأبعاد" <http://www.pnas.org/content/100/10/5591> دونوهو، د. وغرايمز، سي. بروك ناتل أكاد سسي الولايات المتحدة الأمريكية. 100:5591 (2003)

## التضمين الطيفي

التضمين الطيفي هو نهج لحساب التضمين غير الخطي. تنفذ مكتبة "سكيكيت-ليرن" طريقة "لابلاسيان إيغن مابس"، والتي تجد تمثيلًا منخفض الأبعاد للبيانات باستخدام التحليل الطيفي لمؤثر لابلاسيان للرسم البياني. يمكن اعتبار الرسم البياني الذي تم إنشاؤه تقريبًا منفصلًا للمتعدد الخطي منخفض الأبعاد في الفضاء عالي الأبعاد. يضمن تقليل دالة تكلفة تستند إلى الرسم البياني أن النقاط القريبة من بعضها البعض على المتعدد الخطي يتم رسمها بالقرب من بعضها البعض في الفضاء منخفض الأبعاد، مما يحافظ على المسافات المحلية. يمكن تنفيذ التضمين الطيفي باستخدام دالة "سبيكترال_إمبيدينغ" أو نظيرها الموجه نحو الكائنات "سبيكترال_إمبيدينغ".

تتكون خوارزمية التضمين الطيفي (خرائط لابلاسيان) من ثلاث مراحل:

1. **بناء الرسم البياني الموزون**: تحويل بيانات الإدخال الخام إلى تمثيل الرسم البياني باستخدام مصفوفة التشابه (المجاور).
2. **بناء مؤثر لابلاسيان**: يتم بناء مؤثر لابلاسيان غير الموحد على النحو التالي: L = D - A، في حين يتم بناء مؤثر لابلاسيان الموحد على النحو التالي: L = D^{-1/2} (D - A) D^{-1/2}.
3. **التحليل الجزئي للقيمة الذاتية**: يتم إجراء التحليل القيمي الذاتي على مؤثر لابلاسيان.

يبلغ التعقيد الكلي للتضمين الطيفي: O[D log(k) N log(N)] + O[D N k^3] + O[d N^2].

- :math:`N` : عدد نقاط التدريب
- :math:`D` : البعد المدخَل
- :math:`k` : عدد أقرب الجيران
- :math:`d` : بعد الإخراج

**المراجع**:

* "خرائط لابلاسيان للتقليل من الأبعاد وتمثيل البيانات" <https://web.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf> إم. بيلكين، بي. نييوجي، الحساب العصبي، يونيو 2003؛ 15 (6): 1373-1396

## محاذاة الفضاء المماس المحلي

على الرغم من أن طريقة "محاذاة الفضاء المماس المحلي" (LTSA) ليست من الناحية الفنية إحدى متغيرات طريقة "إل إل إي"، إلا أنها تشبهها خوارزميًا بما يكفي لوضعها في هذه الفئة. بدلاً من التركيز على الحفاظ على مسافات الحي كما هو الحال في طريقة "إل إل إي"، تسعى طريقة LTSA إلى توصيف الهندسة المحلية في كل حي من خلال فضاءها المماس، وتؤدي تحسينًا عالميًا لمحاذاة مساحات المماس هذه لتعلم التضمين. يمكن تنفيذ طريقة LTSA باستخدام دالة "لوكالي_لاينر_إمبيدينغ" أو نظيرها الموجه نحو الكائنات "لوكالي_لاينر_إمبيدينغ"، مع الكلمة الأساسية "ميثود = 'إل تي إس إيه'".

تتكون خوارزمية LTSA من ثلاث مراحل:

1. **بحث أقرب الجيران**: نفس طريقة "إل إل إي" القياسية.
2. **بناء مصفوفة الأوزان**: تقريبًا O[D N k^3] + O[k^2 d]. يعكس الحد الأول تكلفة مماثلة لتكلفة طريقة "إل إل إي" القياسية.
3. **التحليل الجزئي للقيمة الذاتية**: نفس طريقة "إل إل إي" القياسية.

يبلغ التعقيد الكلي لطريقة LTSA القياسية: O[D log(k) N log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2].

- :math:`N` : عدد نقاط التدريب
- :math:`D` : البعد المدخَل
- :math:`k` : عدد أقرب الجيران
- :math:`d` : بعد الإخراج

**المراجع**:

* "المنشورات الرئيسية والمنشورات الفرعية والحد من الأبعاد غير الخطية عبر محاذاة الفضاء المماس" <cs/0212008> زانج، ز. وزهو، إتش. مجلة جامعة شانغهاي 8:406 (2004)

## القياس متعدد الأبعاد (MDS)

يسعى "القياس متعدد الأبعاد" <https://en.wikipedia.org/wiki/Multidimensional_scaling> (MDS) إلى إيجاد تمثيل منخفض الأبعاد للبيانات التي تحترم المسافات في الفضاء عالي الأبعاد الأصلي.

بشكل عام، تعد طريقة MDS تقنية مستخدمة لتحليل بيانات التشابه أو الاختلاف. فهي تحاول نمذجة بيانات التشابه أو الاختلاف على أنها مسافات في مساحات هندسية. يمكن أن تكون البيانات تصنيفات للتشابه بين الكائنات، أو ترددات التفاعل بين الجزيئات، أو مؤشرات التداول بين البلدان.

هناك نوعان من خوارزميات MDS: المترية وغير المترية. في مكتبة "سكيكيت-ليرن"، تنفذ طريقة MDS كلا النوعين. في طريقة MDS المترية، تنشأ مصفوفة التشابه من مقياس (وبالتالي تحترم عدم المساواة المثلثية)، ويتم بعد ذلك ضبط المسافات بين نقطتين لتكون قريبة قدر الإمكان من بيانات التشابه أو الاختلاف. في الإصدار غير المتري، ستحاول الخوارزميات الحفاظ على ترتيب المسافات، وبالتالي البحث عن علاقة متناسبة بين المسافات في الفضاء المضمن والتشابهات/الاختلافات.

دع :math:`S` تكون مصفوفة التشابه، و:math:`X` إحداثيات :math:`n` نقاط الإدخال. الفروق :math:`\hat{d}_{ij}` هي تحويل التشابهات التي يتم اختيارها بطرق مثالية. يتم بعد ذلك تعريف الهدف، والذي يُطلق عليه الإجهاد، على النحو التالي: :math:`\sum_{i < j} d_{ij}(X) - \hat{d}_{ij}(X)`

### طريقة MDS المترية

في أبسط نموذج MDS المتري، والذي يُطلق عليه اسم MDS المطلق، يتم تعريف الفروق على النحو التالي: :math:`\hat{d}_{ij} = S_{ij}`. مع طريقة MDS المطلقة، يجب أن تتوافق القيمة :math:`S_{ij` بشكل دقيق مع المسافة بين النقطتين :math:`i` و:math:`j` في نقطة التضمين.

في معظم الأحيان، يتم ضبط الفروق على النحو التالي: :math:`\hat{d}_{ij} = b S_{ij}`.

### طريقة MDS غير المترية

تركز طريقة MDS غير المترية على ترتيب البيانات. إذا كان :math:`S_{ij} > S_{jk}`، فيجب أن يفرض التضمين ما يلي: :math:`d_{ij} < d_{jk}`. لهذا السبب، نناقشها من حيث الاختلافات (:math:`\delta_{ij}`) بدلاً من التشابهات (:math:`S_{ij}`). لاحظ أنه يمكن الحصول على الاختلافات بسهولة من التشابهات من خلال تحويل بسيط، على سبيل المثال: :math:`\delta_{ij}=c_1-c_2 S_{ij}` لبعض الثوابت الحقيقية :math:`c_1, c_2`. إحدى الخوارزميات البسيطة لفرض الترتيب الصحيح هي استخدام الانحدار الأحدبي لـ:math:`d_{ij}` على :math:`\delta_{ij}`، مما يؤدي إلى فروق :math:`\hat{d}_{ij}` بنفس ترتيب :math:`\delta_{ij}`.

الحل البديهي لهذه المشكلة هو تعيين جميع النقاط على الأصل. لتجنب ذلك، يتم تطبيع الفروق :math:`\hat{d}_{ij}`. لاحظ أنه نظرًا لأننا نهتم فقط بالترتيب النسبي، يجب أن يكون هدفنا غير حساس للترجمة البسيطة والمقياس، ومع ذلك فإن الإجهاد المستخدم في طريقة MDS المترية حساس للمقياس. لمعالجة ذلك، قد تستخدم طريقة MDS غير المترية إجهادًا موحدًا، يُعرف باسم Stress-1، على النحو التالي:

.. math::
    \sqrt{\frac{\sum_{i < j} (d_{ij} - \hat{d}_{ij})^2}{\sum_{i < j} d_{ij}^2}}.

يمكن تمكين استخدام Stress-1 الموحد من خلال ضبط `normalized_stress=True`، ولكنه متوافق فقط مع مشكلة MDS غير المترية وسيتم تجاهله في حالة MDS المترية.

**المراجع**:

* "القياس متعدد الأبعاد الحديث - النظرية والتطبيقات" <https://www.springer.com/fr/book/9780387251509> بورغ، آي؛ غرونين بي. سلسلة سبرينغر في الإحصاء (1997)

* "القياس متعدد الأبعاد غير المتري: طريقة رقمية" <http://cda.psych.uiuc.edu/psychometrika_highly_cited_articles/kruskal_1964b.pdf> كروسكال، جي. علم النفس، 29 (1964)

* "القياس متعدد الأبعاد عن طريق تحسين ملاءمة الفرضية غير المترية" <http://cda.psych.uiuc.edu/psychometrika_highly_cited_articles/kruskal_1964a.pdf> كروسكال، جي. علم النفس، 29، (1964)

## t-distributed Stochastic Neighbor Embedding (t-SNE)
يتحول t-SNE (:class:`TSNE`) نقاط البيانات إلى احتمالات.
وتمثل الاحتمالات المشتركة الغاوسية في الفراغ الأصلي، وتمثل احتمالات طالب t-distributions في الفراغ المدمج. وهذا يجعل t-SNE حساسًا للغاية للبنية المحلية وله عدة مزايا على التقنيات الموجودة:

* كشف البنية على العديد من المقاييس على خريطة واحدة
* كشف البيانات التي تقع في المنوعات أو المجموعات المختلفة والمتعددة
* تقليل الميل لتجميع النقاط معًا في الوسط

في حين أن Isomap وLLE والمتغيرات مناسبة بشكل أفضل لتفكيك المنوال المستمر الأحادي البعد، سيركز t-SNE على البنية المحلية للبيانات
ومن المرجح أن تستخرج مجموعات محلية مجمعة من العينات كما هو موضح في مثال منحنى S. وقد تكون هذه القدرة على تجميع العينات بناءً على البنية المحلية مفيدة لفصل مجموعة بيانات بصريًا تتكون من عدة منوعات في نفس الوقت كما هو الحال في مجموعة بيانات الأرقام.

سيتم تقليل التباعد كولباك-ليبلير (KL) للاحتمالات المشتركة في الفراغ الأصلي والمدمج بواسطة التدرج الهابط. لاحظ أن التباعد KL غير محدب، أي أن عمليات إعادة التشغيل المتعددة باستخدام تهيئات مختلفة ستنتهي في الحد الأدنى المحلي للتباعد KL. وبالتالي، من المفيد في بعض الأحيان تجربة بذور مختلفة واختيار التضمين مع أقل تباعد KL.

العيب في استخدام t-SNE هو تقريبًا:

* t-SNE مكلف حسابياً، ويمكن أن يستغرق عدة ساعات في مجموعات بيانات المليون عينة حيث تنتهي PCA في ثوان أو دقائق
* تقتصر طريقة Barnes-Hut t-SNE على تضمينات ثنائية أو ثلاثية الأبعاد.
* الخوارزمية احتمالية ويمكن أن تؤدي عمليات إعادة التشغيل المتعددة باستخدام بذور مختلفة إلى تضمينات مختلفة. ومع ذلك، من المشروع تمامًا اختيار التضمين بأقل خطأ.
* لا يتم الحفاظ على البنية العالمية بشكل صريح. يتم تخفيف هذه المشكلة عن طريق تهيئة النقاط باستخدام PCA (باستخدام `init='pca'`).

.. figure:: ../auto_examples/manifold/images/sphx_glr_plot_lle_digits_013.png
   :target: ../auto_examples/manifold/plot_lle_digits.html
   :align: center
   :scale: 50

.. dropdown:: تحسين t-SNE

  الغرض الرئيسي من t-SNE هو تصور البيانات عالية الأبعاد. وبالتالي، فهو يعمل بشكل أفضل عندما يتم تضمين البيانات في بُعدين أو ثلاثة أبعاد.

  يمكن أن يكون تحسين التباعد KL صعبًا بعض الشيء في بعض الأحيان. هناك خمسة معلمات تتحكم في تحسين t-SNE وبالتالي ربما جودة التضمين الناتج:

  * الحيرة
  * عامل المبالغة المبكر
  * معدل التعلم
  * الحد الأقصى لعدد التكرارات
  * الزاوية (لا تستخدم في الطريقة الدقيقة)

  يتم تعريف الحيرة على أنها :math:`k=2^{(S)}` حيث :math:`S` هو إنتروبيا شانون لتوزيع الاحتمال الشرطي. الحيرة من
  :math:`k`-منضدة النرد هو :math:`k`، بحيث :math:`k` هو في الواقع عدد
  أقرب جيران ينظر إليها t-SNE عند توليد الاحتمالات الشرطية. تؤدي الحيرة الأكبر إلى عدد أكبر من أقرب الجيران وأقل حساسية للبنية الصغيرة. وعلى العكس من ذلك، فإن الحيرة المنخفضة تأخذ في الاعتبار عددًا أقل من الجيران، وبالتالي تتجاهل المعلومات العالمية لصالح
  الجوار المحلي. مع زيادة أحجام مجموعات البيانات، ستكون هناك حاجة إلى المزيد من النقاط للحصول على عينة معقولة من الجوار المحلي، وبالتالي قد تكون هناك حاجة إلى حيرة أكبر. وبالمثل، فإن مجموعات البيانات الأكثر ضوضاءً ستحتاج إلى قيم حيرة أكبر لتشمل ما يكفي من الجيران المحليين لرؤية ما وراء ضوضاء الخلفية.

  عادةً ما يكون العدد الأقصى للتكرارات مرتفعًا بدرجة كافية ولا يحتاج إلى أي ضبط. يتكون التحسين من مرحلتين: مرحلة المبالغة المبكرة والتحسين النهائي. خلال المبالغة المبكرة، سيتم زيادة الاحتمالات المشتركة في الفراغ الأصلي بشكل مصطنع عن طريق
  الضرب في عامل معين. تؤدي العوامل الأكبر إلى فجوات أكبر بين المجموعات الطبيعية في البيانات. إذا كان العامل مرتفعًا جدًا، فقد يزيد التباعد KL خلال هذه المرحلة. عادةً لا يلزم ضبطه. أحد المعلمات الحرجة هو معدل التعلم. إذا كان منخفضًا جدًا، فسيعلق النسب المئوية في الحد الأدنى المحلي السيئ. إذا كان مرتفعًا جدًا، فسوف يزيد التباعد KL أثناء التحسين. تنصح حيلة Belkina et al. (2019) بضبط معدل التعلم على حجم العينة
  مقسومة على عامل المبالغة المبكر. ننفذ هذه الحيلة كـ `learning_rate='auto'` argument. يمكن العثور على مزيد من النصائح في
  قائمة الأسئلة الشائعة الخاصة بـ Laurens van der Maaten (راجع المراجع). المعلمة الأخيرة، الزاوية، هي حل وسط بين الأداء والدقة. تشير الزوايا الأكبر إلى أنه يمكننا تقريب مناطق أكبر بواسطة نقطة واحدة، مما يؤدي إلى سرعة أفضل ولكن نتائج أقل دقة.

  يقدم "كيفية استخدام t-SNE بشكل فعال" <https://distill.pub/2016/misread-tsne/>
  مناقشة جيدة لآثار المعلمات المختلفة، بالإضافة إلى المخططات التفاعلية لاستكشاف آثار معلمات مختلفة.

.. dropdown:: Barnes-Hut t-SNE

  عادةً ما يكون Barnes-Hut t-SNE، الذي تم تنفيذه هنا، أبطأ بكثير من خوارزميات التعلم الأخرى للمنوال. التحسين صعب للغاية
  وحساب التدرج هو :math:`O[d N log(N)]`، حيث :math:`d`
  هو عدد الأبعاد الناتجة و :math:`N` هو عدد العينات. تحسن طريقة Barnes-Hut على الطريقة الدقيقة حيث تكون تعقيد t-SNE
  :math:`O[d N^2]`، ولكن لها عدة اختلافات ملحوظة أخرى:

  * يعمل تنفيذ Barnes-Hut فقط عندما يكون البعد المستهدف 3
    أو أقل. الحالة ثنائية الأبعاد نموذجية عند إنشاء التصورات.
  * يعمل Barnes-Hut فقط مع بيانات الإدخال الكثيفة. يمكن تضمين مصفوفات البيانات المتناثرة فقط
  باستخدام الطريقة الدقيقة أو يمكن تقريبها بواسطة إسقاط كثيف منخفض الترتيب باستخدام :class:`~sklearn.decomposition.PCA`
  * Barnes-Hut هو تقريب للأسلوب الدقيق. يتم معلمجة التقريب
  مع معلمة الزاوية، وبالتالي لا يتم استخدام معلمة الزاوية عندما تكون الطريقة="الدقيقة"
  * Barnes-Hut قابل للتطوير بشكل أكبر. يمكن استخدام Barnes-Hut لتضمين مئات الآلاف من نقاط البيانات في حين أن الطريقة الدقيقة يمكنها التعامل
  مع آلاف العينات قبل أن تصبح غير قابلة للحساب

  بالنسبة لهدف التصور (وهو الاستخدام الأساسي لـ t-SNE)، يوصى بشدة باستخدام
  طريقة Barnes-Hut. تعد طريقة t-SNE الدقيقة مفيدة لفحص الخصائص النظرية للتضمين، ربما في مساحة ذات أبعاد أعلى، ولكنها تقتصر على مجموعات البيانات الصغيرة بسبب القيود الحسابية.

  لاحظ أيضًا أن تسميات الأرقام تتطابق تقريبًا مع التجميع الطبيعي الذي يجده
  t-SNE في حين أن الإسقاط الخطي ثنائي الأبعاد لنموذج PCA ينتج تمثيلًا
  تتداخل فيه مناطق التسمية إلى حد كبير. هذه علامة قوية على أنه يمكن فصل هذه البيانات جيدًا بواسطة طرق غير خطية تركز على البنية المحلية (مثل
  SVM مع نواة RBF غاوسية). ومع ذلك، فإن الفشل في تصور المجموعات ذات التسميات المتجانسة جيدًا والمفصولة جيدًا باستخدام t-SNE في 2D لا يعني بالضرورة أن البيانات لا يمكن تصنيفها بشكل صحيح بواسطة نموذج إشرافي. قد يكون الأمر أن البعدين غير مرتفعين بما يكفي لتمثيل البنية الداخلية للبيانات بدقة.

.. rubric:: المراجع

* `"تصور البيانات عالية الأبعاد باستخدام t-SNE"
  <https://jmlr.org/papers/v9/vandermaaten08a.html>`_
  فان دير مااتين، إل جي بي؛ هينتون، جي. مجلة أبحاث التعلم الآلي (2008)

* `"التضمين المجاور العشوائي الموزع t"
  <https://lvdmaaten.github.io/tsne/>`_ فان دير مااتين، إل جي بي.

* `"تسريع t-SNE باستخدام الخوارزميات القائمة على الشجرة"
  <https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf>`_
  فان دير مااتين، إل جي بي؛ مجلة أبحاث التعلم الآلي 15(أكتوبر): 3221-3245، 2014.

* `"معلمات محسنة تلقائيًا لتضمين الجار العشوائي الموزع t تحسن من التصور والتحليل لمجموعات البيانات الكبيرة"
  <https://www.nature.com/articles/s41467-019-13055-y>`_
  Belkina، AC، Ciccolella، CO، Anno، R.، Halpert، R.، Spidlen، J.،
  Snyder-Cappione، JE، Nature Communications 10، 5415 (2019).

نصائح للاستخدام العملي
=====================

* تأكد من استخدام نفس المقياس لجميع الميزات. نظرًا لأن أساليب التعلم المنوال تعتمد على البحث عن أقرب الجيران، فقد يؤدي ذلك إلى أداء ضعيف في حالة عدم القيام بذلك. راجع :ref:`StandardScaler <preprocessing_scaler>`
  للحصول على طرق ملائمة لقياس البيانات غير المتجانسة.

* يمكن استخدام خطأ إعادة البناء الذي يحسبه كل روتين لاختيار
  البعد الناتج الأمثل. بالنسبة إلى المنوال :math:`d` المضمن
  في مساحة المعلمات :math:`D` الأبعاد، سينخفض خطأ إعادة البناء
  مع زيادة ``n_components`` حتى ``n_components == d``.

* لاحظ أن البيانات الضجيج يمكن أن "تقصر" المنوال، مما يؤدي إلى جسر
  بين أجزاء المنوال التي ستكون منفصلة بشكل جيد.
  التعلم المنوال على البيانات الضجيج و/أو غير المكتملة هو
  مجال نشط للبحث.

* يمكن أن تؤدي بعض تكوينات الإدخال إلى مصفوفات أوزان متساوية، على سبيل المثال عندما يكون أكثر من نقطتين في مجموعة البيانات متطابقة، أو عندما
  تنقسم البيانات إلى مجموعات منفصلة. في هذه الحالة، سوف
  تفشل "الحل='arpack'" في العثور على المساحة الخالية. أسهل طريقة للتعامل مع هذا الأمر هي
  استخدام ``solver='dense'`` الذي سيعمل على مصفوفة متساوية، على الرغم من أنه قد يكون
  بطيئًا جدًا اعتمادًا على عدد نقاط الإدخال. بدلاً من ذلك، يمكن للمرء
  محاولة فهم مصدر التساوي: إذا كان بسبب مجموعات منفصلة، فقد يساعد زيادة ``n_neighbors``. إذا كان بسبب نقاط متطابقة في مجموعة البيانات، فقد يساعد إزالة هذه النقاط.

.. seealso::

   :ref:`random_trees_embedding` يمكن أن يكون مفيدًا أيضًا لاستنتاج التمثيلات غير الخطية
  لمساحة الميزة، كما أنه لا يؤدي
  تقليل الأبعاد.