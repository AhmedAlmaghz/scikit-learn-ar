نماذج الشبكات العصبية (غير الخاضعة للإشراف)
====================================

وحدات بولتزمان المقيدة
=============================

وحدات بولتزمان المقيدة (RBM) هي متعلمات غير خطية للميزات غير الخاضعة للإشراف
تستند إلى نموذج احتمالي. غالباً ما تعطي الميزات التي يستخرجها RBM أو تسلسل هرمي
من RBMs نتائج جيدة عند إدخالها في مصنف خطي مثل SVM الخطي أو محاكي النبضات.

يقوم النموذج بوضع افتراضات فيما يتعلق بتوزيع المدخلات. في الوقت الحالي،
يوفر scikit-learn فقط :class:`BernoulliRBM`، والذي يفترض أن المدخلات هي
إما قيم ثنائية أو قيم بين 0 و 1، حيث ترمز كل منها إلى احتمال
تشغيل الميزة المحددة.

يحاول RBM زيادة احتمال البيانات باستخدام نموذج رسومي معين. تعمل خوارزمية تعلم المعلمات المستخدمة (:ref:`Stochastic
Maximum Likelihood <sml>`) على منع التمثيلات من الابتعاد
عن بيانات الإدخال، مما يجعلها تلتقط انتظامًا مثيرًا للاهتمام، ولكن
يجعل النموذج أقل فائدة لمجموعات البيانات الصغيرة، وعادة ما لا يكون مفيدًا
لتقدير الكثافة.

اكتسبت هذه الطريقة شعبية في تهيئة الشبكات العصبية العميقة بوزن RBMs المستقل. تُعرف هذه الطريقة باسم التهيئة المسبق غير الخاضع للإشراف.

نمذجة رسومية والبرامترية
-----------------------------------

النموذج الرسومي لـ RBM هو رسم بياني ثنائي كامل الاتصال.

تعد العقد متغيرات عشوائية تعتمد حالاتها على حالة العقد الأخرى
التي تتصل بها. وبالتالي، يتم معلم النموذج بواسطة
أوزان الاتصالات، وكذلك مصطلح اعتراض (الانحياز) واحد لكل
وحدة مرئية وخفية، والتي تم إغفالها من الصورة لسهولة العرض.

تقيس دالة الطاقة جودة تعيين مشترك:

في الصيغة أعلاه، :math:`\mathbf{b}` و :math:`\mathbf{c}` هي
متجهات الاعتراض للطبقات المرئية والخفية، على التوالي. يتم تعريف الاحتمالية المشتركة للنموذج من حيث الطاقة:

يشير مصطلح "المقيد" إلى البنية ثنائية الأطراف للنموذج، والتي
تحظر التفاعل المباشر بين الوحدات المخفية، أو بين الوحدات المرئية.
هذا يعني أنه يتم افتراض الاستقلاليات الشرطية التالية:

تسمح البنية ثنائية الأطراف باستخدام عينات جيبس الكتلية الفعالة للاستدلال.

وحدات بولتزمان المقيدة ذات القيمة الثابتة
---------------------------------------

في :class:`BernoulliRBM`، جميع الوحدات هي وحدات احتمالية ثنائية. وهذا
يعني أن بيانات الإدخال يجب أن تكون ثنائية، أو ذات قيمة حقيقية بين 0 و
1 مما يشير إلى احتمال تشغيل الوحدة المرئية أو إيقافها. هذا
نموذج جيد للتعرف على الأحرف، حيث يكمن الاهتمام في البكسلات النشطة وغير النشطة. لم يعد يناسب
لصور المشاهد الطبيعية بسبب الخلفية والعمق وميل البكسلات المجاورة
لأخذ نفس القيم.

يتم إعطاء التوزيع الاحتمالي الشرطي لكل وحدة بواسطة
دالة التنشيط سيجمويد اللوغاريتمية للإدخال الذي تتلقاه:

حيث :math:`\sigma` هي دالة سيجمويد اللوغاريتمية:

التعلم الاحتمالي الأقصى العشوائي
--------------------------------------

تُعرف خوارزمية التدريب المُنفذة في :class:`BernoulliRBM` باسم
Stochastic Maximum Likelihood (SML) أو Persistent Contrastive Divergence
(PCD). من غير العملي تحسين الاحتمالية القصوى مباشرة بسبب
شكل احتمال البيانات:

لتبسيط المعادلة أعلاه، يتم كتابتها لمثال تدريب واحد.
يتكون التدرج بالنسبة للأوزان من حدين يقابلان
التي في الأعلى. عادة ما يُعرفان باسم التدرج الإيجابي والتدرج السلبي،
بسبب إشاراتهم المقابلة. في هذا التنفيذ، يتم تقدير التدرجات
عبر دفعات صغيرة من العينات.

عند زيادة الاحتمال اللوغاريتمي، يجعل التدرج الإيجابي النموذج يفضل
الحالات المخفية المتوافقة مع بيانات التدريب المرصودة. بفضل البنية ثنائية الأطراف لـ RBMs،
يمكن حسابه بكفاءة. ومع ذلك، فإن التدرج السلبي غير قابل للتعقب. هدفه هو
خفض طاقة الحالات المشتركة التي يفضلها النموذج، مما يجعله
مخلصًا للبيانات. يمكن تقريبه باستخدام مونت كارلو ماركوف باستخدام عينات جيبس الكتلية عن طريق
عينات بشكل تكراري لكل من :math:`v` و :math:`h` بالنظر إلى الآخر، حتى تخلط
السلسلة. يُشار إلى العينات التي تم إنشاؤها بهذه الطريقة أحيانًا باسم جسيمات الخيال. هذه الطريقة غير فعالة ومن الصعب تحديد ما إذا كان
سلسلة ماركوف تخلط.

تقترح طريقة التباين التناقضي إيقاف السلسلة بعد عدد صغير
من التكرارات، :math:`k`، عادة ما تكون حتى 1. هذه الطريقة سريعة ولها
انحراف معياري منخفض، لكن العينات بعيدة عن توزيع النموذج.

يعالج التباين التناقضي المستمر هذه المشكلة. بدلاً من بدء سلسلة جديدة
في كل مرة تكون فيها التدرجات مطلوبة، والقيام بخطوة واحدة فقط من عينات جيبس،
في PCD نحتفظ بعدد من السلاسل (جسيمات الخيال) التي يتم تحديثها
:math:`k` خطوات جيبس بعد كل تحديث للوزن. يسمح هذا للجسيمات باستكشاف
المساحة بشكل أكثر شمولاً.

المراجع
* `"خوارزمية تعليم سريعة لشبكات الاعتقاد العميقة"
  <https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf>`_،
  ج. هينتون، إس أوسنديرو، واي-تي تي، 2006

* `"تدريب وحدات بولتزمان المقيدة باستخدام التقريبات
  تدرج الاحتمالية"
  <https://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf>`_،
  ت. تيليمان، 2008