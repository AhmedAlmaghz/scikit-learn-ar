.. currentmodule:: sklearn.feature_selection

.. _feature_selection:

=================
اختيار الخصائص
=================

يمكن استخدام الفئات في وحدة :mod:`sklearn.feature_selection` لاختيار الخصائص/الحد من الأبعاد على مجموعات العينات، إما لتحسين درجات دقة المُقدّر أو لتعزيز أدائه على مجموعات البيانات عالية الأبعاد للغاية.

.. _variance_threshold:

إزالة الخصائص ذات التباين المنخفض
===================================

:class:`VarianceThreshold` هي طريقة بسيطة لاختيار الخصائص بناءً على خط الأساس. إنه يزيل جميع الميزات التي لا يفي تباينها بعتبة معينة. بشكل افتراضي، فإنه يزيل جميع الميزات ذات التباين الصفري، أي الميزات التي لها نفس القيمة في جميع العينات.

على سبيل المثال، لنفترض أن لدينا مجموعة بيانات ذات ميزات منطقية، ونريد إزالة جميع الميزات التي تكون إما واحدًا أو صفرًا (تشغيل أو إيقاف) في أكثر من 80% من العينات.

الميزات المنطقية هي متغيرات برنولي عشوائية، ويُعطى تباين هذه المتغيرات بالصيغة التالية:

.. math:: \mathrm{Var}[X] = p(1 - p)

لذلك، يمكننا الاختيار باستخدام العتبة ``.8 * (1 - .8)``::

  >>> from sklearn.feature_selection import VarianceThreshold
  >>> X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
  >>> sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
  >>> sel.fit_transform(X)
  array([[0, 1],
         [1, 0],
         [0, 0],
         [1, 1],
         [1, 0],
         [1, 1]])

كما هو متوقع، قام ``VarianceThreshold`` بإزالة العمود الأول، والذي لديه احتمال :math:`p = 5/6 > .8` لاحتواء الصفر.

.. _univariate_feature_selection:

اختيار ميزة أحادي المتغير
============================

يعمل اختيار ميزة أحادي المتغير عن طريق اختيار أفضل الميزات بناءً على اختبارات إحصائية أحادية المتغير. يمكن اعتباره خطوة ما قبل المعالجة لمُقدّر. تعرض Scikit-learn روتينات اختيار الميزات ككائنات تقوم بتنفيذ طريقة "التحويل":

* :class:`SelectKBest` يزيل كل شيء باستثناء :math:`k` الميزات ذات الدرجات الأعلى

* :class:`SelectPercentile` يزيل كل شيء باستثناء نسبة مئوية محددة من المستخدم لأعلى الميزات التي تم تسجيلها

* باستخدام الاختبارات الإحصائية أحادية المتغير الشائعة لكل ميزة:
  معدل الإيجابيات الخاطئة :class:`SelectFpr`، ومعدل الاكتشاف الخاطئ :class:`SelectFdr`، أو الخطأ الكلي للعائلة :class:`SelectFwe`.

* :class:`GenericUnivariateSelect` يسمح بإجراء اختيار ميزة أحادي المتغير باستخدام استراتيجية قابلة للتكوين. يتيح ذلك اختيار أفضل استراتيجية اختيار أحادي المتغير باستخدام مُقدّر البحث عن أفضل وسيط.

على سبيل المثال، يمكننا استخدام اختبار F-test لاسترداد أفضل ميزتين لمجموعة بيانات كما يلي:

  >>> from sklearn.datasets import load_iris
  >>> from sklearn.feature_selection import SelectKBest
  >>> from sklearn.feature_selection import f_classif
  >>> X, y = load_iris(return_X_y=True)
  >>> X.shape
  (150, 4)
  >>> X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)
  >>> X_new.shape
  (150, 2)

تأخذ هذه الكائنات كإدخال دالة تسجيل تعيد درجات أحادية المتغير والقيم p (أو الدرجات فقط لـ :class:`SelectKBest` و :class:`SelectPercentile`):

* للانحدار: :func:`r_regression`، :func:`f_regression`، :func:`mutual_info_regression`

* للتصنيف: :func:`chi2`، :func:`f_classif`، :func:`mutual_info_classif`

تقدر الطرق المستندة إلى اختبار F درجة الاعتماد الخطي بين متغيرين عشوائيين. من ناحية أخرى، يمكن لطرق المعلومات المتبادلة التقاط أي نوع من الاعتماد الإحصائي، ولكن نظرًا لأنها غير معلمية، فإنها تتطلب المزيد من العينات للتقدير الدقيق. لاحظ أنه يجب تطبيق اختبار :math:`\chi^2` فقط على الميزات غير السالبة، مثل الترددات.

.. topic:: اختيار الميزة مع البيانات المتناثرة

   إذا كنت تستخدم بيانات متناثرة (أي بيانات ممثلة على شكل مصفوفات متناثرة)، فسيتم التعامل مع :func:`chi2`، :func:`mutual_info_regression`، :func:`mutual_info_classif` مع البيانات دون جعلها كثيفة.

.. warning::

    احترس من عدم استخدام دالة تسجيل الانحدار مع مشكلة التصنيف، فستحصل على نتائج عديمة الفائدة.

.. note::

    تدعم :class:`SelectPercentile` و :class:`SelectKBest` اختيار الميزات غير الخاضعة للإشراف أيضًا. يجب توفير دالة `score_func` حيث `y=None`. يجب أن تستخدم `score_func` داخليًا `X` لحساب الدرجات.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_feature_selection_plot_feature_selection.py`

* :ref:`sphx_glr_auto_examples_feature_selection_plot_f_test_vs_mi.py`

.. _rfe:

القضاء على الميزة التراجعية
=============================

بالنظر إلى مُقدّر خارجي يعين الأوزان للميزات (مثل معاملات نموذج خطي)، فإن الهدف من القضاء على الميزة التراجعية (:class:`RFE`) هو اختيار الميزات عن طريق النظر بشكل متكرر في مجموعات أصغر وأصغر من الميزات. أولاً، يتم تدريب المُقدّر على مجموعة الميزات الأولية ويتم الحصول على أهمية كل ميزة إما من خلال أي سمة محددة (مثل "coef_"، "feature_importances_") أو callable. ثم يتم تقليم الميزات الأقل أهمية من المجموعة الحالية من الميزات. يتم تكرار هذا الإجراء بشكل متكرر على المجموعة المقلمة حتى يتم الوصول إلى العدد المرغوب من الميزات التي سيتم اختيارها في النهاية.

يؤدي :class:`RFECV` إلى RFE في حلقة تصحيح عكسي للعثور على العدد الأمثل من الميزات. بمزيد من التفاصيل، يتم ضبط عدد الميزات المحددة تلقائيًا عن طريق ملاءمة مُحدد :class:`RFE` على تقسيمات التصحيح العكسي المختلفة (التي يوفرها معلمة `cv`). يتم تقييم أداء مُحدد :class:`RFE` باستخدام "scorer" لعدد مختلف من الميزات المحددة وتجميعها معًا. أخيرًا، يتم حساب متوسط الدرجات عبر الطيات ويتم تعيين عدد الميزات المحددة إلى عدد الميزات التي تحقق أقصى درجات التصحيح العكسي.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_digits.py`: مثال على القضاء على الميزة التراجعية
  إظهار أهمية البكسل في مهمة تصنيف الرقم.

* :ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`: مثال على القضاء على الميزة التراجعية
  مثال مع الضبط التلقائي لعدد الميزات المحددة باستخدام التصحيح العكسي.

.. _select_from_model:

اختيار الميزة باستخدام SelectFromModel
=======================================

:class:`SelectFromModel` هو محول عام يمكن استخدامه جنبًا إلى جنب مع أي مُقدّر يعين الأهمية لكل ميزة من خلال سمة محددة (مثل "coef_"، "feature_importances_") أو عبر callable `importance_getter` بعد التجهيز. تعتبر الميزات غير مهمة ويتم إزالتها إذا كانت الأهمية المقابلة لقيم الميزة أقل من معلمة "threshold" المقدمة. بالإضافة إلى تحديد العتبة رقميًا، هناك استراتيجيات مدمجة للعثور على عتبة باستخدام حجة سلسلة. الاستراتيجيات المتوفرة هي "mean"، "median" ومضاعفات الأرقام العشرية لهذه القيم مثل "0.1*mean". بالاشتراك مع معلمة `threshold`، يمكن استخدام معلمة `max_features` لتحديد حد على عدد الميزات التي سيتم اختيارها.

للاطلاع على أمثلة حول كيفية استخدامها، راجع الأقسام أدناه.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_feature_selection_plot_select_from_model_diabetes.py`

.. _l1_feature_selection:

اختيار الميزة المستندة إلى L1
--------------------------

.. currentmodule:: sklearn

:ref:`نماذج خطية <linear_model>` المعاقب عليها بمعيار L1 لها
حلول متفرقة: العديد من معاملاتها المقدرة هي صفر. عندما يكون الهدف هو تقليل أبعاد البيانات لاستخدامها مع مُصنّف آخر، يمكن استخدامها جنبًا إلى جنب مع :class:`~feature_selection.SelectFromModel`
لتحديد المعاملات غير الصفرية. على وجه الخصوص، فإن المقدرات المتناثرة المفيدة لهذا الغرض هي :class:`~linear_model.Lasso` للانحدار، و
:class:`~linear_model.LogisticRegression` و :class:`~svm.LinearSVC`
للتصنيف::

  >>> from sklearn.svm import LinearSVC
  >>> from sklearn.datasets import load_iris
  >>> from sklearn.feature_selection import SelectFromModel
  >>> X, y = load_iris(return_X_y=True)
  >>> X.shape
  (150, 4)
  >>> lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
  >>> model = SelectFromModel(lsvc, prefit=True)
  >>> X_new = model.transform(X)
  >>> X_new.shape
  (150, 3)

مع SVMs والانحدار اللوجستي، يتحكم معلمة C في التفرقة:
كلما قل C، قل عدد الميزات المحددة. مع Lasso، كلما زادت
قيمة معلمة alpha، قل عدد الميزات المحددة.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_linear_model_plot_lasso_dense_vs_sparse_data.py`.

.. _compressive_sensing:

.. dropdown:: L1-recovery and compressive sensing

  بالنسبة للاختيار الجيد لمعلمة alpha، يمكن أن يسترد :ref:`lasso` تمامًا
  مجموعة المتغيرات غير الصفرية تمامًا باستخدام عدد قليل من الملاحظات، بشرط
  تلبية شروط محددة. على وجه الخصوص، يجب أن يكون عدد
  يجب أن تكون العينات "كبيرة بما يكفي"، أو ستؤدي نماذج L1 أداءً عشوائيًا، حيث "كبيرة بما يكفي" تعتمد على عدد المعاملات غير الصفرية، واللوغاريتم من عدد الميزات، ومقدار
  الضوضاء، وأصغر قيمة مطلقة للمعاملات غير الصفرية، وهيكل مصفوفة التصميم X. بالإضافة إلى ذلك، يجب أن تعرض مصفوفة التصميم خصائص محددة، مثل عدم وجود ارتباط كبير.

  لا توجد قاعدة عامة لاختيار معلمة alpha لاسترداد المعاملات غير الصفرية. يمكن تعيينه بواسطة التصحيح العكسي
  (:class:`~sklearn.linear_model.LassoCV` أو
  :class:`~sklearn.linear_model.LassoLarsCV`)، على الرغم من أن هذا قد يؤدي إلى
  نماذج ناقصة العقاب: بما في ذلك عدد صغير من المتغيرات غير ذات الصلة ليس ضارًا
  على درجة التنبؤ. يميل BIC
  (:class:`~sklearn.linear_model.LassoLarsIC`)، على العكس من ذلك، إلى تعيين
  قيم عالية من alpha.

  .. rubric:: المراجع

  Richard G. Baraniuk "Compressive Sensing"، IEEE Signal
  معالجة مجلة [120] يوليو 2007
  http://users.isr.ist.utl.pt/~aguiar/CS_notes.pdf


اختيار ميزة شجرة القرار
----------------------------

يمكن استخدام المقدرات القائمة على الشجرة (راجع وحدة :mod:`sklearn.tree` وغابة الأشجار في وحدة :mod:`sklearn.ensemble`) لحساب أهمية الميزات القائمة على عدم النقاء، والتي يمكن استخدامها بعد ذلك لتجاهل الميزات غير ذات الصلة (عند اقترانها بالمحول الميتا :class:`~feature_selection.SelectFromModel`)::

  >>> from sklearn.ensemble import ExtraTreesClassifier
  >>> from sklearn.datasets import load_iris
  >>> from sklearn.feature_selection import SelectFromModel
  >>> X, y = load_iris(return_X_y=True)
  >>> X.shape
  (150, 4)
  >>> clf = ExtraTreesClassifier(n_estimators=50)
  >>> clf = clf.fit(X, y)
  >>> clf.feature_importances_  # doctest: +SKIP
  array([ 0.04...,  0.05...,  0.4...,  0.4...])
  >>> model = SelectFromModel(clf, prefit=True)
  >>> X_new = model.transform(X)
  >>> X_new.shape               # doctest: +SKIP
  (150, 2)

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_importances.py`: مثال على
  بيانات اصطناعية تظهر استرداد الميزات ذات الصلة بالفعل.

* :ref:`sphx_glr_auto_examples_ensemble_plot_forest_importances_faces.py`: مثال
  على بيانات التعرف على الوجه.

.. _sequential_feature_selection:

اختيار الميزة التسلسلية
يسمح اختيار السمات المتسلسل [sfs] _ (SFS) المتاح في محول :class: ~ sklearn.feature_selection.SequentialFeatureSelector بتحديد السمات إما للأمام أو للخلف:

يعد اختيار السمات المتسلسل للأمام إجراءً جشعاً يجد بشكل تكراري أفضل سمة جديدة لإضافتها إلى مجموعة السمات المحددة. وبشكل ملموس، نبدأ في البداية بدون أي سمات ونبحث عن السمة الواحدة التي تعظم نتيجة التحقق من الصحة المتقاطعة عند تدريب مُقدِّر على هذه السمة الوحيدة. بمجرد اختيار أول سمة، نكرر الإجراء عن طريق إضافة سمة جديدة إلى مجموعة السمات المحددة. يتوقف الإجراء عند الوصول إلى عدد السمات المحددة المطلوب، كما يحدده معامل n_features_to_select.

أما اختيار السمات المتسلسل للخلف فيتبع الفكرة نفسها ولكنه يعمل في الاتجاه المعاكس: بدلاً من البدء بدون أي سمات وإضافة السمات بطريقة جشعة، نبدأ بجميع السمات ونزيلها بطريقة جشعة من المجموعة. يتحكم معامل "الاتجاه" في ما إذا كان يتم استخدام اختيار السمات المتسلسل للأمام أو للخلف.

يختلف اختيار السمات المتسلسل عمومًا عن طريق الاختيار للأمام والاختيار للخلف لا يحقق نتائج مكافئة. أيضًا، قد يكون أحدهما أسرع بكثير من الآخر اعتمادًا على عدد السمات المطلوبة: إذا كان لدينا 10 سمات ونريد 7 سمات محددة، فإن الاختيار المتجه للأمام يتطلب 7 تكرارات بينما يحتاج الاختيار المتجه للخلف إلى 3 تكرارات فقط.

يختلف SFS عن :class: ~ sklearn.feature_selection.RFE و :class: ~ sklearn.feature_selection.SelectFromModel في أنه لا يتطلب من النموذج الأساسي الكشف عن معامل "coef_" أو "feature_importances_". ومع ذلك، فقد يكون أبطأ نظرًا لضرورة تقييم المزيد من النماذج مقارنة بالطرق الأخرى. على سبيل المثال، في الاختيار المتجه للخلف، تتطلب التكرار الذي ينتقل من استخدام "m" سمة إلى "m - 1" سمة باستخدام التحقق من الصحة المتقاطع k-fold ملاءمة نماذج "m * k"، في حين أن :class: ~ sklearn.feature_selection.RFE سيتطلب ملاءمة واحدة فقط، و :class: ~ sklearn.feature_selection.SelectFromModel دائمًا ما يقوم بملاءمة واحدة فقط ولا يتطلب أي تكرارات.

يمكن استخدام اختيار السمات كخطوة ما قبل المعالجة قبل التعلم الفعلي. الطريقة الموصى بها للقيام بذلك في Scikit-learn هي استخدام :class: ~ pipeline.Pipeline:

في هذا المقطع، نستخدم :class: ~ svm.LinearSVC مقترنًا بـ :class: ~ feature_selection.SelectFromModel لتقييم أهمية السمات واختيار أكثرها ملاءمة. بعد ذلك، يتم تدريب :class: ~ ensemble.RandomForestClassifier على الإخراج المحول، أي باستخدام السمات ذات الصلة فقط. يمكنك إجراء عمليات مماثلة باستخدام طرق اختيار السمات الأخرى والتصنيفات التي توفر طريقة لتقييم أهمية السمات. راجع أمثلة :class: ~ pipeline.Pipeline لمزيد من التفاصيل.