

Examples using ``sklearn.covariance.OAS``
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


.. start-sphx-glr-thumbnails


.. raw:: html

    <div class="sphx-glr-thumbnails">

.. thumbnail-parent-div-open

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="The usual covariance maximum likelihood estimate can be regularized using shrinkage. Ledoit and Wolf proposed a close formula to compute the asymptotically optimal shrinkage parameter (minimizing a MSE criterion), yielding the Ledoit-Wolf covariance estimate.">

.. only:: html

  .. image:: /auto_examples/covariance/images/thumb/sphx_glr_plot_lw_vs_oas_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_covariance_plot_lw_vs_oas.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Ledoit-Wolf vs OAS estimation</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_covariance_plot_lw_vs_oas.py`

.. raw:: html

    <div class="sphx-glr-thumbcontainer" tooltip="When working with covariance estimation, the usual approach is to use a maximum likelihood estimator, such as the EmpiricalCovariance. It is unbiased, i.e. it converges to the true (population) covariance when given many observations. However, it can also be beneficial to regularize it, in order to reduce its variance; this, in turn, introduces some bias. This example illustrates the simple regularization used in shrunk_covariance estimators. In particular, it focuses on how to set the amount of regularization, i.e. how to choose the bias-variance trade-off.">

.. only:: html

  .. image:: /auto_examples/covariance/images/thumb/sphx_glr_plot_covariance_estimation_thumb.png
    :alt:

  :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`

.. raw:: html

      <div class="sphx-glr-thumbnail-title">Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood</div>
    </div>


.. only:: not html

 * :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py`
