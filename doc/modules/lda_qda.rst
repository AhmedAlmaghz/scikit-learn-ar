تحليل التمييز الخطي والرباعي
==========================================

تحليل التمييز الخطي (LDA) وتحليل التمييز الرباعي (QDA) هما مصنفان كلاسيكيان، كما يوحي اسماهما، مع سطح قرار خطي ورباعي على التوالي.

تعد هذه المصنفات جذابة لأن لها حلولًا مغلقة الشكل يسهل حسابها، وهي متعددة الفئات بطبيعتها، وقد ثبت أنها تعمل بشكل جيد في الممارسة العملية، وليس لديها أي فرط معلمات للضبط.

يوضح الرسم البياني أدناه حدود القرار لتحليل التمييز الخطي وتحليل التمييز الرباعي. ويظهر الصف السفلي أن تحليل التمييز الخطي يمكنه فقط تعلم الحدود الخطية، في حين أن تحليل التمييز الرباعي يمكنه تعلم حدود رباعية، وبالتالي فهو أكثر مرونة.

أمثلة
------

-  مثال على مقارنة LDA وQDA على بيانات صناعية.

الحد من الأبعاد باستخدام تحليل التمييز الخطي
===========================================================

يمكن استخدام LDA لأداء الحد من الأبعاد الخاضع للإشراف، عن طريق إسقاط بيانات الإدخال على فراغ خطي فرعي يتكون من الاتجاهات التي تزيد من الفصل بين الفئات (بالمعنى الدقيق الذي تمت مناقشته في قسم الرياضيات أدناه). بعدد أبعاد أقل من عدد الفئات، وهذا يمثل بشكل عام تقليلًا قويًا للأبعاد، ولا معنى له إلا في إعداد متعدد الفئات.

تم تنفيذ هذا في طريقة "التحويل". يمكن تعيين البعد المطلوب باستخدام معلمة "n_components". لا تؤثر هذه المعلمة على طرق "التناسب" و"التوقع".

أمثلة
------

-  مثال على مقارنة LDA وPCA للحد من أبعاد مجموعة بيانات Iris

الصيغة الرياضية لمصنفي LDA وQDA
=======================================================

يمكن اشتقاق كل من LDA وQDA من نماذج احتمالية بسيطة تحاكي التوزيع الشرطي للفئة للبيانات P (X | y = k) لكل فئة k. يمكن بعد ذلك الحصول على التوقعات من خلال استخدام قاعدة بايز، لكل عينة تدريب x ∈ R^d:

P (y = k | x) = P (x | y = k) P (y = k) / P (x) = P (x | y = k) P (y = k) / ∑_l P (x | y = l) ⋅ P (y = l)

ونحن نختار الفئة k التي تزيد من هذا الاحتمال اللاحق إلى الحد الأقصى.

وبشكل أكثر تحديدًا، بالنسبة لتحليل التمييز الخطي والرباعي، يتم نمذجة P (x | y) كتوزيع غاوسي متعدد المتغيرات بكثافة:

P (x | y = k) = 1 / ((2π)^ (d/2) |Σ_k|^ (1/2)) exp (-1/2 (x-μ_k)^t Σ_k^ (-1) (x-μ_k))

حيث d هو عدد الميزات.

QDA
---

وفقًا للنموذج الموضح أعلاه، فإن لوغاريتم البعد اللاحق هو:

log P (y = k | x) = log P (x | y = k) + log P (y = k) + Cst
= -1/2 log |Σ_k| -1/2 (x-μ_k)^t Σ_k^ (-1) (x-μ_k) + log P (y = k) + Cst،

حيث مصطلح الثابت Cst يقابل المقام P (x)، بالإضافة إلى مصطلحات ثابتة أخرى من التوزيع الغاوسي. الفئة المتوقعة هي تلك التي تزيد من هذا اللوغاريتم اللاحق إلى الحد الأقصى.

ملاحظة: العلاقة مع غاوسي ناييف بايز

إذا افترضنا في نموذج QDA أن مصفوفات التغاير مائلة، فإن الإدخالات تفترض أنها مستقلة شرطيًا في كل فئة، والمصنف الناتج مكافئ لمصنف غاوسي ناييف بايز.

LDA
---

LDA هو حالة خاصة من QDA، حيث يفترض أن يكون لدى غاوسيات لكل فئة نفس مصفوفة التغاير: Σ_k = Σ لجميع k. وهذا يقلل اللوغاريتم اللاحق إلى:

log P (y = k | x) = -1/2 (x-μ_k)^t Σ^ (-1) (x-μ_k) + log P (y = k) + Cst.

المصطلح (x-μ_k)^t Σ^ (-1) (x-μ_k) يقابل مسافة ماهالانوبيس بين العينة x والمتوسط μ_k. توضح مسافة ماهالانوبيس مدى قرب x من μ_k، مع مراعاة تغاير كل ميزة. يمكننا بعد ذلك تفسير LDA على أنه يعين x إلى الفئة التي يكون متوسطها الأقرب من حيث مسافة ماهالانوبيس، مع مراعاة احتمالات الفئة السابقة.

يمكن أيضًا كتابة اللوغاريتم اللاحق لـ LDA على النحو التالي:

log P (y = k | x) = ω_k^t x + ω_ {k0} + Cst.

حيث ω_k = Σ^ (-1) μ_k وω_ {k0} = -1/2 μ_k^t Σ^ (-1) μ_k + log P (y = k). تتوافق هذه الكميات مع سمات "coef_" و"intercept_"، على التوالي.

من الصيغة الموضحة أعلاه، من الواضح أن LDA لديه سطح قرار خطي. في حالة QDA، لا توجد افتراضات حول مصفوفات التغاير Σ_k للغاوسيات، مما يؤدي إلى أسطح قرار رباعية. راجع [1]_ لمزيد من التفاصيل.

الصيغة الرياضية لتخفيض الأبعاد LDA
========================================================

أولاً، لاحظ أن المتوسطات K_k هي متجهات في R^d، وأنها تقع في فضاء آفيني H بأبعاد لا تزيد عن K - 1 (تقع نقطتان على خط، و3 نقاط على طائرة، وهكذا).

كما ذكرنا سابقًا، يمكننا تفسير LDA على أنه يعين x إلى الفئة التي يكون متوسطها μ_k الأقرب من حيث مسافة ماهالانوبيس، مع مراعاة احتمالات الفئة السابقة. وبدلاً من ذلك، فإن LDA مكافئ أولاً لجعل البيانات كروية بحيث تكون مصفوفة التغاير هي الهوية، ثم تعيين x إلى المتوسط الأقرب من حيث المسافة الإقليدية (مع مراعاة المتوسطات السابقة للفئة).

إن حساب المسافات الإقليدية في هذا الفضاء ذي الأبعاد d مكافئ لإسقاط نقاط البيانات في H، وحساب المسافات هناك (حيث ستساهم الأبعاد الأخرى بالتساوي في كل فئة من حيث المسافة). وبعبارة أخرى، إذا كانت x الأقرب إلى μ_k في الفضاء الأصلي، فستكون كذلك في H.

هذا يظهر أنه، ضمنيًا في مصنف LDA، هناك تقليل للأبعاد عن طريق الإسقاط الخطي على مساحة K - 1 الأبعاد.

يمكننا تقليل البعد أكثر، إلى L المحدد، عن طريق الإسقاط على الفراغ الخطي الفرعي H_L الذي يزيد من تباين μ^*_k بعد الإسقاط (في الواقع، نقوم بشكل من أشكال PCA للمتوسطات المحولة للفئة μ^*_k). يتوافق هذا مع معلمة "n_components" المستخدمة في طريقة "تحويل" LDA. راجع [1]_ لمزيد من التفاصيل.

الانكماش ومقدار التقدير
==================================

الانكماش هو شكل من أشكال التنظيم المستخدم لتحسين تقدير مصفوفات التغاير في الحالات التي يكون فيها عدد عينات التدريب صغيرًا مقارنة بعدد الميزات. في هذا السيناريو، يكون تقدير التغاير التجريبي ضعيفًا، ويساعد الانكماش في تحسين أداء التعميم للمصنف. يمكن استخدام LDA المنكمش عن طريق تعيين معلمة "الانكماش" لفئة LDA إلى "auto". وهذا يحدد تلقائيًا معامل الانكماش الأمثل بطريقة تحليلية تلي المبرهن التي قدمها ليدويت وولف [2]_. لاحظ أن الانكماش يعمل حاليًا فقط عند تعيين معلمة "المحقق" إلى "lsqr" أو "eigen".

يمكن أيضًا تعيين معلمة "الانكماش" يدويًا بين 0 و1. وعلى وجه التحديد، تشير القيمة 0 إلى عدم وجود انكماش (مما يعني أنه سيتم استخدام مصفوفة التغاير التجريبية) وتشير القيمة 1 إلى الانكماش الكامل (مما يعني أنه سيتم استخدام مصفوفة القطرية للمتباينات كتقدير لمصفوفة التغاير). سيؤدي تعيين هذه المعلمة إلى قيمة بين هذين الحدين إلى تقدير نسخة منكمشة من مصفوفة التغاير.

قد لا يكون تقدير الانكماش من ليدويت وولف هو الخيار الأفضل دائمًا. على سبيل المثال، إذا كان توزيع البيانات طبيعيًا، فإن تقدير الانكماش المقرب من Oracle :class: 'sklearn.covariance.OAS' يعطي خطأً متوسطًا مربعًا أصغر من ذلك الذي تعطيه صيغة ليدويت وولف المستخدمة مع "shrinkage"="auto". في LDA، يفترض أن تكون البيانات موزعة بشكل غاوسي مشروط للفئة. إذا كانت هذه الافتراضات صحيحة، فإن استخدام LDA مع تقدير التغاير OAS سيحقق دقة تصنيف أفضل من استخدام ليدويت وولف أو تقدير التغاير التجريبي.

يمكن اختيار تقدير التغاير باستخدام معلمة "covariance_estimator" لفئة LDA. يجب أن يكون لمقدّر التغاير طريقة "تناسب" وسمة "التغاير" مثل جميع مقدرات التغاير في وحدة نمطية "sklearn.covariance".

خوارزميات التقدير
=====================

يتطلب استخدام LDA وQDA حساب اللوغاريتم اللاحق الذي يعتمد على احتمالات الفئة السابقة P (y = k)، والمتوسطات الفئة μ_k، ومصفوفات التغاير.

محلل SVD هو المحلل الافتراضي المستخدم لـ LDA، وهو المحلل الوحيد المتاح لـ QDA. يمكنه تنفيذ كل من التصنيف والتحويل (بالنسبة لـ LDA). نظرًا لأنه لا يعتمد على حساب مصفوفة التغاير، فقد يكون محلل SVD مفضلًا في الحالات التي يكون فيها عدد الميزات كبيرًا. لا يمكن استخدام محلل SVD مع الانكماش.

بالنسبة إلى QDA، يعتمد استخدام محلل SVD على حقيقة أن مصفوفة التغاير Σ_k، بحكم التعريف، تساوي 1/ (n - 1) X_k^tX_k = 1/ (n - 1) V S^2 V^t حيث تأتي V من SVD للمصفوفة (المركزية): X_k = U S V^t. اتضح أنه يمكننا حساب اللوغاريتم اللاحق أعلاه دون الحاجة إلى حساب Σ: يكفي حساب S وV عبر SVD لـ X. بالنسبة إلى LDA، يتم حساب SVDs اثنين: SVD للمصفوفة المدخلة المركزية X وSVD لمتجهات المتوسطات للفئة.

محلل "lsqr" هو خوارزمية فعالة تعمل فقط للتصنيف. يجب أن يحسب صراحةً مصفوفة التغاير Σ، ويدعم الانكماش ومقدرات التغاير المخصصة. تحسب هذه الخوارزمية المعاملات ω_k = Σ^ (-1) μ_k عن طريق حل Σ ω = μ_k، وبالتالي تجنب الحساب الصريح لـ Σ^ (-1).

يعتمد محلل "eigen" على تحسين نسبة التشتت بين الفئة إلى التشتت داخل الفئة. يمكن استخدامه لكل من التصنيف والتحويل، ويدعم الانكماش. ومع ذلك، يحتاج محلل "eigen" إلى حساب مصفوفة التغاير، لذا فقد لا يكون مناسبًا للمواقف التي يكون فيها عدد كبير من الميزات.

المراجع
------

[1] "عناصر التعلم الإحصائي"، هاستي تي.، تيبشيرانى ر.، فريدمان جي.، القسم 4.3، ص. 106-119، 2008.

[2] ليدويت أو، وولف م. العسل، لقد قمت بتقليص مصفوفة التغاير للعينة. مجلة إدارة المحافظ 30 (4)، 110-119، 2004.

[3] ر. أ. دودا، ب. هارت، د. ستورك. التصنيف النمطي (الطبعة الثانية)، القسم 2.6.2.