التعلم شبه المُشرف
===================================================

التعلم شبه المُشرف هو حالة تحتوي فيها بيانات التدريب الخاصة بك على بعض العينات غير المُوسومة. ويمكن لمقدّري التعلم شبه المُشرف في sklearn.semi_supervised الاستفادة من هذه البيانات غير المُوسومة الإضافية لالتقاط شكل توزيع البيانات الأساسي بشكل أفضل والتعميم بشكل أفضل على العينات الجديدة. يمكن أن تؤدي هذه الخوارزميات أداءً جيدًا عندما يكون لدينا عدد قليل جدًا من النقاط المُوسومة وكمية كبيرة من النقاط غير المُوسومة.

الموضوع:: إدخالات غير مُوسومة في "y"

من المهم تعيين مُعرف إلى النقاط غير المُوسومة جنبًا إلى جنب مع البيانات المُوسومة عند تدريب النموذج باستخدام طريقة "التناسب". المُعرف الذي يستخدمه هذا التنفيذ هو القيمة الصحيحة: -1. لاحظ أنه بالنسبة لعلامات السلاسل، يجب أن يكون نوع بيانات "y" كائنًا حتى يتمكن من احتواء كل من السلاسل والعدد الصحيح.

ملاحظة::

تحتاج خوارزميات التعلم شبه المُشرف إلى وضع افتراضات حول توزيع مجموعة البيانات لتحقيق مكاسب في الأداء. راجع هنا لمزيد من التفاصيل.

التدريب الذاتي
=============

يستند هذا التنفيذ للتدريب الذاتي على خوارزمية Yarowsky [1]. باستخدام هذه الخوارزمية، يمكن لمصنف مُشرف العمل كمصنف شبه مُشرف، مما يتيح له التعلم من البيانات غير المُوسومة.

يمكن استدعاء SelfTrainingClassifier بأي مصنف ينفذ وظيفة "predict_proba"، والتي يتم تمريرها كمعلمة "base_classifier". في كل تكرار، يتنبأ "base_classifier" بعلامات للعينات غير المُوسومة ويضيف مجموعة فرعية من هذه العلامات إلى مجموعة البيانات المُوسومة.

يتم تحديد اختيار هذه المجموعة الفرعية بواسطة معيار الاختيار. يمكن إجراء هذا الاختيار باستخدام "عتبة" على احتمالات التنبؤ، أو عن طريق اختيار "k_best" للعينات وفقًا لاحتمالات التنبؤ.

تتوفر العلامات المستخدمة للتناسب النهائي وكذلك التكرار الذي تم فيه وضع علامة على كل عينة كسمات. تحدد معلمة "max_iter" الاختيارية عدد مرات تنفيذ الحلقة كحد أقصى.

يمكن تعيين معلمة "max_iter" على "None"، مما يتسبب في تكرار الخوارزمية حتى يتم وضع علامات على جميع العينات أو عدم تحديد عينات جديدة في تلك الحلقة.

ملاحظة::

عند استخدام مصنف التدريب الذاتي، تكون معايرة المصنف مهمة.

الأمثلة
--------

* sphx_glr_auto_examples_semi_supervised_plot_self_training_varying_threshold.py
* sphx_glr_auto_examples_semi_supervised_plot_semi_supervised_versus_svm_iris.py

المراجع
----------

.. [1] "استنباط المعنى غير المُشرف للكلمة ينافس الأساليب المُشرفة" ديفيد ياروفسكي، وقائع الاجتماع السنوي الثالث والثلاثين حول رابطة اللغويات الحاسوبية (ACL '95). رابطة اللغويات الحاسوبية، سترودسبورغ، بنسلفانيا، الولايات المتحدة الأمريكية، 189-196.

انتشار العلامات
=================

يشير انتشار العلامات إلى بعض التباينات في خوارزميات الاستدلال البياني شبه المُشرف.

بعض الميزات المتوفرة في هذا النموذج:

* تستخدم لمهام التصنيف
* أساليب النواة لإسقاط البيانات في مساحات الأبعاد البديلة

يوفر سكيت-ليرن نموذجين لانتشار العلامات: LabelPropagation وLabelSpreading. يعمل كلاهما من خلال بناء رسم بياني للتشابه عبر جميع العناصر في مجموعة البيانات المدخلة.

**توضيح لانتشار العلامات:** *يتسق هيكل الملاحظات غير المُوسومة مع بنية الفئة، وبالتالي يمكن نشر فئة العلامة إلى الملاحظات غير المُوسومة في مجموعة التدريب.*

يختلف كل من LabelPropagation وLabelSpreading في التعديلات على مصفوفة التشابه التي يرسمها تأثير التثبيت على توزيعات العلامات. يسمح التثبيت للخوارزمية بتغيير وزن البيانات المُوسومة الحقيقية إلى حد ما. تقوم خوارزمية LabelPropagation بالتثبيت الصلب لعلامات الإدخال، مما يعني أن ألفا = 0. يمكن استرخاء عامل التثبيت هذا، لنقول ألفا = 0.2، مما يعني أننا سنحتفظ دائمًا بنسبة 80% من توزيع العلامات الأصلي لدينا، ولكن يمكن للخوارزمية تغيير ثقتها بالتوزيع ضمن 20%.

يستخدم LabelPropagation مصفوفة التشابه الخام المُنشأة من البيانات دون أي تعديلات. على العكس من ذلك، يقلل LabelSpreading من دالة خسارة لها خصائص التنظيم، وبالتالي فهو غالبًا ما يكون أكثر مقاومة للضوضاء. تكرر الخوارزمية على نسخة معدلة من الرسم البياني الأصلي وتطبّع أوزان الحواف عن طريق حساب مصفوفة لابلاسيان الرسم البياني المعياري. يتم استخدام هذا الإجراء أيضًا في: spectral_clustering.

تشتمل نماذج انتشار العلامة على أسلوبين مدمجين للنواة. يؤثر اختيار النواة على كل من قابلية توسع الخوارزميات وأدائها. فيما يلي ما هو متاح:

* RBF (e^(-gamma |x-y|^2), gamma > 0). يتم تحديد جاما بواسطة الكلمة الرئيسية جاما.
* KNN (1 [x' in kNN(x)]). يتم تحديد "k" بواسطة الكلمة الرئيسية n_neighbors.

ستنتج نواة RBF رسمًا بيانيًا متصلاً بالكامل يتم تمثيله في الذاكرة بواسطة مصفوفة كثيفة. قد تكون هذه المصفوفة كبيرة جدًا، وبالاقتران مع تكلفة إجراء عملية ضرب المصفوفة الكاملة لكل تكرار من الخوارزمية، يمكن أن يؤدي ذلك إلى أوقات تشغيل طويلة بشكل محظور. من ناحية أخرى، ستنتج نواة KNN مصفوفة متفرقة أكثر ملاءمة للذاكرة، والتي يمكن أن تقلل بشكل كبير من أوقات التشغيل.

الأمثلة
--------

* sphx_glr_auto_examples_semi_supervised_plot_semi_supervised_versus_svm_iris.py
* sphx_glr_auto_examples_semi_supervised_plot_label_propagation_structure.py
* sphx_glr_auto_examples_semi_supervised_plot_label_propagation_digits.py
* sphx_glr_auto_examples_semi_supervised_plot_label_propagation_digits_active_learning.py

المراجع
----------

[2] يوشوا بنجيو، أوليفييه ديلاليو، نيكولاس لو روكس. في التعلم شبه المُشرف (2006)، ص. 193-216

[3] أوليفييه ديلاليو، يوشوا بنجيو، نيكولاس لو روكس. استقراء الدالة غير المُعلمة الفعالة في التعلم شبه المُشرف. AISTAT 2005 https://www.gatsby.ucl.ac.uk/aistats/fullpapers/204.pdf