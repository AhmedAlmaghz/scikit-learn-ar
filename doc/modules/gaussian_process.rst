.. _gaussian_process:

==================
عمليات غاوسية
==================

.. currentmodule:: sklearn.gaussian_process

**العمليات الغاوسية (GP)** هي طريقة تعلم غير بارامترية موجهة تستخدم لحل مشاكل *الانحدار* و *التصنيف الاحتمالي*.

مزايا العمليات الغاوسية هي:

- يتنبأ التنبؤ بقيم بين المشاهدات (على الأقل بالنسبة للأنوية المنتظمة).

- التنبؤ احتمالي (غاوسي) بحيث يمكن حساب فواصل الثقة التجريبية واتخاذ القرارات بناءً على تلك الفواصل إذا كان يجب إعادة التهيئة (الضبط عبر الإنترنت، الضبط التكيفي) للتنبؤ في بعض المناطق ذات الأهمية.

- تنوع: يمكن تحديد :ref:`أنوية <gp_kernels>` مختلفة. يتم توفير الأنوية الشائعة، ولكن من الممكن أيضًا تحديد أنوية مخصصة.

عيوب العمليات الغاوسية تشمل:

- التنفيذ لدينا ليس خفيفا، أي أنها تستخدم معلومات العينات/الميزات الكاملة لإجراء التنبؤ.

- تفقد الكفاءة في الفضاءات عالية الأبعاد - أي عندما يتجاوز عدد الميزات بضع عشرات.


.. _gpr:

انحدار العمليات الغاوسية (GPR)
=================================

.. currentmodule:: sklearn.gaussian_process

يُنفذ :class:`GaussianProcessRegressor` العمليات الغاوسية (GP) لأغراض الانحدار. لهذا، يجب تحديدCastle المسبق لـ GP. ستقوم GP بدمج هذه المعلومة المسبقة ودالة الاحتمالية بناءً على عينات التدريب. إنها تسمح بإعطاء نهج احتمالي للتنبؤ من خلال إعطاء المتوسط والانحراف المعياري كخرج عند التنبؤ.

.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_targets_002.png
   :target: ../auto_examples/gaussian_process/plot_gpr_noisy_targets.html
   :align: center

من المفترض أن يكون المتوسط المسبق ثابتًا وصفرًا (لـ `normalize_y=False`) أو متوسط بيانات التدريب (لـ `normalize_y=True`). يتم تحديد التباين المسبق لـ GP عن طريق تمرير كائن :ref:`kernel <gp_kernels>`. يتم تحسين المعلمات الفوقية للنواة عند تركيب :class:`GaussianProcessRegressor` عن طريق زيادة log-marginal-likelihood (LML) بناءً على `optimizer` الذي تم تمريره. نظرًا لأن LML قد يكون له تحسينات محلية متعددة، يمكن بدء المحسن بشكل متكرر عن طريق تحديد `n_restarts_optimizer`. دائمًا ما يتم إجراء التشغيل الأول بدءًا من قيم المعلمات الفوقية الأولية للنواة؛ يتم إجراء عمليات التشغيل اللاحقة من قيم المعلمات الفوقية التي تم اختيارها عشوائيًا من نطاق القيم المسموح بها. إذا كان يجب الاحتفاظ بثوابت المعلمات الفوقية الأولية، يمكن تمرير `None` كمحسن.

يمكن تحديد مستوى الضوضاء في الأهداف عن طريق تمريره عبر المعلمة `alpha`، إما عالميًا كمقياس أو لكل نقطة بيانات. لاحظ أن مستوى الضوضاء المعتدل يمكن أن يكون مفيدًا أيضًا في التعامل مع حالات عدم الاستقرار الرقمي أثناء التثبيت لأنه يتم تنفيذه بشكل فعال كتقنين Tikhonov، أي عن طريق إضافته إلى قطر مصفوفة النواة. بديل لتحديد مستوى الضوضاء صراحة هو تضمين مكون :class:`~sklearn.gaussian_process.kernels.WhiteKernel` في النواة، والتي يمكنها تقدير مستوى الضوضاء العالمي من البيانات (انظر المثال أدناه). يوضح الشكل أدناه تأثير الهدف المزعج الذي يتم التعامل معه عن طريق تعيين المعلمة `alpha`.

.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_noisy_targets_003.png
   :target: ../auto_examples/gaussian_process/plot_gpr_noisy_targets.html
   :align: center

يستند التنفيذ إلى الخوارزمية 2.1 من [RW2006]_. بالإضافة إلى واجهة برمجة التطبيقات لمقدري scikit-learn القياسيين، :class:`GaussianProcessRegressor`:

* يسمح بالتنبؤ دون تركيب مسبق (بناءً على GP المسبق)

* يوفر طريقة إضافية ``sample_y(X)``، والتي تقيم عينات مأخوذة من GPR (سابقًا أو لاحقًا) عند المدخلات المحددة

* يعرض طريقة ``log_marginal_likelihood(theta)``، والتي يمكن استخدامها خارجيًا لسبل أخرى لاختيار المعلمات الفوقية، على سبيل المثال، عبر Markov chain Monte Carlo.

.. rubric:: أمثلة

* :ref:`sphx_glr_auto_examples_gaussian_process_plot_gpr_noisy_targets.py`
* :ref:`sphx_glr_auto_examples_gaussian_process_plot_gpr_noisy.py`
* :ref:`sphx_glr_auto_examples_gaussian_process_plot_compare_gpr_krr.py`
* :ref:`sphx_glr_auto_examples_gaussian_process_plot_gpr_co2.py`

.. _gpc:

تصنيف العمليات الغاوسية (GPC)
=====================================

.. currentmodule:: sklearn.gaussian_process

يُنفّذ :class:`GaussianProcessClassifier` العمليات الغاوسية (GP) لأغراض التصنيف، وتحديداً للتصنيف الاحتمالي، حيث تأخذ تنبؤات الاختبار شكل احتمالات الفئة. يضع GaussianProcessClassifier افتراضية GP مسبقة على دالة كامنة :math:`f`، والتي يتم بعد ذلك سحقها من خلال دالة ارتباط للحصول على التصنيف الاحتمالي. الدالة الكامنة :math:`f` هي ما يسمى بدالة الإزعاج، التي لا يتم ملاحظة قيمها ولا تكون ذات صلة من تلقاء نفسها. الغرض منها هو السماح بصياغة ملائمة للنموذج، ويتم إزالة :math:`f` (تكاملها) أثناء التنبؤ. ينفذ GaussianProcessClassifier دالة ارتباط لوجستية، التي لا يمكن حساب تكاملها تحليلياً ولكن يمكن تقريبها بسهولة في الحالة الثنائية.

على النقيض من إعداد الانحدار، فإن الخلفي للدالة الكامنة :math:`f` ليس غاوسياً حتى بالنسبة للاحتمالية المسبقة لـ GP نظراً لأن الاحتمالية الغاوسية غير مناسبة لتسميات الفئة المنفصلة. بدلاً من ذلك، يتم استخدام احتمالية غير غاوسية تتوافق مع دالة الارتباط اللوجستية (لوغيت). يقدّر GaussianProcessClassifier الخلفي غير الغاوسي باستخدام تقريب غاوسي يعتمد على تقري

...

ب لابلاس. يمكن العثور على مزيد من التفاصيل في الفصل 3 من [RW2006]_.

يُفترض أن المتوسط المسبق لـ GP هو صفر. يتم تحديد التباين المسبق عن طريق تمرير كائن :ref:`kernel <gp_kernels>`. يتم تحسين المعلمات الفوقية للنواة أثناء تركيب GaussianProcessRegressor من خلال زيادة log-marginal-likelihood (LML) استناداً إلى ``optimizer`` الذي تم تمريره. ونظراً لأن LML قد يكون لديه تحسينات محلية متعددة، يمكن بدء التشغيل المحسن بشكل متكرر عن طريق تحديد ``n_restarts_optimizer``. يتم إجراء التشغيل الأول دائماً بدءاً من القيم الأولية للمعلمات الفوقية للنواة؛ يتم إجراء عمليات التشغيل اللاحقة من قيم المعلمات الفوقية التي تم اختيارها عشوائياً من نطاق القيم المسموح بها. إذا كان يجب الاحتفاظ بالقيم الأولية للمعلمات الفوقية ثابتة، فيمكن تمرير `None` كـ optimizer.

يدعم :class:`GaussianProcessClassifier` التصنيف متعدد الفئات من خلال إجراء تدريب وتوقع يعتمد على واحد مقابل البقية أو واحد مقابل واحد. في واحد مقابل البقية، يتم تركيب واحد من Gaussian process classifier الثنائي لكل فئة، والذي يتم تدريبه لفصل هذه الفئة عن البقية. في "one_vs_one"، يتم تركيب واحد من Gaussian process classifier الثنائي لكل زوج من الفئات، والذي يتم تدريبه لفصل هاتين الفئتين. يتم دمج تنبؤات هذه النماذج الثنائية في تنبؤات متعددة الفئات. راجع القسم حول :ref:`التصنيف متعدد الفئات <multiclass>` لمزيد من التفاصيل.

في حالة تصنيف العمليات الغاوسية، قد يكون "one_vs_one" أرخص حسابياً لأنه يجب حل العديد من المشكلات التي تشمل فقط مجموعة فرعية من مجموعة التدريب بأكملها بدلاً من مشكلات أقل على مجموعة البيانات بأكملها. ونظراً لأن تصنيف العمليات الغاوسية يتدرج بشكل مكعب مع حجم مجموعة البيانات، فقد يكون هذا أسرع بكثير. ومع ذلك، لاحظ أن "one_vs_one" لا يدعم توقع تقديرات الاحتمالية ولكن فقط التنبؤات العادية. علاوة على ذلك، لاحظ أن :class:`GaussianProcessClassifier` لا (بعد) ينفذ تقريب لابلاس متعدد الفئات الحقيقي داخلياً، ولكن كما تمت مناقشته أعلاه، يستند إلى حل العديد من مهام التصنيف الثنائي داخلياً، والتي يتم دمجها باستخدام واحد مقابل البقية أو واحد مقابل واحد.

أمثلة GPC

    

التنبؤات الاحتمالية باستخدام GPC
-------------------------------

يوضح هذا المثال احتمال التنبؤ بـ GPC لنواة RBF باختيارات مختلفة للهايبرباراميترز. يبين الشكل الأول احتمال التنبؤ بـ GPC مع هايبرباراميترز مختارة بشكل تعسفي ومع الهايبرباراميترز المقابلة لأقصى قيمة للوغاريتم اللاحتمي (LML).

بينما الهايبرباراميترز المختارة عن طريق تحسين LML لديها قيمة LML أكبر بشكل ملحوظ، فإنها تؤدي بشكل أسوأ قليلاً وفقًا للخسارة اللوغاريتمية على بيانات الاختبار. يوضح الشكل أن هذا لأن لديهم تغيرًا حادًا في احتمالات الفئة عند حدود الفئة (وهو أمر جيد) ولكن لديهم احتمالات متوقعة قريبة من 0.5 بعيدًا عن حدود الفئة (وهو أمر سيء). هذا التأثير غير المرغوب فيه ناتج عن تقريب لابلاس المستخدم داخليًا بواسطة GPC.

يوضح الشكل الثاني اللوغاريتم اللاحتمي لاختيارات مختلفة للهايبرباراميترز للنواة، مما يسلط الضوء على خيارين من الهايبرباراميترز المستخدمة في الشكل الأول بالنقاط السوداء.

.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_001.png
   :target: ../auto_examples/gaussian_process/plot_gpc.html
   :align: center

.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_002.png
   :target: ../auto_examples/gaussian_process/plot_gpc.html
   :align: center


توضيح GPC على مجموعة بيانات XOR
-------------------------------

.. currentmodule:: sklearn.gaussian_process.kernels

يوضح هذا المثال GPC على بيانات XOR. تتم مقارنة نواة ثابتة ومتساوية الخواص (:class:`RBF`) ونواة غير ثابتة (:class:`DotProduct`). في هذه المجموعة من البيانات المحددة، تحصل نواة :class:`DotProduct` على نتائج أفضل بكثير لأن حدود الفئة خطية وتتوافق مع محاور الإحداثيات. ومع ذلك، في الممارسة العملية، غالبًا ما تحصل النواة الثابتة مثل :class:`RBF` على نتائج أفضل.

.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_xor_001.png
   :target: ../auto_examples/gaussian_process/plot_gpc_xor.html
   :align: center

.. currentmodule:: sklearn.gaussian_process


تصنيف العمليات الغاوسية (GPC) على مجموعة بيانات iris
--------------------------------------------------

يوضح هذا المثال احتمال التنبؤ بـ GPC لنواة RBF متساوية الخواص وغير متساوية الخواص على إصدار ثنائي الأبعاد لمجموعة بيانات iris. يوضح هذا قابلية تطبيق GPC على تصنيف غير ثنائي. تحصل نواة RBF غير متساوية الخواص على قيمة أعلى قليلاً للوغاريتم اللاحتمي عن طريق تعيين مقاييس طول مختلفة لهاتين البعدين للميزة.

.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpc_iris_001.png
   :target: ../auto_examples/gaussian_process/plot_gpc_iris.html
   :align: center


.. _gp_kernels:

نواة للعمليات الغاوسية
    
    (ملاحظة: لم تتم ترجمة بعض العبارات مثل "RBF kernel" و "DotProduct kernel" وغيرها لأنها مصطلحات تقنية محددة في مجال التعلم الآلي.)

هذا نص بتنسيق RST أريد ترجمته إلى اللغة العربية:

==============================
.. currentmodule:: sklearn.gaussian_process.kernels

النوى (وتسمى أيضًا "دوال التباين" في سياق عمليات غاوسية) هي عنصر حاسم في العمليات الغاوسية التي تحدد شكل التوزيع السابق واللاحق للعملية الغاوسية. إنها تشفر الافتراضات حول الدالة التي يتم تعلمها من خلال تحديد "التشابه" لنقطتي بيانات جنبًا إلى جنب مع افتراض أن نقاط البيانات المتشابهة يجب أن يكون لها قيم هدف متشابهة. يمكن التمييز بين فئتين من النوى: تعتمد النوى المتوقفة فقط على المسافة بين نقطتي البيانات وليس على قيمهما المطلقة :math:`k(x_i, x_j)= k(d(x_i, x_j))` وبالتالي فهي ثابتة بالنسبة للترجمات في فضاء الإدخال، بينما تعتمد النوى غير المتوقفة أيضًا على القيم المحددة لنقاط البيانات. يمكن تقسيم النوى المتوقفة إلى نوى متناحية ونوى متباينة، حيث تكون النوى المتناحية ثابتة أيضًا بالنسبة لعمليات الدوران في فضاء الإدخال. لمزيد من التفاصيل، نشير إلى الفصل 4 من [RW2006]_. للحصول على إرشادات حول كيفية الجمع بين النوى المختلفة بشكل أفضل، نشير إلى [Duv2014]_.

.. dropdown:: واجهة برمجة تطبيقات نوى العمليات الغاوسية

   الاستخدام الرئيسي لـ :class:`Kernel` هو حساب التباين المشترك للعملية الغاوسية بين نقاط البيانات. لهذا الغرض، يمكن استدعاء طريقة ``__call__`` للـ kernel. يمكن استخدام هذه الطريقة إما لحساب "المتباين الذاتي" لجميع أزواج نقاط البيانات في المصفوفة ثنائية الأبعاد X، أو "المتباين المشترك" لجميع تركيبات نقاط البيانات في المصفوفة ثنائية الأبعاد X مع نقاط البيانات في المصفوفة ثنائية الأبعاد Y. الصيغة التالية صحيحة لجميع النوى k (باستثناء :class:`WhiteKernel`): ``k(X) == K(X, Y=X)``

   إذا تم استخدام القطري فقط من المتباين الذاتي، فيمكن استدعاء طريقة ``diag()`` للـ kernel، والتي تكون أكثر كفاءة حسابيًا من الاستدعاء المكافئ لـ ``__call__``: ``np.diag(k(X, X)) == k.diag(X)``

   يتم تحديد النوى بواسطة متجه :math:`\theta` من المعلمات الفوقية. يمكن أن تتحكم هذه المعلمات الفوقية، على سبيل المثال، في المقاييس الطولية أو الدورية للنواة (انظر أدناه). تدعم جميع النوى حساب التدرجات التحليلية لمتباين الذاتي للنواة بالنسبة إلى :math:`log(\theta)` عن طريق تعيين ``eval_gradient=True`` في طريقة ``__call__``.
   أي أنه يتم إرجاع مصفوفة ``(len(X), len(X), len(theta))`` حيث تحتوي الخانة ``[i, j, l]`` على :math:`\frac{\partial k_\theta(x_i, x_j)}{\partial log(\theta_l)}`.
   يتم استخدام هذه التدرجات من قبل العملية الغاوسية (كلا من ريجريسور والمصنف) في حساب تدرج لوغاريتم الدالة الهامشية الاحتمالية، والتي يتم استخدامها بدورها لتحديد قيمة :math:`\theta`، والتي تزيد من لوغاريتم الدالة الهامشية الاحتمالية، عبر الصعود التدرجي. لكل معلمة فوقية، يجب تحديد القيمة الأولية والحدود عند إنشاء نسخة من النواة. يمكن الحصول على القيمة الحالية لـ :math:`\theta` وتعيينها عبر الخاصية ``theta`` لكائن النواة. علاوة على ذلك، يمكن الوصول إلى حدود المعلمات الفوقية بواسطة الخاصية ``bounds`` للنواة. لاحظ أن كلا الخاصيتين (theta وbounds) ترجعان قيمًا محولة اللوغاريتم للقيم المستخدمة داخليًا لأن هذه القيم تكون أكثر ملاءمة عادةً للتحسين القائم على التدرج.
   يتم تخزين مواصفات كل معلمة فوقية في شكل مثيل لـ :class:`Hyperparameter` في النواة المعنية. لاحظ أن النواة التي تستخدم معلمة فوقية باسم "x" يجب أن يكون لها السمتان self.x وself.x\_bounds.

   الفئة الأساسية المجردة لجميع النوى هي :class:`Kernel`. تقوم Kernel بتنفيذ واجهة مشابهة لـ :class:`~sklearn.base.BaseEstimator`، وتوفر طريقتين ``get_params()``، و``set_params()``، و``clone()``. هذا يسمح بتعيين قيم النواة أيضًا عبر ميتا-المصنفات مثل :class:`~sklearn.pipeline.Pipeline` أو :class:`~sklearn.model_selection.GridSearchCV`. لاحظ أنه نظرًا للبنية المتداخلة للنوى (عن طريق تطبيق عوامل تشغيل النوى، انظر أدناه)، فقد تصبح أسماء معلمات النواة معقدة نسبيًا. بشكل عام، بالنسبة لمشغل النواة الثنائي، يتم إضافة بادئة ``k1__`` إلى معلمات المعامل الأيسر و``k2__`` إلى معلمات المعامل الأيمن. هناك أيضًا طريقة مريحة إضافية وهي ``clone_with_theta(theta)``، والتي ترجع نسخة مستنسخة من النواة ولكن مع تعيين المعلمات الفوقية إلى ``theta``. مثال توضيحي:

      >>> from sklearn.gaussian_process.kernels import ConstantKernel, RBF
      >>> kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(0.0, 10.0)) * RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0)) + RBF(length_scale=2.0, length_scale_bounds=(0.0, 10.0))
      >>> for hyperparameter in kernel.hyperparameters: print(hyperparameter)
      Hyperparameter(name='k1__k1__constant_value', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)
      Hyperparameter(name='k1__k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)
      Hyperparameter(name='k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)
      >>> params = kernel.get_params()
      >>> for key in sorted(params): print("%s : %s" % (key, params[key]))
      k1 : 1**2 * RBF(length_scale=0.5)
      k1__k1 : 1**2
      k1__k1__constant_value : 1.0
      k1__k1__constant_value_bounds : (0.0, 10.0)
      k1__k2 : RBF(length_scale=0.5)
      k1__k2__length_scale : 0.5
      k1__k2__length_scale_bounds : (0.0, 10.0)
      k2 : RBF(length_scale=2)
      k2__length_scale : 2.0
      k2__length_scale_bounds : (0.0, 10.0)
      >>> print(kernel.theta)  # Note: log-transformed
      [ 0.         -0.69314718  0.69314718]
      >>> print(kernel.bounds)  # Note: log-transformed
      [[      -inf 2.30258509]
      [      -inf 2.30258509]
      [      -inf 2.30258509]]
    
  [RW2006]_ <NAME>, <NAME>. Gaussian Processes for Machine Learning. The MIT Press, 2006.
  [Duv2014]_ <NAME>, <NAME>, and <NAME>. Additive Gaussian Processes. Journal of Machine Learning Research, 15(1):3073-3101, 2014.

جميع نوى العمليات الغاوسية قابلة للتشغيل البيني مع :mod:`sklearn.metrics.pairwise` والعكس صحيح: يمكن تمرير مثيلات الفئات الفرعية لـ :class:`Kernel` كـ ``metric`` إلى ``pairwise_kernels`` من :mod:`sklearn.metrics.pairwise`. بالإضافة إلى ذلك، يمكن استخدام وظائف النواة من pairwise كنواة GP باستخدام فئة الغلاف :class:`PairwiseKernel`. التحذير الوحيد هو أن تدرجparameters hyperparameters ليس تحليليًا ولكنه رقمي وجميع تلك النواة تدعم فقط المسافات المتساوية. يعتبر parameter ``gamma`` من parameters hyperparameters ويمكن تحسينه. يتم تعيين parameters النواة الأخرى مباشرة عند التهيئة وتظل ثابتة.

النوى الأساسية
---------

يمكن استخدام نواة `ConstantKernel` كجزء من نواة `Product` حيث تقوم بتوسيع قيمة المعامل الآخر (النواة) أو كجزء من نواة `Sum`، حيث تقوم بتعديل متوسط العملية الغاوسية. وتعتمد على معامل `constant_value`. وهي معرفة كالتالي:

.. math::
   k(x_i, x_j) = constant\_value \;\forall\; x_1, x_2

يتم استخدام نواة `WhiteKernel` بشكل رئيسي كجزء من نواة المجموع حيث تشرح مكون الضوضاء في الإشارة. ويتم ضبط معامل `noise_level` للتقدير مستوى الضوضاء. وهي معرفة كالتالي:

.. math::
    k(x_i, x_j) = noise\_level \text{ إذا } x_i == x_j \text{ وإلا } 0


عوامل النواة
------------

تأخذ عوامل النواة نواة أو نواتين أساسيتين وتدمجهما في نواة جديدة. تأخذ نواة `Sum` نواتين `k_1` و `k_2` وتدمجهما عبر `k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)`. وتأخذ نواة `Product` نواتين `k_1` و `k_2` وتدمجهما عبر `k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)`. وتأخذ نواة `Exponentiation` نواة أساسية واحدة ومعامل عددي `p` وتدمجهما عبر `k_{exp}(X, Y) = k(X, Y)^p`. تجدر الإشارة إلى أنه يتم تجاوز الأساليب السحرية `__add__` و `__mul___` و `__pow__` على عناصر Kernel، لذلك يمكن استخدام e.g. `RBF() + RBF()` كاختصار لـ `Sum(RBF(), RBF())`.

نواة الدالة الأساس الشعاعي (RBF)
----------------------------------

نواة `RBF` هي نواة ثابتة. وهي معروفة أيضًا باسم نواة "الأسية التربيعية". وتعتمد على معامل طول-المقياس `l>0`، والذي يمكن أن يكون إما عدد حقيقي (متغير متساوي الخواص للنواة) أو متجه بنفس عدد الأبعاد كالمدخلات `x` (متغير غير متساوي الخواص للنواة). وتعطى النواة بواسطة:

.. math::
   k(x_i, x_j) = \text{exp}\left(- \frac{d(x_i, x_j)^2}{2l^2} \right)

حيث `d(\cdot, \cdot)` هي المسافة الإقليدية.

تعتبر هذه النواة قابلة للاختلاف إلى ما لا نهاية، مما يعني أن عمليات GP مع هذه النواة كدالة التباين لها مشتقات متوسطة مربعة من جميع الرتب، وبالتالي فهي ناعمة جدًا. يتم عرض النموذج المسبق والبعدي لعملية GP الناتجة من نواة RBF في الشكل التالي:

.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_001.png
   :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
   :align: center


نواة Matérn
-------------

نواة `Matern` هي نواة ثابتة وتعميم لنواة `RBF`. ولها معامل إضافي `\nu` الذي يتحكم في نعومة الدالة الناتجة. وتعتمد على معامل طول-المقياس `l>0`، والذي يمكن أن يكون إما عدد حقيقي (متغير متساوي الخواص للنواة) أو متجه بنفس عدد الأبعاد كالمدخلات `x` (متغير غير متساوي الخواص للنواة).

.. dropdown:: التنفيذ الرياضي لنواة Matérn

   وتعطى النواة بواسطة:

   .. math::

      k(x_i, x_j) = \frac{1}{\Gamma(\nu)2^{\nu-1}}\Bigg(\frac{\sqrt{2\nu}}{l} d(x_i , x_j )\Bigg)^\nu K_\nu\Bigg(\frac{\sqrt{2\nu}}{l} d(x_i , x_j )\Bigg),

   حيث `d(\cdot,\cdot)` هي المسافة الإقليدية، `K_\nu(\cdot)` هي دوال بيسيل المعدلة و `\Gamma(\cdot)` هي دالة جاما.
   عندما `\nu\rightarrow\infty`، تتقارب نواة Matérn إلى نواة RBF.
   عندما `\nu = 1/2`، تصبح نواة Matérn متطابقة مع نواة الأسية المطلقة، أي،

   .. math::
      k(x_i, x_j) = \exp \Bigg(- \frac{1}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu= \tfrac{1}{2}

   على وجه الخصوص، `\nu = 3/2`:

   .. math::
      k(x_i, x_j) =  \Bigg(1 + \frac{\sqrt{3}}{l} d(x_i , x_j )\Bigg) \exp \Bigg(-\frac{\sqrt{3}}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu= \tfrac{3}{2}

   و `\nu = 5/2`:

   .. math::
      k(x_i, x_j) = \Bigg(1 + \frac{\sqrt{5}}{l} d(x_i , x_j ) +\frac{5}{3l} d(x_i , x_j )^2 \Bigg) \exp \Bigg(-\frac{\sqrt{5}}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu= \tfrac{5}{2}

   هي خيارات شائعة لتعلم الدوال التي ليست قابلة للاختلاف إلى ما لا نهاية (كما هو مفترض من نواة RBF) ولكن قابلة للاختلاف مرة واحدة على الأقل (`\nu = 3/2`) أو مرتين (`\nu = 5/2`).

   تسمح مرونة التحكم في نعومة الدالة المتعلمة عبر `\nu` بالتكيف مع خصائص العلاقة الوظيفية الحقيقية الكامنة.

يعرض الشكل التالي النموذج المسبق والبعدي لعملية GP الناتجة من نواة Matérn:

.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_005.png
   :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
   :align: center

انظر [RW2006]_، ص 84 لمزيد من التفاصيل بخصوص
الاختلافات المختلفة لنواة Matérn.

نواة التربيعية العقلانية
-------------------------

يمكن اعتبار نواة `RationalQuadratic` كمزيج مقياس (مجموع لا نهائي)
لنواة `RBF` مع طول-مقياس مختلف. وتعتمد على معامل طول-مقياس `l>0` ومعامل مزيج مقياس `\alpha>0`
فقط المتغير المتساوي الخواص حيث `l` هو عدد حقيقي مدعوم في الوقت الحالي.
وتعطى النواة بواسطة:

.. math::
   k(x_i, x_j) = \left(1 + \frac{d(x_i, x_j)^2}{2\alpha l^2}\right)^{-\alpha}

يعرض الشكل التالي النموذج المسبق والبعدي لعملية GP الناتجة من نواة `RationalQuadratic`:

.. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_002.png
   :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
   :align: center

نواة Exp-Sine-Squared
--------------------

[RW2006]_ Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes
for Machine Learning. The MIT Press, 2
  تسمح نواة :class:`ExpSineSquared` بنمذجة الدوال الدورية. إنها معلمة بمعلمة طولية :math:`l>0` ومعلمة دورية :math:`p>0`. في الوقت الحالي ، يتم دعم المتغير الفيزيائي فقط حيث :math:`l` هو قياسي. النواة معطاة بواسطة:

    .. math::
       k(x_i, x_j) = \text{exp}\left(- \frac{ 2\sin^2(\pi d(x_i, x_j) / p) }{ l^ 2} \right)

    يظهر المقدمان والأخير لعملية غاوسية ناتجة عن نواة ExpSineSquared في الشكل التالي:

    .. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_003.png
       :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
       :align: center

    نواة منتج النقطة
    ------------------

    نواة :class:`DotProduct` غير ثابتة ويمكن الحصول عليها من الانحدار الخطي عن طريق وضع :math:`N(0، 1)` على معاملات :math:`x_d (d = 1، . . . ، D)` و أ priori of :math:`N(0، \sigma_0^2)` على الانحياز. نواة :class:`DotProduct` غير قابلة للتغيير إلى دوران الإحداثيات حول الأصل ، ولكن ليس الترجمات.
    إنها معلمة بمعامل :math:`\sigma_0^2`. بالنسبة إلى :math:`\sigma_0^pre = 0` ، تسمى النواة نواة خطية متجانسة ، وإلا فهي غير متجانسة. النواة معطاة بواسطة

    .. math::
       k(x_i، x_j) = \sigma_0 ^ 2 + x_i \cdot x_j

    غالبًا ما يتم دمج نواة :class:`DotProduct` مع الأس. يظهر مثال مع الأس 2 في الشكل التالي:

    .. figure:: ../auto_examples/gaussian_process/images/sphx_glr_plot_gpr_prior_posterior_004.png
       :target: ../auto_examples/gaussian_process/plot_gpr_prior_posterior.html
       :align: center

    مراجع
    ----------

    .. [RW2006] `Carl E. Rasmussen and Christopher K.I. Williams،
       "Gaussian Processes for Machine Learning"،
       MIT Press 2006 <https://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_

    .. [Duv2014] `David Duvenaud، "The Kernel Cookbook: Advice on Covariance functions"، 2014
       <https://www.cs.toronto.edu/~duvenaud/cookbook/>`_

    .. currentmodule:: sklearn.gaussian_process
    