.. _scaling_strategies:

استراتيجيات للتوسع الحسابي: بيانات أكبر
=================================================

بالنسبة لبعض التطبيقات، فإن كمية الأمثلة أو الميزات (أو كليهما) و/أو السرعة التي تحتاج إلى معالجتها تشكل تحديًا للأساليب التقليدية. في هذه الحالات، يوفر scikit-learn عددًا من الخيارات التي يمكنك مراعاتها لجعل نظامك قابلاً للتطوير.

التوسع باستخدام مثيلات باستخدام التعلم خارج النواة
--------------------------------------------------

التعلم خارج النواة (أو "الذاكرة الخارجية") هو تقنية مستخدمة للتعلم من البيانات التي لا يمكن أن تتسع في الذاكرة الرئيسية للكمبيوتر (RAM).

فيما يلي مخطط لنظام مصمم لتحقيق هذا الهدف:

1. طريقة لبث المثيلات
2. طريقة لاستخراج الميزات من المثيلات
3. خوارزمية متزايدة

بث مثيلات
....................

بشكل أساسي، قد يكون 1. قارئًا يقوم بإرجاع مثيلات من ملفات على محرك أقراص ثابت أو قاعدة بيانات أو من دفق شبكة، وما إلى ذلك. ومع ذلك، فإن التفاصيل حول كيفية تحقيق ذلك تتجاوز نطاق هذه الوثيقة.

استخراج الميزات
...................

يمكن أن يكون \ 2. أي طريقة ذات صلة لاستخراج الميزات من بين طرق استخراج الميزات المختلفة \ ref: `feature_extraction` \ التي يدعمها scikit-learn. ومع ذلك، عند العمل مع البيانات التي تحتاج إلى تمثيل إحصائي وحيث لا تكون مجموعة الميزات أو القيم معروفة مسبقًا، يجب توخي الحذر الصريح. مثال جيد هو تصنيف النص حيث من المحتمل العثور على مصطلحات غير معروفة أثناء التدريب. من الممكن استخدام جهاز تمثيل إحصائي يحتفظ بحالته إذا كان من المعقول من وجهة نظر التطبيق إجراء عدة تمريرات عبر البيانات. وإلا، يمكنك زيادة الصعوبة باستخدام مستخرج ميزات لا يحتفظ بحالته. في الوقت الحالي، تتمثل الطريقة المفضلة للقيام بذلك في استخدام ما يسمى بـ: ref: `feature_hashing` \ كما هو منفذ بواسطة: class: `sklearn.feature_extraction.FeatureHasher` \ لمجموعات البيانات ذات المتغيرات الفئوية الممثلة على أنها قائمة من القواميس بايثون أو: class: `sklearn.feature_extraction.text.HashingVectorizer` \ لوثائق النص.

التعلم المتزايد
.....................

أخيرًا، بالنسبة لـ 3. لدينا عدد من الخيارات داخل scikit-learn. على الرغم من أن الخوارزميات لا يمكنها جميعًا التعلم بشكل متزايد (أي دون رؤية جميع المثيلات في وقت واحد)، إلا أن جميع التقديرات التي تنفذ واجهة برمجة التطبيقات "partial_fit" هي مرشحة. في الواقع، فإن القدرة على التعلم بشكل متزايد من دفعة مصغرة من المثيلات (تسمى أحيانًا "التعلم عبر الإنترنت") هي مفتاح التعلم خارج النواة حيث تضمن أنه في أي وقت معين، لن يكون هناك سوى كمية صغيرة من المثيلات في الذاكرة الرئيسية. قد يتضمن اختيار حجم جيد لدفعة مصغرة توازنًا بين الملاءمة وبصمة الذاكرة [1] _.

فيما يلي قائمة بالتقديرات المتزايدة لمهام مختلفة:

- التصنيف
    +: class: `sklearn.naive_bayes.MultinomialNB`
    +: class: `sklearn.naive_bayes.BernoulliNB`
    +: class: `sklearn.linear_model.Perceptron`
    +: class: `sklearn.linear_model.SGDClassifier`
    +: class: `sklearn.linear_model.PassiveAggressiveClassifier`
    +: class: `sklearn.neural_network.MLPClassifier`
- الانحدار
    +: class: `sklearn.linear_model.SGDRegressor`
    +: class: `sklearn.linear_model.PassiveAggressiveRegressor`
    +: class: `sklearn.neural_network.MLPRegressor`
- التجميع
    +: class: `sklearn.cluster.MiniBatchKMeans`
    +: class: `sklearn.cluster.Birch`
- التحليل / استخراج الميزات
    +: class: `sklearn.decomposition.MiniBatchDictionaryLearning`
    +: class: `sklearn.decomposition.IncrementalPCA`
    +: class: `sklearn.decomposition.LatentDirichletAllocation`
    +: class: `sklearn.decomposition.MiniBatchNMF`
- ما قبل المعالجة
    +: class: `sklearn.preprocessing.StandardScaler`
    +: class: `sklearn.preprocessing.MinMaxScaler`
    +: class: `sklearn.preprocessing.MaxAbsScaler`

بالنسبة للتصنيف، من المهم ملاحظة أنه على الرغم من أن روتين استخراج الميزات الذي لا يحتفظ بحالته قد يكون قادرًا على التعامل مع الصفات/القيم الجديدة/غير المرئية، إلا أن المتعلم المتزايد نفسه قد لا يتمكن من التعامل مع فئات الأهداف الجديدة/غير المرئية. في هذه الحالة، يجب تمرير جميع الفئات الممكنة إلى أول مكالمة "partial_fit" باستخدام معلمة "classes=".

هناك جانب آخر يجب مراعاته عند اختيار خوارزمية مناسبة وهو أن الخوارزميات لا تعطي جميعها نفس الأهمية لكل مثال بمرور الوقت. على وجه التحديد، لا يزال "Perceptron" حساسًا للنماذج التي تحمل علامات خاطئة حتى بعد العديد من الأمثلة، في حين أن عائلات "SGD*" و"PassiveAggressive*" أكثر مقاومة لهذا النوع من الآثار. على العكس من ذلك، تميل هذه الأخيرة أيضًا إلى إعطاء أهمية أقل للأمثلة المختلفة بشكل ملحوظ، ولكنها مصنفة بشكل صحيح عندما تأتي في وقت متأخر من الدفق حيث ينخفض معدل تعلمها بمرور الوقت.

أمثلة
..........

أخيرًا، لدينا مثال كامل المواصفات على: ref: `sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`. يهدف إلى توفير نقطة انطلاق للأشخاص الذين يرغبون في بناء أنظمة التعلم خارج النواة ويظهر معظم المفاهيم التي تمت مناقشتها أعلاه.

علاوة على ذلك، فإنه يظهر أيضًا تطور أداء الخوارزميات المختلفة مع عدد من الأمثلة المعالجة.

.. |accuracy_over_time| image:: ../auto_examples/applications/images/sphx_glr_plot_out_of_core_classification_001.png
    :target: ../auto_examples/applications/plot_out_of_core_classification.html
    :scale: 80

.. centered:: |accuracy_over_time|

والآن عند النظر في وقت الحساب للأجزاء المختلفة، نرى أن التمثيل الإحصائي أكثر تكلفة بكثير من التعلم نفسه. من بين الخوارزميات المختلفة، فإن "MultinomialNB" هي الأكثر تكلفة، ولكن يمكن تخفيف عبء العمل الإضافي عن طريق زيادة حجم الدفعات المصغرة (التمرين: تغيير "minibatch_size" إلى 100 و10000 في البرنامج ومقارنتهما).

.. |computation_time| image:: ../auto_examples/applications/images/sphx_glr_plot_out_of_core_classification_003.png
    :target: ../auto_examples/applications/plot_out_of_core_classification.html
    :scale: 80

.. centered:: |computation_time|

ملاحظات
......

.. [1] اعتمادًا على الخوارزمية، يمكن أن يؤثر حجم الدفعة المصغرة على النتائج أو لا. SGD*، PassiveAggressive*، وNaiveBayes المنفصلة هي عبر الإنترنت حقًا ولا تتأثر بحجم الدفعة. على العكس من ذلك، يتأثر معدل تقارب MiniBatchKMeans بحجم الدفعة. أيضًا، يمكن أن تختلف بصمة ذاكرته بشكل كبير مع حجم الدفعة.