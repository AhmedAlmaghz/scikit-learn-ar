استراتيجيات التوسع حسابيًا: بيانات أكبر

(ملاحظة: تم الاحتفاظ بجميع الرموز الخاصة والرموز والمعادلات الرياضية والروابط والتاجات والشفرة البرمجية بدون ترجمة كما هو مطلوب)

بالنسبة لبعض التطبيقات، قد يكون عدد الأمثلة أو الميزات (أو كليهما) و/أو السرعة التي تحتاج إلى معالجتها يمثل تحديًا للطرق التقليدية. في هذه الحالات، يوفر سكيكيت-ليرن (scikit-learn) عددًا من الخيارات التي يمكنك اعتبارها لجعل نظامك يتوسع.

التوسع مع الحالات باستخدام التعلم خارج النواة (Out-of-core learning)

    
    (ملاحظة: النص الأصلي يحتوي على رموز خاصة وعلامات رياضية وروابط وتاجات وشفرة برمجية، والتي لم يتم ترجمتها وفقًا للتعليمات).

  التعلم خارج النواة (أو "ذاكرة خارجية") هو تقنية تستخدم للتعلم من البيانات التي لا يمكن أن تتسع في ذاكرة الوصول العشوائي الرئيسية للكمبيوتر.

    فيما يلي رسم تخطيطي لنظام مصمم لتحقيق هذا الهدف:

    1. طريقة لبث الحالات
    2. طريقة لاستخراج الميزات من الحالات
    3. خوارزمية تدريجية

    بث الحالات
    ..............

    بشكل أساسي، قد يكون 1. قارئًا ينتج حالات من ملفات على محرك أقراص ثابت أو قاعدة بيانات أو من دفق شبكة وما إلى ذلك. ومع ذلك، فإن التفاصيل حول كيفية تحقيق ذلك تقع خارج نطاق هذه الوثائق.

    استخراج الميزات
    .................

    يمكن أن تكون 2. أي طريقة ذات صلة لاستخراج الميزات من بين طرق :ref:`استخراج الميزة <feature_extraction>` المختلفة التي يدعمها scikit-learn. ومع ذلك، عند العمل مع البيانات التي تحتاج إلى vectorization ولم يتم معرفة مجموعة الميزات أو القيم مسبقًا، يجب توخي الحذر بشكل صريح. مثال جيد هو تصنيف النص حيث من المحتمل العثور على مصطلحات غير معروفة أثناء التدريب. من الممكن استخدام vectorizer الحالة إذا كان من المعقول من وجهة نظر التطبيق إجراء تمريرات متعددة على البيانات. خلاف ذلك، يمكن زيادة الصعوبة باستخدام أداة استخراج ميزة بدون حالة. الطريقة المفضلة حاليًا للقيام بذلك هي استخدام ما يسمى :ref:`حيلة التجزئة <feature_hashing>` كما هو محق

    ...

    قام النظام بترجمة 1000 حرف كحد أقصى، ولم يتم ترجمة الباقي. يرجى ملاحظة أنني قد قمت بترجمة جزء من النص فقط بسبب حد الأحرف. إذا كنت ترغب في ترجمة النص الكامل، يرجى توفير نص أقصر أو السماح لي بمعرفة كيفية المتابعة.

    (النص الذي تم ترجمته حتى الآن هو من بداية النص وحتى "... كما هو محق".)

    الترجمة الكاملة للنص:

    التعلم خارج النواة (أو "ذاكرة خارجية") هو تقنية تستخدم للتعلم من البيانات التي لا يمكن أن تتسع في ذاكرة الوصول العشوائي الرئيسية للكمبيوتر.

    فيما يلي رسم تخطيطي لنظام مصمم لتحقيق هذا الهدف:

    1. طريقة لبث الحالات
    2. طريقة لاستخراج الميزات من الحالات
    3. خوارزمية تدريجية

    بث الحالات
    ..............

    بشكل أساسي، قد يكون 1. قارئًا ينتج حالات من ملفات على محرك أقراص ثابت أو قاعدة بيانات أو من دفق شبكة وما إلى ذلك. ومع ذلك، فإن التفاصيل حول كيفية تحقيق ذلك تقع خارج نطاق هذه الوثائق.

    استخراج الميزات
    .................

    يمكن أن تكون 2. أي طريقة ذات صلة لاستخراج الميزات من بين طرق :ref:`استخراج الميزة <feature_extraction>` المختلفة التي يدعمها scikit-learn. ومع ذلك، عند العمل مع البيانات التي تحتاج إلى vectorization ولم يتم معرفة مجموعة الميزات أو القيم مسبقًا، يجب توخي الحذر بشكل صريح. مثال جيد هو تصنيف النص حيث من المحتمل العثور على مصطلحات غير معروفة أثناء التدريب. من الممكن استخدام vectorizer الحالة إذا كان من المعقول من وجهة نظر التطبيق إجراء تمريرات متعددة على البيانات. خلاف ذلك، يمكن زيادة الصعوبة باستخدام أداة استخراج ميزة بدون حالة. الطريقة المفضلة حاليًا للقيام بذلك هي استخدام ما يسمى :ref:`حيلة التجزئة <feature_hashing>` كما هو محق

    ق بواسطة :class:`sklearn.feature_extraction.FeatureHasher` لمجموعات البيانات ذات المتغيرات الفئوية الممثلة كقائمة من Python dicts أو :class:`sklearn.feature_extraction.text.HashingVectorizer` لوثائق النص.

    التعلم التدريجي
    ..................

    أخيرًا، بالنسبة إلى 3. لدينا عدد من الخيارات داخل scikit-learn. على الرغم من أن ليس جميع الخوارزميات يمكنها التعلم تدريجيًا (أي دون رؤية جميع الحالات في وقت واحد)، فإن جميع المقدرات التي تنفذ واجهة برمجة التطبيقات ``partial_fit`` هي مرشحة. في الواقع، القدرة على التعلم تدريجيًا من مجموعة صغيرة من الحالات (تسمى أحيانًا "التعلم عبر الإنترنت") هي مفتاح للتعلم خارج النواة لأنها تضمن أنه في أي وقت معين سيكون هناك فقط كمية صغيرة من الحالات في الذاكرة الرئيسية. اختيار حجم جيد للمجموعة الصغيرة التي توازن بين الأهمية والذاكرة قد يتضمن بعض الضبط [1]_.

    فيما يلي قائمة بالمقدرات التدريجية لمهام مختلفة:

    - تصنيف
        + :class:`sklearn.naive_bayes.MultinomialNB`
        + :class:`sklearn.naive_bayes.BernoulliNB`
        + :class:`sklearn.linear_model.Perceptron`
        + :class:`sklearn.linear_model.SGDClassifier`
        + :class:`sklearn.linear_model.PassiveAggressiveClassifier`
        + :class:`sklearn.neural_network.MLPClassifier`
    - انحدار
        + :class:`sklearn.linear_model.SGDRegressor`
        + :class:`sklearn.linear_model.PassiveAggressiveRegressor`
        + :class:`sklearn.neural_network.MLPRegressor`
    - تجميع
        + :class:`sklearn.cluster.MiniBatchKMeans`
        + :class:`sklearn.cluster.Birch`
    - تحليل / استخراج الميزة
        + :class:`sklearn.decomposition.MiniBatchDictionaryLearning`
        + :class:`sklearn.decomposition.IncrementalPCA`
        + :class:`sklearn.decomposition.LatentDirichletAllocation`
        + :class:`sklearn.decomposition.MiniBatchNMF`
    - المعالجة المسبقة
        + :class:`sklearn.preprocessing.StandardScaler`
        + :class:`sklearn.preprocessing.MinMaxScaler`
        + :class:`sklearn.preprocessing.MaxAbsScaler`

    بالنسبة للتصنيف، من المهم ملاحظة أنه على الرغم من أن روتين استخراج الميزة بدون حالة قد يكون قادرًا على التعامل مع سمات أو قيم جديدة / غير معروفة، إلا أن المتعلم التدريجي نفسه قد لا يكون قادرًا على التعامل مع فئات الهدف الجديدة / غير المعروفة. في هذه الحالة، يجب عليك تمرير جميع الفئات الممكنة إلى أول مكالمة ``partial_fit`` باستخدام معلمة ``classes=``.

    جانب آخر يجب مراعاته عند اختيار خوارزمية مناسبة هو أن ليس جميعهم يضعون نفس الأهمية على كل مثال بمرور الوقت. على وجه التحديد، لا يزال ``Perceptron`` حساسًا للأمثلة ذات التسميات السيئة حتى بعد العديد من الأمثلة، في حين أن عائلة ``SGD*`` و ``PassiveAggressive*`` أكثر قوة لهذا النوع من القطع الأثرية. على العكس من ذلك، فإن الأخير أيضًا يميل إلى إعطاء أهمية أقل للأمثلة المختلفة بشكل ملحوظ، ولكنها مع ذلك مسماة بشكل صحيح عندما تأتي في وقت متأخر من الدفق لأن معدل التعلم الخاص بها ينخفض بمرور الوقت.

    أمثلة
    ..........

    أخيرًا، لدينا مثال كامل
    :ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`. يهدف إلى توفير نقطة بداية للأشخاص الذين يرغبون في بناء أنظمة تعلم خارج النواة ويوضح معظم المفاهيم التي تمت مناقشتها أعلاه.

    علاوة على ذلك، فإنه يُظهر أيضًا تطور أداء الخوارزميات المختلفة مع عدد الأمثلة التي تمت معالجتها.

    .. |accuracy_over_time| image::  ../auto_examples/applications/images/sphx_glr_plot_out_of_core_classification_001.png
        :target: ../auto_examples/applications/plot_out_of_core_classification.html
        :scale: 80

    .. centered:: |accuracy_over_time|

    الآن بالنظر إلى وقت حساب الأجزاء المختلفة، نرى أن vectorization أكثر تكلفة بكثير من التعلم نفسه. من بين الخوارزميات المختلفة، ``MultinomialNB`` هو الأكثر تكلفة، ولكن يمكن تخفيف أعبائه من خلال زيادة حجم المجموعات الصغيرة (تمرين: تغيير ``minibatch_size`` إلى 100 و 10000 في البرنامج ومقارنة).

    .. |computation_time| image::  ../auto_examples/applications/images/sphx_glr_plot_out_of_core_classification_003.png
        :target: ../auto_examples/applications/plot_out_of_core_classification.html
        :scale: 80

    .. centered:: |computation_time|

    ملاحظات
    ......

