
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="الاستخدام" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://scikit-learn/stable/modules/ensemble.html" />
<meta property="og:site_name" content="scikit-learn" />
<meta property="og:description" content="طريقة التجميع: Gradient boosting, random forests, bagging, voting, stacking طرق التجميع تجمع تنبؤات العديد من المُقدرات الأساسية التي تم بناؤها باستخدام خوارزمية تعلم معينة من أجل تحسين قابلية التع..." />
<meta property="og:image" content="https://scikit-learn/stable/_images/sphx_glr_plot_gradient_boosting_regression_001.png" />
<meta property="og:image:alt" content="scikit-learn" />
<meta name="description" content="طريقة التجميع: Gradient boosting, random forests, bagging, voting, stacking طرق التجميع تجمع تنبؤات العديد من المُقدرات الأساسية التي تم بناؤها باستخدام خوارزمية تعلم معينة من أجل تحسين قابلية التع..." />

    <title>الاستخدام &#8212; scikit-learn 1.5.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/colors.css?v=cc94ab7d" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/custom.css?v=e4cb1417" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=44dfd65d"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=97f0b27d"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script data-domain="scikit-learn.org" defer="defer" src="https://views.scientific-python.org/js/script.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/ensemble';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.15.4';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://scikit-learn.org/dev/_static/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '1.5.1';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = true;
        </script>
    <script src="../_static/scripts/dropdown.js?v=e2048168"></script>
    <script src="../_static/scripts/version-switcher.js?v=a6dd8357"></script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/scikit-learn-logo-small.png" class="logo__image only-light" alt="scikit-learn homepage"/>
    <script>document.write(`<img src="../_static/scikit-learn-logo-small.png" class="logo__image only-dark" alt="scikit-learn homepage"/>`);</script>
  
  
</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../install.html">
    Install
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide.html">
    مرجع المستخدم
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/index.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://blog.scikit-learn.org/">
    Community
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../getting_started.html">
    بدء الاستخدام
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../whats_new.html">
    تاريخ الإصدارات
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../glossary.html">
    Glossary
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-external" href="https://scikit-learn.org/dev/developers/index.html">
    Development
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../faq.html">
    FAQ
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../support.html">
    الدعم
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../related_projects.html">
    التعاون مع الأطر الأخرى وتحسينها
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../roadmap.html">
    خارطة الطريق
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../governance.html">
    Governance
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../about.html">
    الحوكمة
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
        </div>
      
      
        <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/scikit-learn/scikit-learn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-2"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-2"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-2"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-2">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../install.html">
    Install
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide.html">
    مرجع المستخدم
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/index.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://blog.scikit-learn.org/">
    Community
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../getting_started.html">
    بدء الاستخدام
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../whats_new.html">
    تاريخ الإصدارات
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../glossary.html">
    Glossary
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-external" href="https://scikit-learn.org/dev/developers/index.html">
    Development
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../support.html">
    الدعم
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../related_projects.html">
    التعاون مع الأطر الأخرى وتحسينها
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../roadmap.html">
    خارطة الطريق
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../governance.html">
    Governance
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../about.html">
    الحوكمة
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/scikit-learn/scikit-learn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
</ul></div>
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="pst-version-switcher-button-3"
      type="button"
      class="version-switcher__button btn btn-sm dropdown-toggle"
      data-bs-toggle="dropdown"
      aria-haspopup="listbox"
      aria-controls="pst-version-switcher-list-3"
      aria-label="Version switcher list"
    >
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="pst-version-switcher-list-3"
      class="version-switcher__menu dropdown-menu list-group-flush py-0"
      role="listbox" aria-labelledby="pst-version-switcher-button-3">
      <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page">الاستخدام</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p>طريقة التجميع: Gradient boosting, random forests, bagging, voting, stacking</p>
<p><strong>طرق التجميع</strong> تجمع تنبؤات العديد من المُقدرات الأساسية التي تم بناؤها باستخدام خوارزمية تعلم معينة من أجل تحسين قابلية التعميم/الصلابة مقارنة بمقدر واحد.</p>
<p>هناك مثالان شهيران جدا لطرق التجميع هما: Gradient-boosted trees و Random forests.</p>
<p>وبشكل أكثر عمومية، يمكن تطبيق نماذج التجميع على أي متعلم أساسي بخلاف الأشجار، في طرق المتوسطات مثل Bagging methods و model stacking أو Voting، أو في التعزيز، كما هو الحال في AdaBoost.
طريقة “غراديانت تري بوستينغ” أو “غراديانت بوستيد ديكيزن تريز” (جى بى دي تي) هي تعميم لطريقة “البوستينغ” باستخدام دالة خسارة قابلة للاشتقاق، انظر إلى العمل الرائد لـ [Friedman2001]. تعتبر طريقة جى بى دي تي نموذجًا ممتازًا لكل من الانحدار والتصنيف، خاصةً لبيانات الجداول.</p>
<p><strong>موضوع:</strong> <code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> مقابل <code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code>:</p>
<p>يوفر سكيت-ليرن تنفيذين لطريقة الجرديانت-بوستيد تريز: <code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code> مقابل <code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> للتصنيف، وفئات مماثلة للانحدار. يمكن أن يكون الأول أسرع من الأخير <strong>بأضعاف</strong> عندما يكون عدد العينات أكبر من عشرات الآلاف.</p>
<p>تدعم النسخة Hist… بشكلٍ أصيل القيم المفقودة والبيانات الفئوية، مما يلغي الحاجة إلى معالجة مسبقة إضافية مثل الاستيفاء.</p>
<p>قد يُفضل استخدام <code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> و <code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code> لعينات صغيرة الحجم، لأن التصنيف إلى فئات قد يؤدي إلى نقاط انقسام تقريبية للغاية في هذا السياق.</p>
<p id="histogram-based-gradient-boosting">طريقة Gradient Boosting المستندة إلى التصنيف
قدمت Scikit-learn 0.21 تنفيذين جديدين لشجرة Gradient Boosted، وهما على وجه التحديد: class: ‘HistGradientBoostingClassifier’ و class: ‘HistGradientBoostingRegressor’، المستوحاة من ‘LightGBM’ (راجع [LightGBM]).</p>
<p>يمكن أن تكون هذه المقدرات القائمة على المخطط التكراري أسرع <strong>بمقدار كبير</strong> من class: ‘GradientBoostingClassifier’ و class: ‘GradientBoostingRegressor’ عندما يكون عدد العينات أكبر من عشرات الآلاف من العينات.</p>
<p>كما أن لديها دعم مدمج للقيم المفقودة، مما يلغي الحاجة إلى imputer.</p>
<p>تقوم هذه المقدرات السريعة أولاً بتقسيم عينات الإدخال “X” إلى صناديق ذات قيم صحيحة (عادة 256 صندوقًا) مما يقلل بشكل كبير من عدد نقاط الانقسام التي يجب مراعاتها، ويتيح للخوارزمية الاستفادة من البنى البيانات المعتمدة على الأعداد الصحيحة (المخططات التكرارية) بدلاً من الاعتماد على القيم المستمرة المرتبة عند بناء الأشجار. تختلف واجهة برمجة التطبيقات الخاصة بهذه المقدرات قليلاً، ولا يتم دعم بعض الميزات من class: ‘GradientBoostingClassifier’ و class: ‘GradientBoostingRegressor’ بعد، على سبيل المثال بعض دالات الخسارة.</p>
<p class="rubric">أمثلة</p>
<ul class="simple">
<li><dl class="field-list simple">
<dt class="field-odd">ref<span class="colon">:</span></dt>
<dd class="field-odd"><p>‘sphx_glr_auto_examples_inspection_plot_partial_dependence.py’</p>
</dd>
</dl>
</li>
<li><dl class="field-list simple">
<dt class="field-odd">ref<span class="colon">:</span></dt>
<dd class="field-odd"><p>‘sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py’</p>
</dd>
</dl>
</li>
</ul>
<section id="id1">
<h1>الاستخدام<a class="headerlink" href="#id1" title="Link to this heading">#</a></h1>
<p>لا تتغير معظم المعلمات من class: ‘GradientBoostingClassifier’ و class: ‘GradientBoostingRegressor’.</p>
<p>الاستثناء هو معلمة “max_iter” التي تحل محل “n_estimators”، وتتحكم في عدد تكرارات عملية التعزيز:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_hastie_10_2</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.8965</span>
</pre></div>
</div>
<p>تتوفر خسائر <strong>الانحدار</strong> التالية:</p>
<ul class="simple">
<li><p>‘squared_error’، وهو دالة الخسارة الافتراضية؛</p></li>
<li><p>‘absolute_error’، وهو أقل حساسية للقيم الشاذة من الخطأ المربع؛</p></li>
<li><p>‘gamma’، وهو مناسب لنمذجة النتائج الإيجابية الصارمة؛</p></li>
<li><p>‘poisson’، وهو مناسب لنمذجة العد والترددات؛</p></li>
<li><p>‘quantile’، والذي يسمح بتقدير كمية شرطية يمكن استخدامها لاحقًا للحصول على فترات تنبؤ.</p></li>
</ul>
<p>بالنسبة لل**تصنيف**، فإن ‘log_loss’ هو الخيار الوحيد. بالنسبة للتصنيف الثنائي، فإنه يستخدم دالة الخسارة اللوغاريتمية الثنائية، المعروفة أيضًا باسم الانحراف الثنائي أو الانتروبيا الثنائية. بالنسبة لـ ‘n_classes &gt;= 3’، فإنه يستخدم دالة الخسارة اللوغاريتمية متعددة الفئات، مع الانحراف متعدد الحدود والانتروبيا الفئوية كأسماء بديلة. يتم تحديد إصدار الخسارة المناسب بناءً على ‘y’ التي تم تمريرها إلى ‘fit’.</p>
<p>يمكن التحكم في حجم الأشجار من خلال معلمات “max_leaf_nodes” و “max_depth” و “min_samples_leaf”.</p>
<p>يتم التحكم في عدد الصناديق المستخدمة لتقسيم البيانات بواسطة معلمة “max_bins”. إن استخدام صناديق أقل يعمل كشكل من أشكال التنظيم. يوصى عمومًا باستخدام أكبر عدد ممكن من الصناديق (255)، وهو الافتراضي.</p>
<p>تعمل معلمة “l2_regularization” كمنظم لدالة الخسارة، وتتوافق مع ‘lambda’ في التعبير التالي (راجع المعادلة (2) في [XGBoost]):</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\phi) = \sum_i l(\hat{y}_i, y_i) + \frac12 \sum_k \lambda ||w_k||^2\]</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" id="تفاصيل-عن-التنظيم-l2">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">تفاصيل عن التنظيم L2<a class="headerlink" href="#تفاصيل-عن-التنظيم-l2" title="Link to this dropdown">#</a></span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">من المهم ملاحظة أن مصطلح الخسارة: math: ‘l (hat {y} _i، y_i)’ يصف
نصف دالة الخسارة الفعلية فقط باستثناء الخسارة pinball والخطأ المطلق.</p>
<p class="sd-card-text">يشير الفهرس: math: ‘k’ إلى الشجرة k-th في مجموعة الأشجار. في
حالة مشكلات الانحدار والتصنيف الثنائي، تنمو نماذج التعزيز التدريجي شجرة واحدة
لكل تكرار، ثم: math: ‘k’ يعمل حتى ‘max_iter’. في حالة
مشكلات التصنيف متعدد الفئات، تكون القيمة القصوى لفهرس: math: ‘k’ هي
‘n_classes’ :math: ‘times` ‘max_iter’.</p>
<p class="sd-card-text">إذا: math: ‘T_k’ يمثل عدد الأوراق في الشجرة k-th، ثم: math: ‘w_k’
هو متجه بطول: math: ‘T_k’، والذي يحتوي على قيم الأوراق على شكل ‘w
= -sum_gradient / (sum_hessian + l2_regularization)’ (راجع المعادلة (5) في
[XGBoost]).</p>
<p class="sd-card-text">يتم اشتقاق قيم الأوراق: math: ‘w_k’ عن طريق قسمة مجموع تدرجات
دالة الخسارة على مجموع مجموع القيم الذاتية. يضيف إضافة التنظيم إلى
المقام عقوبة على الأوراق ذات القيم الذاتية الصغيرة (المناطق المسطحة)،
مما يؤدي إلى تحديثات أصغر. ثم تساهم قيم: math: ‘w_k’ هذه في
تنبؤ النموذج لعينة معينة تنتهي في الورقة المقابلة. التنبؤ النهائي هو
مجموع التنبؤ الأساسي ومساهمات كل شجرة. نتيجة هذا المجموع هي
ثم يتم تحويلها بواسطة دالة الرابط العكسي اعتمادًا على اختيار دالة الخسارة (راجع
:ref: ‘gradient_boosting_formulation’).</p>
<p class="sd-card-text">لاحظ أن الورقة الأصلية <a class="reference internal" href="#xgboost" id="id2"><span>[XGBoost]</span></a> تقدم مصطلحًا: math: ‘gamma sum_k
T_k’ الذي يعاقب عدد الأوراق (مما يجعلها نسخة سلسة من
‘max_leaf_nodes’) غير مقدمة هنا نظرًا لعدم تنفيذها في scikit-learn؛ في حين أن: math: ‘lambda’ يعاقب على حجم تنبؤات الشجرة الفردية قبل إعادة تحجيمها بمعدل التعلم، راجع
:ref: ‘gradient_boosting_shrinkage’.</p>
</div>
</details><p>لاحظ أن <strong>التوقف المبكر ممكن بشكل افتراضي إذا كان عدد العينات أكبر من 10000</strong>. يتم التحكم في سلوك التوقف المبكر من خلال معلمات “early_stopping” و “scoring” و “validation_fraction” و “n_iter_no_change” و “tol”. من الممكن التوقف المبكر باستخدام أي مصنف، أو مجرد خسارة التدريب أو التحقق من الصحة. لاحظ أنه لأسباب فنية، يكون استخدام مصنف قابل للاستدعاء أبطأ بكثير من استخدام الخسارة. بشكل افتراضي، يتم تنفيذ التوقف المبكر إذا كان هناك ما لا يقل عن 10000 عينة في مجموعة التدريب، باستخدام خسارة التحقق من الصحة.</p>
</section>
<section id="nan-support-hgbt">
<span id="id3"></span><h1>دعم القيم المفقودة<a class="headerlink" href="#nan-support-hgbt" title="Link to this heading">#</a></h1>
<p>لدى class: ‘HistGradientBoostingClassifier’ و class: ‘HistGradientBoostingRegressor’ دعم مدمج للقيم المفقودة (NaNs).</p>
<p>أثناء التدريب، يتعلم منشئ الشجرة في كل نقطة انقسام ما إذا كان يجب إرسال العينات ذات القيم المفقودة إلى الطفل الأيسر أو الأيمن، بناءً على المكسب المحتمل. عند التنبؤ، يتم تعيين العينات ذات القيم المفقودة إلى الطفل الأيسر أو الأيمن بناءً على ذلك:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([0, 0, 1, 1])</span>
</pre></div>
</div>
<p>عندما يكون نمط القيم المفقودة تنبئيًا، يمكن إجراء الانقسامات بناءً على ما إذا كانت قيمة الميزة مفقودة أم لا:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                                      <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                                      <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                                      <span class="n">max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([0, 1, 0, 0, 1])</span>
</pre></div>
</div>
<p>إذا لم يتم العثور على أي قيم مفقودة لميزة معينة أثناء التدريب، يتم تعيين العينات ذات القيم المفقودة إلى الطفل الذي يحتوي على معظم العينات.</p>
<p class="rubric">أمثلة</p>
<ul class="simple">
<li><dl class="field-list simple">
<dt class="field-odd">ref<span class="colon">:</span></dt>
<dd class="field-odd"><p>‘sphx_glr_auto_examples_ensemble_plot_hgbt_regression.py’</p>
</dd>
</dl>
</li>
</ul>
</section>
<section id="sw-hgbdt">
<span id="id4"></span><h1>دعم وزن العينة<a class="headerlink" href="#sw-hgbdt" title="Link to this heading">#</a></h1>
<p>يدعم class: ‘HistGradientBoostingClassifier’ و class: ‘HistGradientBoostingRegressor’ أوزان العينات أثناء: term: ‘fit’.</p>
<p>يوضح المثال التالي كيف يتم تجاهل العينات ذات وزن العينة صفر:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; X = [[1, 0],
...      [1, 0],
...      [1, 0],
...      [0, 1]]
&gt;&gt;&gt; y = [0, 0, 1, 0]
&gt;&gt;&gt; # تجاهل أول عينات تدريب 2 من خلال تعيين أوزانها إلى 0
&gt;&gt;&gt; sample_weight = [0, 0, 1, 1]
&gt;&gt;&gt; gb = HistGradientBoostingClassifier(min_samples_leaf=1)
&gt;&gt;&gt; gb.fit(X، y، sample_weight=sample_weight)
HistGradientBoostingClassifier (...)
&gt;&gt;&gt; gb.predict ([[1،0]])
array ([1])
&gt;&gt;&gt; gb.predict_proba ([[1،0]]) [0،1]
0.99 ...
</pre></div>
</div>
<p>كما ترون، يتم تصنيف ‘[1، 0]’ بشكل مريح على أنه ‘1’ نظرًا لتجاهل أول عينات تدريب 2 بسبب أوزان العينات الخاصة بها.</p>
<p>تفاصيل التنفيذ: إن أخذ أوزان العينات في الاعتبار يعادل ضرب التدرجات (والقيم الذاتية) بأوزان العينات. لاحظ أن مرحلة التصنيف (بشكل خاص حساب الكميات) لا تأخذ الأوزان في الاعتبار.</p>
</section>
<section id="categorical-support-gbdt">
<span id="id5"></span><h1>دعم الميزات الفئوية<a class="headerlink" href="#categorical-support-gbdt" title="Link to this heading">#</a></h1>
<p>لدى class: ‘HistGradientBoostingClassifier’ و class: ‘HistGradientBoostingRegressor’ دعم أصلي للميزات الفئوية: يمكنهما مراعاة الانقسامات على البيانات الفئوية غير المرتبة.</p>
<p>بالنسبة لمجموعات البيانات التي تحتوي على ميزات فئوية، غالبًا ما يكون من الأفضل استخدام الدعم الفئوي الأصلي بدلاً من الترميز أحادي الساخن (: class: ‘~sklearn.preprocessing.OneHotEncoder’)، لأن الترميز أحادي الساخن يتطلب المزيد من عمق الشجرة لتحقيق انقسامات مكافئة. من الأفضل عادةً الاعتماد على الدعم الفئوي الأصلي بدلاً من التعامل مع الميزات الفئوية على أنها مستمرة (ترتيبية)، والتي تحدث لبيانات الميزات الفئوية الترتيبية، حيث أن الفئات هي كميات اسمية لا يهم فيها الترتيب.</p>
<p>لتمكين الدعم الفئوي، يمكن تمرير قناع منطقي إلى معلمة “categorical_features”، مما يشير إلى ما إذا كانت الميزة فئوية أم لا. في ما يلي، سيتم التعامل مع الميزة الأولى على أنها فئوية والثانية على أنها رقمية:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">categorical_features</span><span class="o">=</span><span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">])</span>
</pre></div>
</div>
<p>وبالمثل، يمكن تمرير قائمة من الأعداد الصحيحة التي تشير إلى فهارس الميزات الفئوية:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">categorical_features</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>عندما يكون الإدخال عبارة عن DataFrame، يمكن أيضًا تمرير قائمة بأسماء الأعمدة:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt;&gt;&gt; gbdt = HistGradientBoostingClassifier(categorical_features=[&quot;site&quot;، &quot;manufacturer&quot;])
</pre></div>
</div>
<p>أخيرًا، عندما يكون الإدخال عبارة عن DataFrame، يمكننا استخدام “categorical_features”=”from_dtype” في هذه الحالة، سيتم التعامل مع جميع الأعمدة ذات النوع الفئوي ‘dtype’ على أنها ميزات فئوية.</p>
<p>يجب أن تكون قابلية كل ميزة فئوية أقل من معلمة “max_bins”. للحصول على مثال على استخدام التعزيز التدريجي القائم على المخطط التكراري على الميزات الفئوية، راجع
:ref: ‘sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py’.</p>
<p>إذا كانت هناك قيم مفقودة أثناء التدريب، فسيتم التعامل مع القيم المفقودة على أنها فئة صحيحة. إذا لم تكن هناك قيم مفقودة أثناء التدريب، فسيتم تعيين القيم المفقودة عند التنبؤ إلى العقدة الفرعية التي تحتوي على معظم العينات (مثل الميزات المستمرة). عند التنبؤ، يتم التعامل مع الفئات التي لم يتم رؤيتها أثناء وقت التجهيز على أنها قيم مفقودة.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" id="العثور-على-الانقسامات-مع-الميزات-الفئوية">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">العثور على الانقسامات مع الميزات الفئوية<a class="headerlink" href="#العثور-على-الانقسامات-مع-الميزات-الفئوية" title="Link to this dropdown">#</a></span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">الطريقة المعتادة للنظر في الانقسامات الفئوية في شجرة هي النظر في جميع الأقسام: math: ‘2 ^ {K - 1} - 1’، حيث: math: ‘K’ هو عدد
الفئات. يمكن أن يصبح هذا الأمر سريعًا محظورًا عندما يكون: math: ‘K’ كبيرًا. لحسن الحظ، نظرًا لأن أشجار التعزيز التدريجي هي دائمًا أشجار انحدار (حتى
لمشكلات التصنيف)، توجد استراتيجية أسرع يمكن أن تؤدي إلى انقسامات مكافئة. أولاً، يتم فرز الفئات الخاصة بميزة ما وفقًا لتغاير هدف التباين، لكل فئة ‘k’. بمجرد فرز الفئات، يمكن اعتبار <em>التقسيمات المستمرة</em>، أي معاملة الفئات كما لو كانت قيمًا مستمرة مرتبة (راجع Fisher <a class="reference internal" href="#fisher1958" id="id6"><span>[Fisher1958]</span></a> لإثبات رسمي). ونتيجة لذلك، يجب مراعاة: math: ‘K - 1’ فقط من الانقسامات بدلاً من: math: ‘2 ^ {K - 1} - 1’. الفرز الأولي هو عملية: math: ‘mathcal {O} (K log (K))’، مما يؤدي إلى تعقيد إجمالي قدره: math: ‘mathcal {O} (K log (K) + K)’، بدلاً من: math: ‘O (2 ^ K)’.</p>
</div>
</details><p class="rubric">أمثلة</p>
<ul class="simple">
<li><dl class="field-list simple">
<dt class="field-odd">ref<span class="colon">:</span></dt>
<dd class="field-odd"><p>‘sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py’</p>
</dd>
</dl>
</li>
</ul>
</section>
<section id="monotonic-cst-gbdt">
<span id="id7"></span><h1>القيود الأحادية الاتجاه<a class="headerlink" href="#monotonic-cst-gbdt" title="Link to this heading">#</a></h1>
<p>فيما يلي ترجمة للنص المحدد بتنسيق RST إلى اللغة العربية، مع اتباع التعليمات المذكورة:</p>
<p>اعتمادًا على المشكلة التي بين يديك، قد يكون لديك معرفة مسبقة تشير إلى أن سمة معينة يجب أن يكون لها تأثير إيجابي (أو سلبي) بشكل عام على القيمة المستهدفة. على سبيل المثال، مع ثبات العوامل الأخرى، يجب أن يؤدي ارتفاع درجة الائتمان إلى زيادة احتمال الموافقة على القرض. تسمح القيود الاحادية الاتجاه بدمج مثل هذه المعرفة المسبقة في النموذج.</p>
<p>بالنسبة لمتنبئ :math: ‘F’ بميزتين:</p>
<ul>
<li><p>قيد <strong>الزيادة الأحادية الاتجاه</strong> هو قيد على الشكل:</p>
<div class="math notranslate nohighlight">
\[x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2)\]</div>
</li>
<li><p>قيد <strong>الانخفاض الأُحادي</strong> هو قيد على الشكل:</p>
<div class="math notranslate nohighlight">
\[x_1 \leq x_1' \implies F(x_1, x_2) \geq F(x_1', x_2)\]</div>
</li>
</ul>
<p>يمكنك تحديد قيد أحادي الاتجاه لكل ميزة باستخدام معلمة ‘monotonic_cst’. بالنسبة لكل ميزة، تشير القيمة 0 إلى عدم وجود قيد، في حين تشير القيمتان 1 و-1 على التوالي إلى قيد الزيادة الأحادية الاتجاه وقيد الانخفاض الأُحادي:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingRegressor</span>

<span class="go">... # قيد الزيادة الأحادية الاتجاه، والانخفاض الأُحادي، وعدم وجود قيد على الميزات الثلاث</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingRegressor</span><span class="p">(</span><span class="n">monotonic_cst</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>في سياق التصنيف الثنائي، يعني فرض قيد الزيادة الأحادية (الانخفاض الأُحادي) أن القيم الأعلى للميزة يفترض أن يكون لها تأثير إيجابي (سلبي) على احتمال انتماء العينات إلى الفئة الإيجابية.</p>
<p>ومع ذلك، فإن القيود الأحادية الاتجاه تقيد بشكل هامشي فقط تأثيرات الميزة على الإخراج. على سبيل المثال، لا يمكن استخدام قيود الزيادة والانخفاض الأحادية الاتجاه لفرض قيد النمذجة التالي:</p>
<div class="math notranslate nohighlight">
\[x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2')\]</div>
<p>كما أن القيود الأحادية الاتجاه غير مدعومة للتصنيف متعدد الفئات.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>نظرًا لأن الفئات هي كميات غير مرتبة، فمن غير الممكن فرض قيود أحادية الاتجاه على الميزات الفئوية.</p>
</div>
<p class="rubric">أمثلة</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_monotonic_constraints.html#sphx-glr-auto-examples-ensemble-plot-monotonic-constraints-py"><span class="std std-ref">Monotonic Constraints</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_hgbt_regression.html#sphx-glr-auto-examples-ensemble-plot-hgbt-regression-py"><span class="std std-ref">Features in Histogram Gradient Boosting Trees</span></a></p></li>
</ul>
</section>
<section id="interaction-cst-hgbt">
<span id="id8"></span><h1>قيود التفاعل<a class="headerlink" href="#interaction-cst-hgbt" title="Link to this heading">#</a></h1>
<p>بداهة، يُسمح لأشجار التدرج المُعزز بالهستوجرام باستخدام أي ميزة لتقسيم العقدة إلى عقد أطفال. وهذا يخلق ما يسمى بالتفاعلات بين الميزات، أي استخدام ميزات مختلفة كتقسيم على طول فرع. في بعض الأحيان، يرغب المرء في تقييد التفاعلات الممكنة، انظر [Mayer2022] _. يمكن القيام بذلك باستخدام معلمة “interaction_cst”، حيث يمكنك تحديد مؤشرات الميزات المسموح لها بالتفاعل.
على سبيل المثال، مع 3 ميزات في المجموع، “interaction_cst = [{0}، {1}، {2}]” يحظر جميع التفاعلات.
تُحدد القيود “[{0، 1}، {1، 2}]” مجموعتين من الميزات التي يمكن أن تتفاعل. قد تتفاعل الميزتان 0 و1 مع بعضهما البعض، وكذلك الميزتان 1 و2. ولكن لاحظ أنه لا يُسمح للميزتين 0 و2 بالتفاعل.
يوضح ما يلي شجرة والانقسامات الممكنة للشجرة:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>   1      &lt;- يمكن تطبيق كل من مجموعات القيود من الآن فصاعدًا
  / \
 1   2    &lt;- الانقسام الأيسر لا يزال يفي بمجموعات القيود كليهما.
/ \ / \      الانقسام الأيمن عند الميزة 2 له فقط المجموعة {1، 2} من الآن فصاعدًا.
</pre></div>
</div>
<p>يستخدم LightGBM نفس المنطق للمجموعات المتداخلة.</p>
<p>لاحظ أن الميزات غير المدرجة في “interaction_cst” يتم تلقائيًا تعيين مجموعة تفاعل لها. مع 3 ميزات مرة أخرى، وهذا يعني أن “[{0}]” مكافئ لـ “[{0}، {1، 2}]”</p>
<p class="rubric">أمثلة</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/inspection/plot_partial_dependence.html#sphx-glr-auto-examples-inspection-plot-partial-dependence-py"><span class="std std-ref">Partial Dependence and Individual Conditional Expectation Plots</span></a></p></li>
</ul>
<p class="rubric">مراجع</p>
<div role="list" class="citation-list">
<div class="citation" id="mayer2022" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Mayer2022<span class="fn-bracket">]</span></span>
<p>M. Mayer, S.C. Bourassa, M. Hoesli, and D.F. Scognamiglio.
2022. <a class="reference external" href="https://doi.org/10.3390/jrfm15050193">تطبيقات التعلم الآلي على تقييم الأراضي والمباني</a>.
Journal of Risk and Financial Management 15, no. 5: 193</p>
</div>
</div>
</section>
<section id="id9">
<h1>التوازي منخفض المستوى<a class="headerlink" href="#id9" title="Link to this heading">#</a></h1>
<p>:class:’HistGradientBoostingClassifier’ و
:class:’HistGradientBoostingRegressor’ يستخدم OpenMP
للتوازي من خلال Cython. لمزيد من التفاصيل حول كيفية التحكم في
عدد الخيوط، يرجى الرجوع إلى ملاحظاتنا: ref:<code class="docutils literal notranslate"><span class="pre">parallelism</span></code>.</p>
<p>الأجزاء التالية متوازية:</p>
<ul class="simple">
<li><p>رسم خريطة العينات من القيم الحقيقية إلى العلب ذات القيم الصحيحة (ومع ذلك، فإن العثور على عتبات العلب تسلسلي)</p></li>
<li><p>بناء الهستوجرامات متوازي عبر الميزات</p></li>
<li><p>العثور على أفضل نقطة انقسام في العقدة متوازية عبر الميزات</p></li>
<li><p>أثناء التثبيت، يتم رسم خريطة العينات إلى العقد اليسرى واليمنى متوازية عبر العينات</p></li>
<li><p>حسابات التدرج والهيسيان متوازية عبر العينات</p></li>
<li><p>التنبؤ متوازي عبر العينات</p></li>
</ul>
</section>
<section id="why-it-s-faster">
<span id="id10"></span><h1>لماذا هو أسرع<a class="headerlink" href="#why-it-s-faster" title="Link to this heading">#</a></h1>
<p>عنق الزجاجة في إجراء التعزيز التدريجي هو بناء أشجار القرار. يتطلب بناء شجرة قرار تقليدية (كما هو الحال في GBDTs الأخرى)
:class:’GradientBoostingClassifier’ و: class:’GradientBoostingRegressor’)
فرز العينات في كل عقدة (لكل ميزة). الفرز مطلوب حتى يمكن حساب المكاسب المحتملة لنقطة الانقسام بكفاءة. وبالتالي، فإن تقسيم عقدة واحدة له تعقيد
<span class="math notranslate nohighlight">\(\mathcal{O}(n_\text {features} \times n \log (n))\)</span> حيث <span class="math notranslate nohighlight">\(n\)</span>
هو عدد العينات في العقدة.</p>
<p>:class:’HistGradientBoostingClassifier’ و
:class:’HistGradientBoostingRegressor’، من ناحية أخرى، لا تتطلب فرز قيم الميزة وتستخدم بدلاً من ذلك بنية بيانات تسمى الهستوجرام، حيث يتم ترتيب العينات ضمنيًا.
للبناء الهستوجرام تعقيد <span class="math notranslate nohighlight">\(\mathcal{O}(n)\)</span>، لذا فإن إجراء تقسيم العقدة له تعقيد
<span class="math notranslate nohighlight">\(\mathcal{O}(n_\text{features} \times n)\)</span>، وهو أصغر بكثير من السابق. بالإضافة إلى ذلك، بدلاً من النظر في <span class="math notranslate nohighlight">\(n\)</span>
نقاط الانقسام، فإننا نأخذ في الاعتبار فقط “max_bins” نقاط الانقسام، والتي قد تكون أصغر بكثير.</p>
<p>من أجل بناء الهستوجرامات، يجب تصنيف بيانات الإدخال ‘X’ في علب ذات قيم صحيحة. يتطلب إجراء التصنيف هذا فرز قيم الميزة، ولكنه يحدث مرة واحدة فقط في بداية عملية التعزيز (وليس في كل عقدة، مثل
:class:’GradientBoostingClassifier’ و: class:’GradientBoostingRegressor’).</p>
<p>أخيرًا، يتم توازي العديد من أجزاء تنفيذ
:class:’HistGradientBoostingClassifier’ و
:class:’HistGradientBoostingRegressor’.</p>
<p class="rubric">مراجع</p>
<div role="list" class="citation-list">
<div class="citation" id="xgboost" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">XGBoost</a><span class="fn-bracket">]</span></span>
<p>Tianqi Chen, Carlos Guestrin, <a class="reference external" href="https://arxiv.org/abs/1603.02754">“XGBoost: A Scalable Tree
Boosting System”</a></p>
</div>
<div class="citation" id="lightgbm" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LightGBM<span class="fn-bracket">]</span></span>
<p>Ke et. al. <a class="reference external" href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree">“LightGBM: A Highly Efficient Gradient
BoostingDecision Tree”</a></p>
</div>
<div class="citation" id="fisher1958" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">Fisher1958</a><span class="fn-bracket">]</span></span>
<p>Fisher, W.D. (1958). <a class="reference external" href="http://csiss.ncgia.ucsb.edu/SPACE/workshops/2004/SAC/files/fisher.pdf">“On Grouping for Maximum Homogeneity”</a>
Journal of the American Statistical Association, 53, 789-798.</p>
</div>
</div>
<dl class="field-list simple">
<dt class="field-odd">class:’GradientBoostingClassifier’ و<span class="colon">:</span></dt>
<dd class="field-odd"><p>class:’GradientBoostingRegressor’</p>
</dd>
</dl>
<p>فيما يلي وصف لاستخدام ومعلمات <code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> و <code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>. أهم معلمتين في هاتين الأداتين التقديريتين هما <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> و <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" id="التصنيف">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">التصنيف<a class="headerlink" href="#التصنيف" title="Link to this dropdown">#</a></span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">تدعم <code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> التصنيف الثنائي والمتعدد الفئات.
يوضح المثال التالي كيفية ملاءمة مصنف التعزيز التدريجي
باستخدام 100 من أشجار القرار الضعيفة كمتعلمين ضعفاء:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_hastie_10_2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.913...</span>
</pre></div>
</div>
<p class="sd-card-text">يتم التحكم في عدد المتعلمين الضعفاء (أي أشجار القرار) بواسطة معلمة
<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>؛ يمكن التحكم في حجم كل شجرة إما عن طريق ضبط عمق الشجرة
باستخدام <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> أو عن طريق ضبط عدد العقد الورقية باستخدام
<code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code>. <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> هو معلمة فائقة القيمة في النطاق
(0.0، 1.0] تتحكم في الإفراط في التكيّف عبر <span class="xref std std-ref">shrinkage</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p class="sd-card-text">يتطلب التصنيف بأكثر من فئتين استقراء <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> من أشجار القرار في كل تكرار،
وبالتالي، فإن العدد الإجمالي للأشجار المستقرأة يساوي
<code class="docutils literal notranslate"><span class="pre">n_classes</span> <span class="pre">*</span> <span class="pre">n_estimators</span></code>. بالنسبة لمجموعات البيانات التي تحتوي على عدد كبير
من الفئات، نوصي بشدة باستخدام
<code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code> كبديل لـ
<code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code>.</p>
</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3" id="الانحدار">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">الانحدار<a class="headerlink" href="#الانحدار" title="Link to this dropdown">#</a></span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">تدعم <code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code> عددًا من
<span class="xref std std-ref">وظائف الخسارة المختلفة</span>
للانحدار والتي يمكن تحديدها من خلال حجة
<code class="docutils literal notranslate"><span class="pre">loss</span></code>؛ ووظيفة الخسارة الافتراضية للانحدار هي الخطأ التربيعي
(<code class="docutils literal notranslate"><span class="pre">'squared_error'</span></code>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">200</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">200</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">200</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">200</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;squared_error&#39;</span>
<span class="gp">... </span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="go">5.00...</span>
</pre></div>
</div>
<p class="sd-card-text">يوضح الشكل أدناه نتائج تطبيق <code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code>
مع خسارة المربعات الصغرى و500 متعلم أساسي على مجموعة بيانات مرض السكري
(<a class="reference internal" href="../datasets/toy_dataset.html#sklearn.datasets.load_diabetes" title="sklearn.datasets.load_diabetes"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.datasets.load_diabetes</span></code></a>).
يوضح الرسم الخطي خطأ التدريب والاختبار في كل تكرار.
يتم تخزين خطأ التدريب في كل تكرار في الخاصية
<code class="docutils literal notranslate"><span class="pre">train_score_</span></code> لنموذج التعزيز التدريجي.
يمكن الحصول على خطأ الاختبار في كل تكرار
عبر طريقة <code class="xref py py-meth docutils literal notranslate"><span class="pre">staged_predict</span></code> التي تعيد مولدًا
يقوم بتقديم التنبؤات في كل مرحلة. يمكن استخدام الرسوم البيانية مثل هذه لتحديد
العدد الأمثل للأشجار (أي <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>) عن طريق التوقف المبكر.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html"><img alt="../_images/sphx_glr_plot_gradient_boosting_regression_001.png" src="../_images/sphx_glr_plot_gradient_boosting_regression_001.png" style="width: 450.0px; height: 450.0px;" />
</a>
</figure>
</div>
</details><p class="rubric">الأمثلة</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="std std-ref">Gradient Boosting regression</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_oob.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-oob-py"><span class="std std-ref">Gradient Boosting Out-of-Bag estimates</span></a></p></li>
</ul>
</section>
<section id="gradient-boosting-warm-start">
<span id="id11"></span><h1>ملاءمة متعلمين ضعفاء إضافيين<a class="headerlink" href="#gradient-boosting-warm-start" title="Link to this heading">#</a></h1>
<p>يدعم كل من <code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code> و <code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code>
خاصية <code class="docutils literal notranslate"><span class="pre">warm_start=True</span></code> التي تسمح بإضافة المزيد من المتعلمين إلى نموذج تم ملاءمته بالفعل.</p>
</section>
<section id="id12">
<h1>تحكم حجم الشجرة<a class="headerlink" href="#id12" title="Link to this heading">#</a></h1>
<p>يحدد حجم شجرة الانحدار الأساسي مستوى تفاعلات المتغيرات التي يمكن أن يلتقطها نموذج Gradient Boosting. بشكل عام، يمكن لشجرة ذات عمق “h” أن تلتقط تفاعلات من الرتبة “h”.</p>
<p>هناك طريقتان يمكن من خلالهما التحكم في حجم شجرة الانحدار الفردية.</p>
<p>إذا قمت بتحديد “max_depth=h”، فسيتم إنشاء أشجار ثنائية كاملة بعمق “h”. سيكون لهذه الأشجار (على الأكثر) “2**h” من العقد الورقية و “2**h - 1” من العقد الانقسامية.</p>
<p>أو يمكنك التحكم في حجم الشجرة عن طريق تحديد عدد العقد الورقية باستخدام المعلمة “max_leaf_nodes”. في هذه الحالة، سيتم إنشاء الأشجار باستخدام البحث الأفضل أولاً حيث يتم توسيع العقد التي لها أعلى تحسن في النقاء أولاً.</p>
<p>الشجرة التي تحتوي على “max_leaf_nodes=k” لها “k - 1” من العقد الانقسامية وبالتالي يمكنها نمذجة التفاعلات حتى الرتبة “max_leaf_nodes - 1”.</p>
<p>وجدنا أن “max_leaf_nodes=k” يعطي نتائج مماثلة لـ “max_depth=k-1”، ولكنه أسرع بكثير في التدريب على حساب خطأ تدريب أعلى قليلاً.</p>
<p>ترتبط معلمة “max_leaf_nodes” بالمتغير “J” في الفصل الخاص بـ Gradient Boosting في [Friedman2001] _ وهي مرتبطة بمعلمة “interaction.depth” في حزمة gbm في R حيث “max_leaf_nodes == interaction.depth + 1”.</p>
</section>
<section id="id13">
<h1>الصيغة الرياضية<a class="headerlink" href="#id13" title="Link to this heading">#</a></h1>
<p>نقدم أولاً GBRT للانحدار، ثم نتناول حالة التصنيف بالتفصيل.</p>
<p>انحدار</p>
<p>تعد نماذج GBRT للانحدار نماذجًا إضافية يكون توقعها <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> لإدخال معين <span class="math notranslate nohighlight">\(x_i\)</span> على الشكل التالي:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_i = F_M(x_i) = \sum_{m=1}^{M} h_m(x_i)\]</div>
<p>حيث <span class="math notranslate nohighlight">\(h_m\)</span> هي تقديرات تسمى <em>weak learners</em> في سياق التعزيز. يستخدم Gradient Tree Boosting :ref: <code class="docutils literal notranslate"><span class="pre">decision</span> <span class="pre">tree</span> <span class="pre">regressors</span> <span class="pre">&lt;tree&gt;</span></code> ذات الحجم الثابت كـ weak learners. يرمز الثابت M إلى معلمة <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>.</p>
<p>مثل خوارزميات التعزيز الأخرى، يتم بناء GBRT بطريقة جشعة:</p>
<div class="math notranslate nohighlight">
\[F_m(x) = F_{m-1}(x) + h_m(x),\]</div>
<p>حيث يتم تركيب الشجرة المضافة حديثًا <span class="math notranslate nohighlight">\(h_m\)</span> لتقليل مجموع الخسائر <span class="math notranslate nohighlight">\(L_m\)</span>، بالنظر إلى المجموعة السابقة <span class="math notranslate nohighlight">\(F_{m-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[h_m = \arg\min_{h} L_m = \arg\min_{h} \sum_{i=1}^{n}
l(y_i, F_{m-1}(x_i) + h(x_i)),\]</div>
<p>حيث <span class="math notranslate nohighlight">\(l(y_i, F(x_i))\)</span> تحددها معلمة <code class="docutils literal notranslate"><span class="pre">loss</span></code>، والتي يتم تفصيلها في القسم التالي.</p>
<p>بشكل افتراضي، يتم اختيار النموذج الأولي <span class="math notranslate nohighlight">\(F_{0}\)</span> على أنه الثابت الذي يقلل الخسارة: بالنسبة لخسارة المربعات الصغرى، يكون هذا هو المتوسط الحسابي لقيم الهدف. يمكن أيضًا تحديد النموذج الأولي عبر حجة “init”.</p>
<p>باستخدام تقريب تايلور من الدرجة الأولى، يمكن تقريب قيمة <span class="math notranslate nohighlight">\(l\)</span> على النحو التالي:</p>
<div class="math notranslate nohighlight">
\[l(y_i, F_{m-1}(x_i) + h_m(x_i)) \approx
l(y_i, F_{m-1}(x_i))
+ h_m(x_i)
\left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m - 1}}.\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>باختصار، يقول تقريب تايلور من الدرجة الأولى أن <span class="math notranslate nohighlight">\(l(z) \approx l(a) + (z - a) \frac{\partial l}{\partial z}(a)\)</span>. هنا، <span class="math notranslate nohighlight">\(z\)</span> يقابل <span class="math notranslate nohighlight">\(F_{m - 1}(x_i) + h_m(x_i)\)</span>، و:math:<code class="docutils literal notranslate"><span class="pre">a</span></code> يقابل <span class="math notranslate nohighlight">\(F_{m-1}(x_i)\)</span>.</p>
</div>
<p>الكمية <span class="math notranslate nohighlight">\(\left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m - 1}}\)</span> هي مشتقة الخسارة فيما يتعلق بمعلمتها الثانية، المقدرة عند <span class="math notranslate nohighlight">\(F_{m-1}(x)\)</span>. من السهل حسابها لأي <span class="math notranslate nohighlight">\(F_{m - 1}(x_i)\)</span> في شكل مغلق منذ قابلية الخسارة للاشتقاق. سنرمزها بـ <span class="math notranslate nohighlight">\(g_i\)</span>.</p>
<p>بعد إزالة المصطلحات الثابتة، نحصل على:</p>
<div class="math notranslate nohighlight">
\[h_m \approx \arg\min_{h} \sum_{i=1}^{n} h(x_i) g_i\]</div>
<p>هذا الحد الأدنى إذا تم تركيب <span class="math notranslate nohighlight">\(h(x_i)\)</span> للتنبؤ بقيمة تتناسب مع التدرج السلبي <span class="math notranslate nohighlight">\(-g_i\)</span>. لذلك، في كل تكرار، يتم تركيب <strong>المقدر</strong> <span class="math notranslate nohighlight">\(h_m\)</span> <strong>للتنبؤ بالتدرجات السلبية للعينات</strong>. يتم تحديث التدرجات في كل تكرار. يمكن اعتبار هذا شكلًا من أشكال الانحدار التدريجي في مساحة الدالة.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>بالنسبة لبعض الخسائر، على سبيل المثال “absolute_error” حيث تكون التدرجات <span class="math notranslate nohighlight">\(\pm 1\)</span>، فإن القيم التي يتنبأ بها <span class="math notranslate nohighlight">\(h_m\)</span> المجهزة ليست دقيقة بدرجة كافية: يمكن للشجرة أن تخرج فقط قيمًا صحيحة. ونتيجة لذلك، يتم تعديل قيم الأوراق للشجرة <span class="math notranslate nohighlight">\(h_m\)</span> بمجرد تركيب الشجرة، بحيث تقلل قيم الأوراق من الخسارة <span class="math notranslate nohighlight">\(L_m\)</span>. يعتمد التحديث على الخسارة: بالنسبة لخسارة الخطأ المطلق، يتم تحديث قيمة الورقة إلى الوسيط للعينات في تلك الورقة.</p>
</div>
<p>تصنيف</p>
<p>يتشابه التعزيز التدريجي للتصنيف بشكل كبير مع حالة الانحدار. ومع ذلك، فإن مجموع الأشجار <span class="math notranslate nohighlight">\(F_M(x_i) = \sum_m h_m(x_i)\)</span> ليس متجانسًا مع التنبؤ: لا يمكن أن يكون فئة، نظرًا لأن الأشجار تتنبأ بقيم مستمرة.</p>
<p>يعتمد رسم الخريطة من القيمة <span class="math notranslate nohighlight">\(F_M(x_i)\)</span> إلى فئة أو احتمال على الخسارة. بالنسبة إلى الخسارة اللوغاريتمية، يتم نمذجة احتمال انتماء <span class="math notranslate nohighlight">\(x_i\)</span> إلى الفئة الإيجابية على النحو التالي <span class="math notranslate nohighlight">\(p(y_i = 1 | x_i) = \sigma(F_M(x_i))\)</span> حيث <span class="math notranslate nohighlight">\(\sigma\)</span> هي دالة التغليف أو دالة التغليف.</p>
<p>بالنسبة للتصنيف متعدد الفئات، يتم بناء K شجرة (لفئات K) في كل واحدة من <span class="math notranslate nohighlight">\(M\)</span> تكرارات. يتم نمذجة احتمال انتماء <span class="math notranslate nohighlight">\(x_i\)</span> إلى الفئة k كدالة softmax للقيم <span class="math notranslate nohighlight">\(F_{M,k}(x_i)\)</span>.</p>
<p>لاحظ أنه حتى لمهمة التصنيف، فإن <span class="math notranslate nohighlight">\(h_m\)</span> sub-estimator هو لا يزال مقدر الانحدار، وليس مصنف. ويرجع ذلك إلى أن sub-estimators يتم تدريبها للتنبؤ بالتدرجات (السلبية)، والتي تكون دائمًا كميات مستمرة.</p>
</section>
<section id="id14">
<h1>وظائف الخسارة<a class="headerlink" href="#id14" title="Link to this heading">#</a></h1>
<p>تتم دعم وظائف الخسارة التالية ويمكن تحديدها باستخدام معلمة “loss”:</p>
<p>انحدار</p>
<ul class="simple">
<li><p>خطأ تربيعي (<code class="docutils literal notranslate"><span class="pre">'squared_error'</span></code>): الخيار الطبيعي للانحدار بسبب خصائصه الحسابية المتفوقة. يتم إعطاء النموذج الأولي بمتوسط قيم الهدف.</p></li>
<li><p>خطأ مطلق (<code class="docutils literal notranslate"><span class="pre">'absolute_error'</span></code>): دالة خسارة قوية للانحدار. يتم إعطاء النموذج الأولي بواسطة الوسيط لقيم الهدف.</p></li>
<li><p>هابر (<code class="docutils literal notranslate"><span class="pre">'huber'</span></code>): دالة خسارة قوية أخرى تجمع بين المربعات الصغرى والانحرافات المطلقة الصغرى؛ استخدم “alpha” للتحكم في الحساسية فيما يتعلق بالقيم الشاذة (راجع [Friedman2001] _ لمزيد من التفاصيل).</p></li>
<li><p>الكمية (<code class="docutils literal notranslate"><span class="pre">'quantile'</span></code>): دالة خسارة للانحدار الكمي. استخدم “0 &lt; alpha &lt; 1” لتحديد الكمية. يمكن استخدام دالة الخسارة هذه لإنشاء فترات تنبؤ (راجع :ref: <code class="docutils literal notranslate"><span class="pre">sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py</span></code>).</p></li>
</ul>
<p>تصنيف</p>
<ul class="simple">
<li><p>الخسارة اللوغاريتمية الثنائية (<code class="docutils literal notranslate"><span class="pre">'log-loss'</span></code>): دالة الخسارة اللوغاريتمية السالبة للاحتمالية الثنائية للتصنيف. يوفر تقديرات الاحتمالية. يتم إعطاء النموذج الأولي بواسطة نسبة الاحتمالات.</p></li>
<li><p>الخسارة اللوغاريتمية متعددة الفئات (<code class="docutils literal notranslate"><span class="pre">'log-loss'</span></code>): دالة الخسارة اللوغاريتمية السالبة للاحتمالية متعددة الفئات للتصنيف مع <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> فئات حصرية. يوفر تقديرات الاحتمالية. يتم إعطاء النموذج الأولي بواسطة الاحتمالية السابقة لكل فئة. في كل تكرار، يجب بناء <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> من أشجار الانحدار، مما يجعل GBRT غير فعال إلى حد ما لمجموعات البيانات التي تحتوي على عدد كبير من الفئات.</p></li>
<li><p>الخسارة الأسية (<code class="docutils literal notranslate"><span class="pre">'exponential'</span></code>): نفس دالة الخسارة مثل :class: <code class="docutils literal notranslate"><span class="pre">AdaBoostClassifier</span></code>. أقل قوة من <code class="docutils literal notranslate"><span class="pre">'log-loss'</span></code>؛ يمكن استخدامه فقط للتصنيف الثنائي.</p></li>
</ul>
</section>
<section id="id15">
<h1>التقلص عبر معدل التعلم<a class="headerlink" href="#id15" title="Link to this heading">#</a></h1>
<p>اقترح [Friedman2001] _ استراتيجية تنظيم بسيطة تقوم بضبط مساهمة كل weak learner بمعامل ثابت <span class="math notranslate nohighlight">\(\nu\)</span>:</p>
<div class="math notranslate nohighlight">
\[F_m(x) = F_{m-1}(x) + \nu h_m(x)\]</div>
<p>يُطلق على المعلمة <span class="math notranslate nohighlight">\(\nu\)</span> أيضًا اسم <strong>معدل التعلم</strong> لأنها تحدد حجم خطوة إجراء الانحدار التدريجي؛ يمكن تعيينه عبر معلمة “learning_rate”.</p>
<p>تتفاعل معلمة “learning_rate” بشكل كبير مع معلمة “n_estimators”، والتي هي عدد weak learners التي سيتم تركيبها. تتطلب قيم “learning_rate” الأصغر أعدادًا أكبر من weak learners للحفاظ على خطأ التدريب الثابت. تشير الأدلة التجريبية إلى أن قيم “learning_rate” الصغيرة تفضل خطأ الاختبار الأفضل. [HTF] _ يوصي بتعيين معدل التعلم إلى ثابت صغير (على سبيل المثال “learning_rate &lt;= 0.1”) واختيار “n_estimators” كبير بما يكفي للتوقف المبكر، راجع :ref: <code class="docutils literal notranslate"><span class="pre">sphx_glr_auto_examples_ensemble_plot_gradient_boosting_early_stopping.py</span></code> لمزيد من المناقشة حول التفاعل بين “learning_rate” و “n_estimators” راجع [R2007] _.</p>
</section>
<section id="id16">
<h1>الاستعانة بعينة جزئية<a class="headerlink" href="#id16" title="Link to this heading">#</a></h1>
<p>اقترح [Friedman2002] _ التعزيز التدريجي العشوائي، والذي يجمع بين التعزيز التدريجي والمتوسط التجميعي (bagging). في كل تكرار، يتم تدريب المصنف الأساسي على جزء “subsample” من بيانات التدريب المتاحة. يتم رسم العينة العشوائية بدون استبدال.</p>
<p>القيمة النموذجية لـ “subsample” هي 0.5.</p>
<p>يوضح الشكل أدناه تأثير التقلص والاستعانة بعينة جزئية على دقة النموذج. يمكننا أن نرى بوضوح أن التقلص يتفوق على عدم التقلص. يمكن أن تزيد الاستعانة بعينة جزئية مع التقلص من دقة النموذج. من ناحية أخرى، فإن الاستعانة بعينة جزئية بدون تقلص لا تؤدي أداءً جيدًا.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_gradient_boosting_regularization.html"><img alt="../_images/sphx_glr_plot_gradient_boosting_regularization_001.png" src="../_images/sphx_glr_plot_gradient_boosting_regularization_001.png" style="width: 480.0px; height: 360.0px;" />
</a>
</figure>
<p>تتمثل إحدى الاستراتيجيات الأخرى لتقليل التباين في الاستعانة بعينة جزئية للميزات على غرار الانقسامات العشوائية في :class: <code class="docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code>. يمكن التحكم في عدد الميزات المستعان بعينة جزئية منها من خلال معلمة “max_features”.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>يمكن أن يؤدي استخدام قيمة “max_features” صغيرة إلى تقليل وقت التشغيل بشكل كبير.</p>
</div>
<p>يتيح التعزيز التدريجي العشوائي حساب تقديرات خارج الكيس لمقاييس الاختبار عن طريق حساب التحسن في مقياس الاختبار على الأمثلة غير المدرجة في عينة الإقلاع (أي الأمثلة خارج الكيس). يتم تخزين التحسينات في السمة <code class="docutils literal notranslate"><span class="pre">oob_improvement_</span></code>. يحتوي “oob_improvement_[i]” على التحسن من حيث الخسارة على عينات OOB إذا أضفت مرحلة i-th إلى التوقعات الحالية. يمكن استخدام تقديرات خارج الكيس لاختيار النموذج، على سبيل المثال لتحديد العدد الأمثل من التكرارات. تقديرات خارج الكيس متشائمة للغاية، لذلك نوصي باستخدام التحقق من صحة الاستعانة بعينة جزئية بدلاً من ذلك، واستخدام خارج الكيس فقط إذا كان التحقق من صحة الاستعانة بعينة جزئية يستغرق وقتًا طويلاً للغاية.</p>
<p class="rubric">أمثلة</p>
<ul class="simple">
<li><dl class="field-list simple">
<dt class="field-odd">ref<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regularization.py</span></code></p>
</dd>
</dl>
</li>
<li><dl class="field-list simple">
<dt class="field-odd">ref<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">sphx_glr_auto_examples_ensemble_plot_gradient_boosting_oob.py</span></code></p>
</dd>
</dl>
</li>
<li><dl class="field-list simple">
<dt class="field-odd">ref<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">sphx_glr_auto_examples_ensemble_plot_ensemble_oob.py</span></code></p>
</dd>
</dl>
</li>
</ul>
</section>
<section id="id17">
<h1>التفسير بأهمية الميزة<a class="headerlink" href="#id17" title="Link to this heading">#</a></h1>
<p>يمكن تفسير شجرة القرار الفردية بسهولة عن طريق تصور بنية الشجرة. ومع ذلك، فإن نماذج التعزيز التدريجي تتكون من مئات أشجار الانحدار، وبالتالي لا يمكن تفسيرها بسهولة عن طريق الفحص البصري للأشجار الفردية. لحسن الحظ، تم اقتراح عدد من التقنيات لتلخيص نماذج التعزيز التدريجي وتفسيرها.
في كثير من الأحيان، لا تساهم الخصائص بشكل متساوٍ في التنبؤ بالاستجابة المستهدفة؛ وفي العديد من الحالات، تكون أغلب الخصائص غير ذات صلة في الواقع.</p>
<p>عند تفسير نموذج ما، يكون السؤال الأول عادةً هو: ما هي تلك الخصائص المهمة، وكيف تساهم في التنبؤ بالاستجابة المستهدفة؟</p>
<p>أشجار القرار الفردية تقوم بشكل جوهري باختيار الخصائص عن طريق اختيار نقاط الانقسام المناسبة. يمكن استخدام هذه المعلومات لقياس أهمية كل خاصية؛ والفكرة الأساسية هي: كلما تم استخدام خاصية ما بشكل متكرر في نقاط انقسام الشجرة، كلما زادت أهمية تلك الخاصية. يمكن توسيع هذا المفهوم للأهمية ليشمل مجموعات أشجار القرار ببساطة عن طريق حساب المتوسط لأهمية كل خاصية بناءً على عدم النقاء (انعدام الصفاء) في كل شجرة (راجع <span class="xref std std-ref">random_forest_feature_importance</span> لمزيد من التفاصيل).</p>
<p>يمكن الوصول إلى درجات أهمية الخصائص لنموذج Gradient Boosting المناسب عن طريق خاصية <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_hastie_10_2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="go">array([0.10..., 0.10..., 0.11..., ...</span>
</pre></div>
</div>
<p>لاحظ أن حساب أهمية الخصائص هذا يعتمد على الإنتروبيا، وهو مختلف عن <code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance</span></code> الذي يعتمد على تبديل الخصائص.</p>
<p class="rubric">أمثلة</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="std std-ref">Gradient Boosting regression</span></a></p></li>
</ul>
<p class="rubric">مراجع</p>
<div role="list" class="citation-list">
<div class="citation" id="friedman2001" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Friedman2001<span class="fn-bracket">]</span></span>
<p>Friedman, J.H. (2001). <a class="reference external" href="https://doi.org/10.1214/aos/1013203451">Greedy function approximation: A gradient
boosting machine</a>.
Annals of Statistics, 29, 1189-1232.</p>
</div>
<div class="citation" id="friedman2002" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Friedman2002<span class="fn-bracket">]</span></span>
<p>Friedman, J.H. (2002). <a class="reference external" href="https://statweb.stanford.edu/~jhf/ftp/stobst.pdf">Stochastic gradient boosting.</a>.
Computational Statistics &amp; Data Analysis, 38, 367-378.</p>
</div>
<div class="citation" id="r2007" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>R2007<span class="fn-bracket">]</span></span>
<p>G. Ridgeway (2006). <a class="reference external" href="https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf">Generalized Boosted Models: A guide to the gbm
package</a></p>
</div>
</div>
<p id="forest">Random forests and other randomized tree ensembles
يحتوي نمط sklearn.ensemble على خوارزميتي متوسط بناءً على شجرة القرار المعشاة: خوارزمية RandomForest وطريقة Extra-Trees. كلتا الخوارزميتين هما من تقنيات Perturb-and-Combine المصممة خصيصًا للشجر. وهذا يعني أنه يتم إنشاء مجموعة متنوعة من المصنفات عن طريق إدخال العشوائية في بناء المصنف. يتم إعطاء تنبؤ المجموعة كمتوسط للتنبؤات من المصنفات الفردية.</p>
<p>مثل المصنفات الأخرى، يجب ضبط مصنفات الغابة باستخدام صفيفين: صفيف X متفرق أو كثيف الشكل (n_samples، n_features) يحتوي على عينات التدريب، وصفيف Y من الشكل (n_samples،) يحتوي على قيم الهدف (ملصقات الفئات) لعينات التدريب:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>مثل شجرة القرار، تمتد غابات الأشجار أيضًا إلى مشكلات الإخراج المتعدد (إذا كان Y عبارة عن صفيف من الشكل (n_samples، n_outputs)).</p>
<section id="id18">
<h2>غابات عشوائية<a class="headerlink" href="#id18" title="Link to this heading">#</a></h2>
<p>في الغابات العشوائية (راجع فئات RandomForestClassifier وRandomForestRegressor)، يتم بناء كل شجرة في المجموعة من عينة مستخلصة مع الاستبدال (أي عينة الإقلاع) من مجموعة التدريب.</p>
<p>علاوة على ذلك، عند تقسيم كل عقدة أثناء بناء الشجرة، يتم العثور على أفضل تقسيم من خلال البحث المستنفد لقيم ميزات جميع الميزات المدخلة أو مجموعة فرعية عشوائية من حجم “max_features”. (راجع إرشادات ضبط المعلمات لمزيد من التفاصيل).</p>
<p>الغرض من هذين المصدرين للعشوائية هو تقليل تغايرية مؤشر الغابة. في الواقع، عادة ما تظهر أشجار القرار الفردية تغايرية عالية وتميل إلى الإفراط في التكيّف. تؤدي العشوائية التي يتم حقنها في الغابات إلى أشجار قرار ذات أخطاء تنبؤ غير مترابطة إلى حد ما. عن طريق حساب متوسط هذه التنبؤات، يمكن أن تلغي بعض الأخطاء. تحقق الغابات العشوائية تقليل التباين عن طريق دمج الأشجار المتنوعة، أحيانًا على حساب زيادة طفيفة في الانحياز. في الممارسة العملية، يكون تقليل التباين كبيرًا في كثير من الأحيان، مما يؤدي إلى نموذج أفضل بشكل عام.</p>
<p>على عكس المنشور الأصلي <a class="reference internal" href="#b2001" id="id19"><span>[B2001]</span></a>، يجمع تنفيذ Scikit-learn المصنفات عن طريق حساب المتوسط التنبؤات الاحتمالية الخاصة بها، بدلاً من السماح لكل مصنف بالتصويت لفئة واحدة.</p>
<p>بديل تنافسي للغابات العشوائية هو نموذج Histogram-based Gradient Boosting (HGBT):</p>
<ul class="simple">
<li><p>بناء الأشجار: تعتمد الغابات العشوائية عادةً على الأشجار العميقة (التي تفرط في التكيف بشكل فردي) والتي تستخدم الكثير من الموارد الحسابية، حيث تتطلب العديد من الانقسامات وتقييمات الانقسامات المرشحة. تقوم نماذج التعزيز ببناء أشجار ضحلة (التي لا تتناسب بشكل فردي) والتي تكون أسرع في التجهيز والتنبؤ.</p></li>
<li><p>التعزيز التسلسلي: في HGBT، يتم بناء أشجار القرار بشكل تسلسلي، حيث يتم تدريب كل شجرة لتصحيح الأخطاء التي ارتكبتها الأشجار السابقة. يسمح لهم هذا بتحسين أداء النموذج بشكل تكراري باستخدام عدد قليل نسبيًا من الأشجار. على النقيض من ذلك، تستخدم الغابات العشوائية تصويت الأغلبية للتنبؤ بالنتيجة، والذي قد يتطلب عددًا أكبر من الأشجار لتحقيق نفس مستوى الدقة.</p></li>
<li><p>التصنيف الفعال: يستخدم HGBT خوارزمية تصنيف فعالة يمكنها التعامل مع مجموعات البيانات الكبيرة ذات العدد الكبير من الميزات. يمكن لخوارزمية التصنيف معالجة البيانات مسبقًا لتسريع بناء الشجرة لاحقًا (راجع لماذا يكون أسرع للحصول على مزيد من التفاصيل). على النقيض من ذلك، لا يستخدم تنفيذ Scikit-learn للغابات العشوائية التصنيف ويعتمد على الانقسام الدقيق، والذي يمكن أن يكون مكلفًا حسابياً.</p></li>
</ul>
<p>بشكل عام، تتوقف التكلفة الحسابية لـ HGBT مقابل RF على الخصائص المحددة لمجموعة البيانات ومهمة النمذجة. من الجيد تجربة كلا النموذجين ومقارنة أدائهما وكفاءتهما الحسابية على مشكلتك المحددة لتحديد النموذج الأنسب.</p>
</section>
<section id="id20">
<h2>أمثلة<a class="headerlink" href="#id20" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py</p></li>
</ul>
</section>
<section id="id21">
<h2>الأشجار العشوائية للغاية<a class="headerlink" href="#id21" title="Link to this heading">#</a></h2>
<p>في الأشجار العشوائية للغاية (راجع فئات ExtraTreesClassifier وExtraTreesRegressor)، تذهب العشوائية خطوة أخرى إلى الأمام في طريقة حساب الانقسامات. كما هو الحال في الغابات العشوائية، يتم استخدام مجموعة فرعية عشوائية من الميزات المرشحة، ولكن بدلاً من البحث عن عتبات الأكثر تمييزًا، يتم رسم العتبات بشكل عشوائي لكل ميزة مرشحة ويتم اختيار أفضل هذه العتبات العشوائية كقاعدة للتقسيم. عادة ما يسمح هذا بخفض تغايرية النموذج قليلاً، على حساب زيادة طفيفة في الانحياز:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="go">0.98...</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="go">0.999...</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.999</span>
<span class="go">True</span>
</pre></div>
</div>
</section>
<section id="id22">
<h2>المعلمات<a class="headerlink" href="#id22" title="Link to this heading">#</a></h2>
<p>المعلمات الرئيسية التي يجب ضبطها عند استخدام هذه الطرق هي “n_estimators” و”max_features”. الأول هو عدد الأشجار في الغابة. كلما كان أكبر، كان ذلك أفضل، ولكن أيضًا سيستغرق وقتًا أطول لحساب. بالإضافة إلى ذلك، لاحظ أن النتائج لن تتحسن بشكل كبير بعد عدد حرج من الأشجار. الأخير هو حجم المجموعات الفرعية العشوائية من الميزات للنظر فيها عند تقسيم عقدة. كلما انخفضت، زاد تقليل التباين، ولكن أيضًا زادت الزيادة في الانحياز. القيم الافتراضية الجيدة هي “max_features=1.0” أو ما يعادلها “max_features=None” (النظر دائمًا في جميع الميزات بدلاً من مجموعة فرعية عشوائية) لمشكلات الانحدار، و”max_features=”sqrt” (باستخدام مجموعة فرعية عشوائية من الحجم “sqrt(n_features)”) لمهمات التصنيف (حيث “n_features” هو عدد الميزات في البيانات). القيمة الافتراضية “max_features=1.0” تعادل الأشجار المعبأة ويمكن تحقيق المزيد من العشوائية عن طريق تعيين قيم أصغر (على سبيل المثال، 0.3 هو افتراضي نموذجي في الأدبيات). يتم تحقيق نتائج جيدة غالبًا عند تعيين “max_depth=None” بالاشتراك مع “min_samples_split=2” (أي عند تطوير الأشجار بالكامل). ضع في اعتبارك أنه قد لا تكون هذه القيم مثالية، وقد تؤدي إلى نماذج تستهلك الكثير من ذاكرة الوصول العشوائي. يجب دائمًا التحقق من صحة أفضل قيم المعلمات. بالإضافة إلى ذلك، لاحظ أنه في الغابات العشوائية، يتم استخدام عينات الإقلاع بشكل افتراضي (bootstrap=True) في حين أن الاستراتيجية الافتراضية للأشجار الإضافية هي استخدام مجموعة البيانات بأكملها (bootstrap=False). عند استخدام عينات الإقلاع، يمكن تقدير خطأ التعميم على العينات المتروكة أو العينات الخارجة عن الحزمة. يمكن تمكين هذا عن طريق تعيين “oob_score=True”.</p>
<p>ملاحظة</p>
<p>حجم النموذج بالمعلمات الافتراضية هو: O (M * N * log (N))، حيث M هو عدد الأشجار وN هو عدد العينات. لتقليل حجم النموذج، يمكنك تغيير هذه المعلمات: “min_samples_split”، “max_leaf_nodes”، “max_depth” و”min_samples_leaf”.</p>
</section>
<section id="id23">
<h2>التوازي<a class="headerlink" href="#id23" title="Link to this heading">#</a></h2>
<p>أخيرًا، تتميز هذه الوحدة أيضًا بالبناء الموازي للأشجار والحساب الموازي للتنبؤات من خلال معلمة “n_jobs”. إذا كان n_jobs=k، فسيتم تقسيم الحسابات إلى k وظائف، ويتم تشغيلها على k نواة من الآلة. إذا كان n_jobs=-1، فسيتم استخدام جميع النوى المتوفرة على الآلة. لاحظ أنه بسبب النفقات العامة للاتصال بين العمليات، فقد لا يكون التسريع خطيًا (أي أن استخدام k وظائف لن يكون للأسف أسرع k مرة). لا يزال من الممكن تحقيق تسريع كبير عند بناء عدد كبير من الأشجار، أو عند بناء شجرة واحدة تتطلب قدرًا عادلًا من الوقت (على سبيل المثال، على مجموعات البيانات الكبيرة).</p>
</section>
<section id="id24">
<h2>أمثلة<a class="headerlink" href="#id24" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>sphx_glr_auto_examples_ensemble_plot_forest_iris.py</p></li>
<li><p>sphx_glr_auto_examples_ensemble_plot_forest_importances_faces.py</p></li>
<li><p>sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py</p></li>
</ul>
</section>
<section id="id25">
<h2>مراجع<a class="headerlink" href="#id25" title="Link to this heading">#</a></h2>
<div role="list" class="citation-list">
<div class="citation" id="b2001" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">B2001</a><span class="fn-bracket">]</span></span>
<ol class="upperalpha simple" start="12">
<li><p>Breiman، “Random Forests”، Machine Learning، 45(1)، 5-32، 2001.</p></li>
</ol>
</div>
<div class="citation" id="b1998" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>B1998<span class="fn-bracket">]</span></span>
<ol class="upperalpha simple" start="12">
<li><p>Breiman، “Arcing Classifiers”، Annals of Statistics 1998.</p></li>
</ol>
</div>
</div>
<ul class="simple">
<li><ol class="upperalpha simple" start="16">
<li><p>Geurts، D. Ernst.، وL. Wehenkel، “الأشجار العشوائية للغاية”، Machine Learning، 63(1)، 3-42، 2006.</p></li>
</ol>
</li>
</ul>
</section>
<section id="id26">
<h2>تقييم أهمية الميزة<a class="headerlink" href="#id26" title="Link to this heading">#</a></h2>
<p>يمكن استخدام المرتبة النسبية (أي العمق) لميزة مستخدمة كعقدة قرار في شجرة لتقييم الأهمية النسبية لتلك الميزة فيما يتعلق بإمكانية التنبؤ بالمتغير المستهدف. تساهم الميزات المستخدمة في الجزء العلوي من الشجرة في قرار التنبؤ النهائي لجزء أكبر من عينات الإدخال. يمكن استخدام <strong>النسبة المتوقعة من العينات</strong> التي تساهم فيها كتقدير لـ <strong>الأهمية النسبية للميزات</strong>. في Scikit-learn، يتم دمج نسبة العينات التي تساهم بها ميزة مع الانخفاض في النقاء من تقسيمها لإنشاء تقدير معاير لقوة التنبؤ لتلك الميزة.</p>
<p>عن طريق <strong>المتوسط</strong> التقديرات لقدرة التنبؤ عبر عدة أشجار عشوائية يمكن <strong>تقليل التباين</strong> في مثل هذا التقدير واستخدامه لاختيار الميزة. يُعرف هذا باسم الانخفاض المتوسط في النقاء، أو MDI. راجع <a class="reference internal" href="#l2014" id="id27"><span>[L2014]</span></a> لمزيد من المعلومات حول MDI وتقييم أهمية الميزة باستخدام Random Forests.</p>
<p>تحذير</p>
<p>تعاني أهمية الميزة القائمة على النقاء المحسوبة على النماذج القائمة على الأشجار من عيبين يمكن أن يؤديان إلى استنتاجات مضللة. أولاً، يتم حسابها على الإحصاءات المستمدة من مجموعة البيانات التدريبية وبالتالي <strong>لا تخبرنا بالضرورة بالميزات الأكثر أهمية للتنبؤات الدقيقة بمجموعة بيانات محجوزة</strong>. ثانيًا، <strong>يفضلون ميزات التعددية العالية</strong>، أي الميزات ذات القيم الفريدة العديدة. الأهمية حسب الترتيب هي بديل لأهمية النقاء لا يعاني من هذه العيوب. يتم استكشاف هاتين الطريقتين للحصول على أهمية الميزة في: sphx_glr_auto_examples_inspection_plot_permutation_importance.py.</p>
<p>يوضح المثال التالي تمثيلًا ملونًا لأهمية النسبية لكل بكسل فردي لمهمة التعرف على الوجه باستخدام نموذج ExtraTreesClassifier.</p>
<p>في الممارسة العملية، يتم تخزين هذه التقديرات كسمة تسمى “<a href="#id36"><span class="problematic" id="id37">feature_importances_</span></a>” على النموذج المناسب. هذا هو صفيف الشكل (n_features،) بقيم إيجابية ومجموع 1.0. كلما كانت القيمة أعلى، كلما كانت مساهمة الميزة المطابقة أكثر أهمية في دالة التنبؤ.</p>
</section>
<section id="id28">
<h2>أمثلة<a class="headerlink" href="#id28" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>sphx_glr_auto_examples_ensemble_plot_forest_importances_faces.py</p></li>
<li><p>sphx_glr_auto_examples_ensemble_plot_forest_importances.py</p></li>
</ul>
</section>
<section id="id29">
<h2>مراجع<a class="headerlink" href="#id29" title="Link to this heading">#</a></h2>
<div role="list" class="citation-list">
<div class="citation" id="l2014" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">L2014</a><span class="fn-bracket">]</span></span>
<p>G. Louppe،: arXiv: “Understanding Random Forests: From Theory to
Practice” &lt;1407.7502&gt;،
أطروحة دكتوراه، جامعة لييج، 2014.</p>
</div>
</div>
<p>غرس الأشجار العشوائية تمامًا
ينفذ <code class="xref py py-class docutils literal notranslate"><span class="pre">RandomTreesEmbedding</span></code> تحويلًا غير خاضع للإشراف للبيانات. باستخدام غابة من الأشجار العشوائية تمامًا، يشفر <code class="xref py py-class docutils literal notranslate"><span class="pre">RandomTreesEmbedding</span></code> البيانات من خلال مؤشرات الأوراق التي تنتهي فيها نقطة البيانات. بعد ذلك، يتم تشفير هذا المؤشر بطريقة واحدة من K، مما يؤدي إلى ترميز ثنائي عالي الأبعاد ومُنَفَّذ.</p>
<p>يمكن حساب هذا الترميز بكفاءة عالية ويمكن استخدامه بعد ذلك كأساس لمهام التعلم الأخرى.</p>
<p>يمكن التأثير على حجم الترميز وتباعده عن طريق اختيار عدد الأشجار والعمق الأقصى لكل شجرة. لكل شجرة في المجموعة، يحتوي الترميز على إدخال واحد من واحد. ويبلغ حجم الترميز كحد أقصى “n_estimators * 2 ** max_depth”، وهو العدد الأقصى للأوراق في الغابة.</p>
<p>نظرًا لأن نقاط البيانات المجاورة أكثر عرضة للتواجد داخل نفس ورقة الشجرة، فإن التحويل يؤدي إلى تقدير كثافة غير معلم ضمني.</p>
<p class="rubric">أمثلة</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_random_forest_embedding.html#sphx-glr-auto-examples-ensemble-plot-random-forest-embedding-py"><span class="std std-ref">Hashing feature transformation using Totally Random Trees</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py"><span class="std std-ref">Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…</span></a> يقارن تقنيات تقليل الأبعاد غير الخطية لأرقام مكتوبة بخط اليد.</p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py"><span class="std std-ref">Feature transformations with ensembles of trees</span></a> يقارن بين التحولات المميزة المستندة إلى الإشراف وغير الخاضعة للإشراف والقائمة على الأشجار.</p></li>
</ul>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>يمكن أيضًا أن تكون تقنيات <span class="xref std std-ref">manifold</span> مفيدة لاستنتاج التمثيلات غير الخطية لمساحة الميزة، كما تركز هذه الأساليب على تقليل الأبعاد.</p>
</div>
</section>
<section id="tree-ensemble-warm-start">
<span id="id30"></span><h2>تناسب أشجار إضافية<a class="headerlink" href="#tree-ensemble-warm-start" title="Link to this heading">#</a></h2>
<p>تدعم جميع تقديرات RandomForest وExtra-Trees و:class:<code class="docutils literal notranslate"><span class="pre">RandomTreesEmbedding</span></code> <code class="docutils literal notranslate"><span class="pre">warm_start=True</span></code>، والتي تتيح لك إضافة المزيد من الأشجار إلى نموذج تم تناسبه بالفعل.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># تناسب مع 10 أشجار</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
<span class="go">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># تعيين warm_start وزيادة عدد المؤشرات</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># تناسب 10 أشجار إضافية</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
<span class="go">20</span>
</pre></div>
</div>
<p>عندما يتم أيضًا تعيين “random_state”، يتم الاحتفاظ بحالة عشوائية داخلية بين مكالمات “fit”. وهذا يعني أن تدريب نموذج مرة واحدة مع “n” من المؤشرات هو نفسه بناء النموذج بشكل تكراري عبر مكالمات “fit” متعددة، حيث يكون العدد النهائي للمؤشرات مساويًا لـ “n”.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> <span class="c1"># تعيين `n_estimators` إلى 10 + 10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># تناسب `estimators_` ستكون هي نفسها كما `clf` أعلاه</span>
</pre></div>
</div>
<p>لاحظ أن هذا يختلف عن السلوك المعتاد لـ <a class="reference internal" href="../glossary.html#term-random_state"><span class="xref std std-term">random_state</span></a> في أنه لا يؤدي إلى نفس النتيجة عبر المكالمات المختلفة.</p>
<p id="bagging">Bagging meta-estimator
في خوارزميات التجميع، تشكل طرق المعايرة فئة من الخوارزميات التي تقوم ببناء عدة مثيلات لمقدّر الصندوق الأسود على مجموعات فرعية عشوائية من مجموعة التدريب الأصلية، ثم تجميع تنبؤاتها الفردية لتشكيل تنبؤ نهائي. وتُستخدم هذه الطرق كوسيلة للحد من تباين مُقدّر أساسي (مثل شجرة القرار)، من خلال إدخال العشوائية في إجراء بنائها ومن ثم تشكيل مجموعة منها. وفي العديد من الحالات، تشكل طرق المعايرة طريقة بسيطة للغاية للتحسين فيما يتعلق بنموذج واحد، دون أن يكون من الضروري تكييف خوارزمية القاعدة الأساسية. وبما أنها توفر طريقة للحد من الإفراط في التخصيص، فإن طرق المعايرة تعمل بشكل أفضل مع النماذج القوية والمعقدة (مثل شجرة القرار الكاملة)، على عكس طرق التعزيز التي تعمل عادة بشكل أفضل مع النماذج الضعيفة (مثل شجرة القرار الضحلة).</p>
<p>تأتي طرق المعايرة بنكهات عديدة ولكنها تختلف في الغالب عن بعضها البعض بطريقة سحبها لمجموعات فرعية عشوائية من مجموعة التدريب:</p>
<ul class="simple">
<li><p>عندما يتم سحب المجموعات الفرعية العشوائية من مجموعة البيانات كمجموعات فرعية عشوائية من العينات، يُعرف هذا الخوارزم باسم Pasting <span id="id31">[B1999]</span>.</p></li>
<li><p>عندما يتم سحب العينات مع الاستبدال، تُعرف الطريقة باسم Bagging <span id="id32">[B1996]</span>.</p></li>
<li><p>عندما يتم سحب المجموعات الفرعية العشوائية من مجموعة البيانات كمجموعات فرعية عشوائية من السمات، تُعرف الطريقة باسم Random Subspaces <span id="id33">[H1998]</span>.</p></li>
<li><p>وأخيرًا، عندما يتم بناء المُقدّرات الأساسية على مجموعات فرعية من كل من العينات والسمات، تُعرف الطريقة باسم Random Patches <span id="id34">[LG2012]</span>.</p></li>
</ul>
<p>في scikit-learn، تُقدم طرق المعايرة كمُقدّر موحد من فئة BaggingClassifier (أو BaggingRegressor)، والذي يأخذ كمدخلات مُقدّر محدد من قبل المستخدم بالإضافة إلى معلمات تحدد الاستراتيجية المستخدمة لسحب المجموعات الفرعية العشوائية. وعلى وجه التحديد، يتحكم كل من “max_samples” و”max_features” في حجم المجموعات الفرعية (من حيث العينات والسمات)، في حين يتحكم كل من “bootstrap” و”bootstrap_features” في ما إذا كانت العينات والسمات يتم سحبها مع الاستبدال أو بدونه. وعندما يتم استخدام مجموعة فرعية من العينات المتاحة، يمكن تقدير دقة التعميم باستخدام العينات خارج الحقيبة من خلال تعيين “oob_score=True”. وكمثال على ذلك، يوضح الجزء التالي من التعليمات البرمجية كيفية إنشاء مجموعة من المعايرة لمقدّرات KNeighborsClassifier، يتم بناء كل منها على مجموعات فرعية عشوائية من 50% من العينات و50% من السمات.</p>
<p>يتمثل المفهوم الأساسي وراء فئة VotingClassifier في دمج مصنفات التعلم الآلي المختلفة مفاهيمياً واستخدام التصويت بالأغلبية أو متوسط الاحتمالات المتوقعة (التصويت اللين) للتنبؤ بعلامات الفئات. ويمكن أن يكون مثل هذا المصنف مفيدًا لمجموعة من النماذج التي لها نفس الأداء الجيد من أجل موازنة نقاط ضعفها الفردية.</p>
<p>في التصويت بالأغلبية، تكون علامة الفئة المتوقعة لعينة معينة هي علامة الفئة التي تمثل أغلبية (الوضع) لعلامات الفئات التي تنبأ بها كل مصنف فردي.</p>
<p>على سبيل المثال، إذا كان التنبؤ لعينة معينة هو:</p>
<ul class="simple">
<li><p>المصنف 1 -&gt; الفئة 1</p></li>
<li><p>المصنف 2 -&gt; الفئة 1</p></li>
<li><p>المصنف 3 -&gt; الفئة 2</p></li>
</ul>
<p>فإن مصنف التصويت (مع “التصويت=’الصعب’”) سيصنف العينة على أنها “الفئة 1” بناءً على أغلبية علامات الفئات.</p>
<p>في حالة التعادل، سيختار مصنف التصويت الفئة بناءً على ترتيب الفرز التصاعدي. على سبيل المثال، في السيناريو التالي:</p>
<ul class="simple">
<li><p>المصنف 1 -&gt; الفئة 2</p></li>
<li><p>المصنف 2 -&gt; الفئة 1</p></li>
</ul>
<p>سيتم تعيين علامة الفئة 1 للعينة.</p>
<p>الاستخدام</p>
<p>يوضح المثال التالي كيفية ملاءمة مصنف قاعدة الأغلبية:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;hard&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">clf</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">clf1</span><span class="p">,</span> <span class="n">clf2</span><span class="p">,</span> <span class="n">clf3</span><span class="p">,</span> <span class="n">eclf</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Logistic Regression&#39;</span><span class="p">,</span> <span class="s1">&#39;Random Forest&#39;</span><span class="p">,</span> <span class="s1">&#39;naive Bayes&#39;</span><span class="p">,</span> <span class="s1">&#39;Ensemble&#39;</span><span class="p">]):</span>
<span class="gp">... </span>    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">) [</span><span class="si">%s</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">label</span><span class="p">))</span>
<span class="go">Accuracy: 0.95 (+/- 0.04) [Logistic Regression]</span>
<span class="go">Accuracy: 0.94 (+/- 0.04) [Random Forest]</span>
<span class="go">Accuracy: 0.91 (+/- 0.04) [naive Bayes]</span>
<span class="go">Accuracy: 0.95 (+/- 0.04) [Ensemble]</span>
</pre></div>
</div>
<p>على عكس التصويت بالأغلبية (التصويت الصعب)، يعيد التصويت اللين علامة الفئة كدالة argmax من مجموع الاحتمالات المتوقعة.</p>
<p>يمكن تعيين أوزان محددة لكل مصنف عبر معلمة “الأوزان”. عندما يتم توفير الأوزان، يتم جمع الاحتمالات المتوقعة للفئة لكل مصنف، وضربها بوزن المصنف، ثم حساب متوسطها. يتم بعد ذلك اشتقاق علامة الفئة النهائية من علامة الفئة ذات أعلى متوسط احتمال.</p>
<p>ولتوضيح ذلك بمثال بسيط، دعونا نفترض أن لدينا 3 مصنفات ومشكلة تصنيف من 3 فئات نقوم فيها بتعيين أوزان متساوية لجميع المصنفات: w1=1، w2=1، w3=1.</p>
<p>سيتم بعد ذلك حساب متوسطات الأوزان الاحتمالية لعينة ما على النحو التالي:</p>
<p>هنا، تكون علامة الفئة المتوقعة هي 2، لأنها تمتلك أعلى متوسط احتمال.</p>
<p>يوضح المثال التالي كيف يمكن أن تتغير مناطق القرار عند استخدام مصنف تصويت لين يعتمد على مصنف Support Vector Machine خطي، وشجرة قرار، ومصنف K-nearest neighbor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># تحميل بعض بيانات المثال</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># تدريب المصنفات</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;dt&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;knn&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>                        <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">clf1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">clf2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">clf3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">eclf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>الاستخدام</p>
<p>من أجل التنبؤ بعلامات الفئات بناءً على الاحتمالات المتوقعة للفئات (يجب أن تدعم مصنفات scikit-learn في مصنف التصويت طريقة “predict_proba”):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>يمكن أيضًا توفير أوزان للمصنفات الفردية بشكل اختياري:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>يمكن أيضًا استخدام مصنف التصويت مع GridSearchCV من أجل ضبط فرط معلمات المصنفات الفردية:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lr__C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">],</span> <span class="s1">&#39;rf__n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">]}</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">eclf</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grid</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
<p>الفكرة الأساسية وراء فئة VotingRegressor هي دمج مراجعات التعلم الآلي المفاهيمية المختلفة وإرجاع المتوسطات المتوقعة. ويمكن أن يكون مثل هذا المرجع مفيدًا لمجموعة من النماذج التي لها نفس الأداء الجيد من أجل موازنة نقاط ضعفها الفردية.</p>
<p>الاستخدام</p>
<p>يوضح المثال التالي كيفية ملاءمة مصنف التصويت:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingRegressor</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># تحميل بعض بيانات المثال</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># تدريب المراجعات</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg1</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg2</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg3</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ereg</span> <span class="o">=</span> <span class="n">VotingRegressor</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;gb&#39;</span><span class="p">,</span> <span class="n">reg1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">reg2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">reg3</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ereg</span> <span class="o">=</span> <span class="n">ereg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>التعميم المكدس
التعميم المكدس هو طريقة لدمج المُقدّرين لتقليل انحيازاتهم. وبشكل أكثر دقة، يتم تكديس تنبؤات كل مُقدّر فردي واستخدامها كمدخلات لمُقدّر نهائي لحساب التنبؤ. يتم تدريب هذا المُقدّر النهائي من خلال التصديق المتبادل.</p>
<p>توفر كل من class: ‘StackingClassifier’ و class: ‘StackingRegressor’ مثل هذه الاستراتيجيات التي يمكن تطبيقها على مشكلات التصنيف والانحدار.</p>
<p>يرتبط معامل ‘estimators’ بقائمة المُقدّرين الذين يتم تكديسهم معًا بالتوازي على بيانات المدخلات. يجب إعطاؤه على شكل قائمة من الأسماء والمُقدّرين.</p>
<p>يستخدم المُقدّر النهائي ‘final_estimator’ تنبؤات ‘estimators’ كمدخلات. يجب أن يكون مصنفًا أو مُقدّر انحدار عند استخدام class: ‘StackingClassifier’ أو class: ‘StackingRegressor’، على التوالي.</p>
<p>لتدريب المُقدّرين ‘estimators’ والمُقدّر النهائي ‘final_estimator’، يجب استدعاء طريقة ‘fit’ على بيانات التدريب.</p>
<p>خلال التدريب، يتم ضبط المُقدّرين ‘estimators’ على كامل بيانات التدريب ‘X_train’. سيتم استخدامها عند استدعاء طريقة ‘predict’ أو ‘predict_proba’. ولتعميم وتجنب الإفراط في الضبط، يتم تدريب المُقدّر النهائي ‘final_estimator’ على عينات خارجية باستخدام طريقة ‘sklearn.model_selection.cross_val_predict’ داخليًا.</p>
<p>بالنسبة لـ class: ‘StackingClassifier’، لاحظ أن إخراج المُقدّرين ‘estimators’ يتحكم فيه معامل ‘stack_method’ ويتم استدعاؤه بواسطة كل مُقدّر. هذا المعامل إما أن يكون سلسلة، أو أسماء طرق المُقدّر، أو ‘auto’ الذي سيحدد تلقائيًا طريقة متوفرة اعتمادًا على التوفر، ويتم اختباره حسب تفضيل: ‘predict_proba’، أو ‘decision_function’، أو ‘predict’.</p>
<p>يمكن استخدام class: ‘StackingRegressor’ و class: ‘StackingClassifier’ كأي مُقدّر انحدار أو تصنيف آخر، مع عرض طريقة ‘predict’، أو ‘predict_proba’، أو ‘decision_function’، على سبيل المثال.</p>
<p>لاحظ أنه من الممكن أيضًا الحصول على إخراج المُقدّرين المكدسين ‘estimators’ باستخدام طريقة ‘transform’.</p>
<p>في الممارسة العملية، يتنبأ مُقدّر التكديس بنفس جودة أفضل مُقدّر في الطبقة الأساسية، بل ويتفوق عليه أحيانًا من خلال الجمع بين نقاط القوة المختلفة لهذه المُقدّرات. ومع ذلك، فإن تدريب مُقدّر التكديس مكلف من الناحية الحسابية.</p>
<p>بالنسبة لـ class: ‘StackingClassifier’، عند استخدام ‘stack_method_=’predict_proba’، يتم إسقاط العمود الأول عندما تكون المشكلة هي مشكلة تصنيف ثنائي. في الواقع، كلا عمودي الاحتمالية التي يتنبأ بها كل مُقدّر متطابقان تمامًا.</p>
<p>يمكن تحقيق طبقات التكديس المتعددة من خلال تعيين ‘final_estimator’ إلى class: ‘StackingClassifier’ أو class: ‘StackingRegressor’.</p>
<p>يحتوي نموذج sklearn.ensemble على خوارزمية التعزيز AdaBoost الشائعة، التي قدمها Freund و Schapire في عام 1995.</p>
<p>المبدأ الأساسي لـ AdaBoost هو ضبط تسلسل من المتعلمين الضعفاء (أي النماذج التي تكون أفضل بقليل من التخمين العشوائي، مثل أشجار القرار الصغيرة) على الإصدارات المعدلة بشكل متكرر من البيانات. ثم يتم الجمع بين التنبؤات من كل منهم من خلال تصويت الأغلبية المرجح (أو المجموع) لإنتاج التنبؤ النهائي. تتكون تعديلات البيانات في كل ما يسمى بتكرار التعزيز من تطبيق الأوزان :math: ‘w_1’، :math: ‘w_2’، …، :math: ‘w_N’ على كل من عينات التدريب. في البداية، يتم تعيين جميع هذه الأوزان إلى :math: ‘w_i = 1/N’، بحيث تقتصر الخطوة الأولى ببساطة على تدريب متعلم ضعيف على البيانات الأصلية. بالنسبة لكل تكرار لاحق، يتم تعديل الأوزان الفردية للنماذج ويتم إعادة تطبيق خوارزمية التعلم على البيانات المعاد ترجيحها. في خطوة معينة، يتم زيادة أوزان نماذج التدريب التي تم التنبؤ بها بشكل غير صحيح بواسطة النموذج المدعوم المستحث في الخطوة السابقة، في حين يتم تقليل الأوزان للنماذج التي تم التنبؤ بها بشكل صحيح. مع تقدم التكرارات، تتلقى الأمثلة التي يصعب التنبؤ بها تأثيرًا متزايدًا باستمرار. يتم إجبار كل متعلم ضعيف لاحق على التركيز على الأمثلة التي فاتته من قبل المتعلمين السابقين في التسلسل.</p>
<p>يمكن استخدام AdaBoost لكل من مشكلات التصنيف والانحدار:</p>
<ul class="simple">
<li><p>بالنسبة للتصنيف متعدد الفئات، ينفذ class: ‘AdaBoostClassifier’ خوارزمية AdaBoost.SAMME.</p></li>
<li><p>للانحدار، ينفذ class: ‘AdaBoostRegressor’ خوارزمية AdaBoost.R2.</p></li>
</ul>
</section>
<section id="id35">
<h2>الاستخدام<a class="headerlink" href="#id35" title="Link to this heading">#</a></h2>
<p>يُظهر المثال التالي كيفية ضبط مصنف AdaBoost مع 100 متعلم ضعيف:</p>
<p>من sklearn.model_selection استورد cross_val_score</p>
<p>من sklearn.datasets استورد load_iris</p>
<p>من sklearn.ensemble استورد AdaBoostClassifier</p>
<p>X، مجموعة بيانات load_iris (return_X_y = True)</p>
<p>clf = AdaBoostClassifier (n_estimators = 100، algorithm = “SAMME”)</p>
<p>التدرجات = cross_val_score (clf، X، y، cv = 5)</p>
<p>التدرجات. mean ()</p>
<p>0.9…</p>
<p>يتم التحكم في عدد المتعلمين الضعفاء بواسطة معامل “n_estimators”. يتحكم معامل “learning_rate” في مساهمة المتعلمين الضعفاء في المزيج النهائي. بشكل افتراضي، تكون المتعلمات الضعيفة هي جذوع القرار. يمكن تحديد متعلمين ضعفاء مختلفين من خلال معامل “estimator”.</p>
<p>المعلمات الرئيسية التي يجب ضبطها للحصول على نتائج جيدة هي “n_estimators” ومدى تعقيد المُقدّرات الأساسية (على سبيل المثال، عمقها “max_depth” أو الحد الأدنى المطلوب من العينات للنظر في الانقسام “min_samples_split”).</p>
</section>
</section>


                </article>
              
              
              
                <footer class="bd-footer-article">
                  <div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item">
<div class="prev-next-area">
</div></div>
  
</div>
                </footer>
              
              
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">الاستخدام</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#nan-support-hgbt">دعم القيم المفقودة</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#sw-hgbdt">دعم وزن العينة</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-support-gbdt">دعم الميزات الفئوية</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#monotonic-cst-gbdt">القيود الأحادية الاتجاه</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#interaction-cst-hgbt">قيود التفاعل</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">التوازي منخفض المستوى</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#why-it-s-faster">لماذا هو أسرع</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-boosting-warm-start">ملاءمة متعلمين ضعفاء إضافيين</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">تحكم حجم الشجرة</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">الصيغة الرياضية</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">وظائف الخسارة</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">التقلص عبر معدل التعلم</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">الاستعانة بعينة جزئية</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">التفسير بأهمية الميزة</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">غابات عشوائية</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">أمثلة</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">الأشجار العشوائية للغاية</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">المعلمات</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">التوازي</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">أمثلة</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">مراجع</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">تقييم أهمية الميزة</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">أمثلة</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">مراجع</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-ensemble-warm-start">تناسب أشجار إضافية</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id35">الاستخدام</a></li>
</ul>
</li>
</ul>

  </nav></div>

  <div class="sidebar-secondary-item">

  <div class="tocsection sourcelink">
    <a href="../_sources/modules/ensemble.rst.txt">
      <i class="fa-solid fa-file-lines"></i> Show Source
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2007 - 2024, scikit-learn developers (BSD License).
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
</div>

  </footer>
  </body>
</html>